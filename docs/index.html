<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Volcano SDK - Build MCP-powered AI Agents</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="https://unpkg.com/prismjs@1.29.0/themes/prism-tomorrow.min.css"/>
  <script src="https://unpkg.com/prismjs@1.29.0/components/prism-core.min.js"></script>
  <script src="https://unpkg.com/prismjs@1.29.0/components/prism-clike.min.js"></script>
  <script src="https://unpkg.com/prismjs@1.29.0/components/prism-javascript.min.js"></script>
  <script src="https://unpkg.com/prismjs@1.29.0/components/prism-typescript.min.js"></script>
  <script src="https://unpkg.com/prismjs@1.29.0/components/prism-bash.min.js"></script>
</head>
<body>
  <div class="layout">
    <!-- Sidebar Navigation -->
    <aside class="sidebar">
      <div class="sidebar-header">
        <a href="index.html" class="logo">
          <span class="emoji">üåã</span>
          <span class="text">Volcano SDK</span>
        </a>
      </div>
      
      <nav class="sidebar-nav">
        <div class="nav-section">
          <div class="nav-section-title">Getting Started</div>
          <a href="index.html#introduction" class="nav-link active">Introduction</a>
          <a href="index.html#installation" class="nav-link">Installation</a>
          <a href="index.html#quick-start" class="nav-link">Quick Start</a>
          <a href="index.html#core-concepts" class="nav-link">Core Concepts</a>
        </div>

        <div class="nav-section">
          <div class="nav-section-title">Providers</div>
          <a href="providers.html" class="nav-link">Overview</a>
          <a href="providers.html#openai" class="nav-link">OpenAI</a>
          <a href="providers.html#anthropic" class="nav-link">Anthropic (Claude)</a>
          <a href="providers.html#mistral" class="nav-link">Mistral</a>
          <a href="providers.html#llama" class="nav-link">Llama</a>
          <a href="providers.html#bedrock" class="nav-link">AWS Bedrock</a>
          <a href="providers.html#vertex" class="nav-link">Google Vertex Studio</a>
          <a href="providers.html#azure" class="nav-link">Azure AI</a>
        </div>

        <div class="nav-section">
          <div class="nav-section-title">Advanced Patterns</div>
          <a href="patterns.html" class="nav-link">Overview</a>
          <a href="patterns.html#parallel" class="nav-link">Parallel Execution</a>
          <a href="patterns.html#branching" class="nav-link">Conditional Branching</a>
          <a href="patterns.html#loops" class="nav-link">Loops</a>
          <a href="patterns.html#sub-agents" class="nav-link">Sub-Agent Composition</a>
        </div>

        <div class="nav-section">
          <div class="nav-section-title">Features</div>
          <a href="features.html" class="nav-link">Overview</a>
          <a href="features.html#streaming" class="nav-link">Streaming</a>
          <a href="features.html#retries" class="nav-link">Retries & Timeouts</a>
          <a href="features.html#hooks" class="nav-link">Step Hooks</a>
          <a href="features.html#errors" class="nav-link">Error Handling</a>
          <a href="features.html#mcp-tools" class="nav-link">MCP Tools</a>
        </div>

        <div class="nav-section">
          <div class="nav-section-title">API Reference</div>
          <a href="api.html" class="nav-link">Overview</a>
          <a href="api.html#agent" class="nav-link">agent()</a>
          <a href="api.html#step" class="nav-link">Step Types</a>
          <a href="api.html#results" class="nav-link">Step Results</a>
        </div>
      </nav>
    </aside>

    <!-- Main Content -->
    <main class="content">
      <div class="content-inner">
        <!-- Introduction -->
        <section id="introduction" class="doc-section">
          <h1>Volcano SDK üåã</h1>
          <p class="lead">A TypeScript SDK for building AI agents that combine LLM reasoning with real-world actions via MCP tools.</p>
          
          <div class="highlight-box orange">
            <strong>Design Philosophy:</strong> Volcano provides a fluent, chainable API that scales from simple single-step agents to complex multi-provider workflows with parallel execution, retries, and streaming.
          </div>
          
          <div class="feature-grid">
            <div class="feature-card">
              <div class="feature-icon">‚ö°Ô∏è</div>
              <h3>Chainable API</h3>
              <p>Chain steps with <code>.then()</code> and <code>.run()</code>. Promise-like syntax for building multi-step workflows.</p>
            </div>
            <div class="feature-card">
              <div class="feature-icon">‚ú®</div>
              <h3>Automatic Tool Selection</h3>
              <p>The LLM selects and calls appropriate MCP tools based on the prompt. No manual routing required.</p>
            </div>
            <div class="feature-card">
              <div class="feature-icon">üîß</div>
              <h3>7 LLM Providers</h3>
              <p>Supports OpenAI, Anthropic, Mistral, Llama, Bedrock, Vertex, and Azure. Switch providers per-step or globally.</p>
            </div>
            <div class="feature-card">
              <div class="feature-icon">üõ°Ô∏è</div>
              <h3>TypeScript-First</h3>
              <p>Full TypeScript support with type inference and IntelliSense for all APIs.</p>
            </div>
            <div class="feature-card">
              <div class="feature-icon">üîÑ</div>
              <h3>Advanced Patterns</h3>
              <p>Parallel execution, conditional branching, loops, and sub-agent composition for complex workflows.</p>
            </div>
            <div class="feature-card">
              <div class="feature-icon">‚è±Ô∏è</div>
              <h3>Retries & Timeouts</h3>
              <p>Configurable retry strategies (immediate, delayed, exponential backoff) and per-step timeouts.</p>
            </div>
            <div class="feature-card">
              <div class="feature-icon">üì°</div>
              <h3>Streaming Workflows</h3>
              <p>Stream step results as they complete using async generators. Useful for long-running tasks and real-time UIs.</p>
            </div>
            <div class="feature-card">
              <div class="feature-icon">üéØ</div>
              <h3>MCP Integration</h3>
              <p>Native Model Context Protocol support with connection pooling, tool discovery, and JSON schema validation.</p>
            </div>
            <div class="feature-card">
              <div class="feature-icon">üß©</div>
              <h3>Sub-Agent Composition</h3>
              <p>Define reusable agent components and compose them into larger workflows.</p>
            </div>
          </div>

          <div class="nav-cards">
            <a href="providers.html" class="nav-card">
              <h3>üîå LLM Providers</h3>
              <p>Connect to OpenAI, Anthropic, Mistral, and more</p>
            </a>
            <a href="patterns.html" class="nav-card">
              <h3>üîÑ Advanced Patterns</h3>
              <p>Parallel execution, branching, loops, and composition</p>
            </a>
            <a href="features.html" class="nav-card">
              <h3>‚ú® Features</h3>
              <p>Streaming, retries, hooks, and error handling</p>
            </a>
            <a href="api.html" class="nav-card">
              <h3>üìö API Reference</h3>
              <p>Complete API documentation and types</p>
            </a>
          </div>
        </section>

        <!-- Installation -->
        <section id="installation" class="doc-section">
          <h2>Installation</h2>
          <p>Install Volcano SDK and its peer dependencies:</p>
          
          <div class="code-block">
            <pre><code class="language-bash">npm install volcano-sdk
npm install @modelcontextprotocol/sdk openai

# For AWS Bedrock support (optional)
npm install @aws-sdk/client-bedrock-runtime @aws-sdk/credential-providers

# For Azure support (optional)
npm install @azure/identity</code></pre>
          </div>

          <div class="info-box success">
            <div class="info-icon">‚úì</div>
            <div>
              <strong>Requirements:</strong> Node.js 18.17 or later
            </div>
          </div>
        </section>

        <!-- Quick Start -->
        <section id="quick-start" class="doc-section">
          <h2>Quick Start</h2>
          
          <h3>Hello World</h3>
          <p>The simplest possible agent with a single LLM step:</p>
          
          <div class="code-block">
            <pre><code class="language-typescript">import { agent, llmOpenAI } from "volcano-sdk";

const llm = llmOpenAI({ 
  apiKey: process.env.OPENAI_API_KEY!, 
  model: "gpt-4o-mini" 
});

const results = await agent({ llm })
  .then({ prompt: "Say hello to Marco in one short sentence." })
  .run();

console.log(results[0].llmOutput);
// Output: "Hello Marco! Hope you're having a great day!"</code></pre>
          </div>

          <h3>Two-Step Agent with MCP Tools</h3>
          <p>Automatically discover and use MCP tools across multiple steps:</p>
          
          <div class="code-block">
            <pre><code class="language-typescript">import { agent, llmOpenAI, mcp } from "volcano-sdk";

const llm = llmOpenAI({ 
  apiKey: process.env.OPENAI_API_KEY!, 
  model: "gpt-4o-mini" 
});

const astro = mcp("http://localhost:3211/mcp");

const steps = await agent({ llm })
  .then({
    prompt: "Determine the astrological sign for 1993-07-11.",
    mcps: [astro]  // Automatic tool selection
  })
  .then({
    prompt: "Write a one-line fortune for that sign."
    // Context from previous step is automatically included
  })
  .run();

console.log(steps[0].toolCalls); // Tools that were called
console.log(steps[1].llmOutput); // Fortune based on the sign</code></pre>
          </div>

          <div class="next-steps">
            <h3>Next Steps</h3>
            <ul>
              <li>Explore <a href="providers.html">LLM Providers</a> for configuration options</li>
              <li>Learn <a href="patterns.html">Advanced Patterns</a> for complex workflows</li>
              <li>Discover <a href="features.html">Features</a> like streaming and error handling</li>
              <li>Check the <a href="api.html">API Reference</a> for complete documentation</li>
            </ul>
          </div>
        </section>

        <!-- Core Concepts -->
        <section id="core-concepts" class="doc-section">
          <h2>Core Concepts</h2>
          
          <h3>Agents</h3>
          <p>An agent is a workflow builder that chains together steps. Each step can call an LLM, execute MCP tools, or both. Create an agent with <code>agent()</code>:</p>
          
          <div class="code-block">
            <pre><code class="language-typescript">const myAgent = agent({ 
  llm,                        // Default LLM provider (OpenAI, Claude, etc.)
  instructions: "...",        // Global system instructions for ALL steps
  timeout: 60,                // Default timeout in seconds (prevents hanging)
  retry: { retries: 3 },      // Retry configuration (automatic failure recovery)
  contextMaxChars: 20480,     // Context size limit (prevent token overflow)
  contextMaxToolResults: 8    // How many tool results to keep in context
});</code></pre>
          </div>

          <div class="highlight-box blue">
            <strong>Note:</strong> All options are optional. The minimal configuration is <code>agent({ llm })</code>.
          </div>

          <h3>Steps</h3>
          <p>Each step in a workflow can perform one or more of the following:</p>
          <ul class="benefits-list">
            <li><strong>Generate text</strong> using any supported LLM provider</li>
            <li><strong>Call MCP tools</strong> either explicitly or via automatic selection</li>
            <li><strong>Override configuration</strong> such as timeout, LLM provider, or retry strategy</li>
            <li><strong>Access context</strong> from previous steps automatically</li>
            <li><strong>Execute hooks</strong> before or after execution</li>
          </ul>

          <h3>Context & History</h3>
          <p>Each step automatically receives context from previous steps. This includes:</p>
          
          <ul class="benefits-list">
            <li><strong>Previous LLM responses</strong> from earlier steps</li>
            <li><strong>Tool results</strong> from MCP tool calls</li>
            <li><strong>Automatic compaction</strong> to stay within token limits</li>
          </ul>
          
          <p>Use <code>resetHistory()</code> to clear context:</p>
          
          <div class="code-block">
            <pre><code class="language-typescript">await agent({ llm })
  .then({ prompt: "Analyze this document..." })
  .then({ prompt: "What were the key points?" }) // Has context from step 1
  .resetHistory()  // üßπ Clear all context
  .then({ prompt: "Now analyze this OTHER document..." }) // Fresh start
  .run();</code></pre>
          </div>
          
          <div class="info-box tip">
            <div class="info-icon">üí°</div>
            <div>
              <strong>Note:</strong> Use <code>resetHistory()</code> in long workflows to prevent context from growing too large. Each agent instance maintains its own history.
            </div>
          </div>

          <h3>Multi-Provider Workflows</h3>
          <p>Different LLM providers can be used for different steps in the same workflow:</p>
          
          <div class="code-block">
            <pre><code class="language-typescript">import { agent, llmOpenAI, llmAnthropic, llmMistral } from "volcano-sdk";

const openai = llmOpenAI({ apiKey: process.env.OPENAI_API_KEY! });
const claude = llmAnthropic({ apiKey: process.env.ANTHROPIC_API_KEY! });
const mistral = llmMistral({ apiKey: process.env.MISTRAL_API_KEY! });

await agent()
  .then({ llm: openai, prompt: "Get astrological sign for 1993-07-11" })
  .then({ llm: claude, prompt: "Analyze personality traits" })
  .then({ llm: mistral, prompt: "Write a creative horoscope" })
  .run();
// Each step uses a different LLM. Context flows between steps.</code></pre>
          </div>
          
          <div class="highlight-box purple">
            <strong>Example use cases:</strong>
            <ul style="margin: 12px 0 0; padding-left: 24px;">
              <li>GPT-4 for data extraction, Claude for analysis, Mistral for summaries</li>
              <li>Local Llama for preprocessing, GPT-4 for final processing</li>
              <li>Test different models to optimize cost and quality</li>
            </ul>
          </div>
        </section>

      </div>

      <!-- Table of Contents (Right Sidebar) -->
      <aside class="toc">
        <div class="toc-header">On this page</div>
        <nav class="toc-nav" id="toc-nav">
          <!-- Dynamically populated by script.js -->
        </nav>
      </aside>
    </main>
  </div>

  <script src="script.js"></script>
</body>
</html>