import { agent, llmOpenAI, llmVertexStudio, mcp } from "../dist/volcano-sdk.js";

// Run with: npx tsx examples/streaming.ts
// This example demonstrates agent-level streaming for real-time step results

(async () => {
  console.log("Volcano SDK - Streaming Workflow Example");
  console.log("==========================================");
  console.log("Watch steps complete in real-time!\n");

  // Configure LLM provider
  const llm = llmOpenAI({ 
    apiKey: process.env.OPENAI_API_KEY!, 
    model: "gpt-4o-mini" 
  });

  // Alternative: Use Vertex Studio
  // const llm = llmVertexStudio({ 
  //   apiKey: process.env.GCP_VERTEX_API_KEY!, 
  //   model: "gemini-2.5-flash-lite" 
  // });

  // Setup MCP service (optional - will work without it too)
  const astro = mcp("http://localhost:3211/mcp");

  let stepCount = 0;
  const totalSteps = 4;
  const startTime = Date.now();

  try {
    console.log("ğŸš€ Starting streaming workflow...\n");

    // Stream a multi-step workflow in real-time
    for await (const stepResult of agent({ 
      llm,
      instructions: "You are a helpful assistant. Be concise but friendly."
    })
      .then({ 
        prompt: "Generate 3 random positive words",
        pre: () => console.log("ğŸ“ Step 1: Generating positive words..."),
        post: () => console.log("âœ… Step 1: Words generated!")
      })
      .then({ 
        prompt: "Create a short motivational quote using those words",
        pre: () => console.log("ğŸ“ Step 2: Creating motivational quote..."),
        post: () => console.log("âœ… Step 2: Quote created!")
      })
      .then({ 
        prompt: "Explain why motivation is important in 1 sentence",
        pre: () => console.log("ğŸ“ Step 3: Explaining motivation..."),
        post: () => console.log("âœ… Step 3: Explanation complete!")
      })
      .then({ 
        prompt: "Rate the overall positivity of this conversation from 1-10",
        pre: () => console.log("ğŸ“ Step 4: Rating conversation..."),
        post: () => console.log("âœ… Step 4: Rating complete!")
      })
      .stream((step, stepIndex) => {
        // This callback fires immediately when each step completes
        stepCount++;
        const progress = Math.round((stepCount / totalSteps) * 100);
        const elapsed = Date.now() - startTime;
        
        console.log(`\nğŸ”„ STEP ${stepIndex + 1} COMPLETED (${progress}%)`);
        console.log(`   â±ï¸  Duration: ${step.durationMs}ms`);
        console.log(`   ğŸ“Š Total elapsed: ${elapsed}ms`);
        if (step.llmMs) console.log(`   ğŸ§  LLM time: ${step.llmMs}ms`);
        console.log(`   ğŸ“ Prompt: ${step.prompt}`);
        console.log(`   ğŸ’­ Response: ${step.llmOutput}`);
        console.log("   " + "â”€".repeat(50));
      })) {

      // This also executes for each step (alternative to the callback above)
      const elapsed = Date.now() - startTime;
      console.log(`\nğŸ”” Step notification: "${stepResult.llmOutput?.substring(0, 50)}..." (${elapsed}ms total)`);
    }

    const totalTime = Date.now() - startTime;
    console.log(`\nğŸ‰ Streaming workflow complete!`);
    console.log(`ğŸ“Š Total time: ${totalTime}ms`);
    console.log(`ğŸ“ˆ Average per step: ${Math.round(totalTime / totalSteps)}ms`);
    console.log(`âš¡ Steps streamed: ${stepCount}/${totalSteps}`);

  } catch (error) {
    console.error(`âŒ Streaming error: ${error.message}`);
  }

})();
