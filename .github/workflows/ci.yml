name: CI

on:
  push:
    branches: [ main ]
    paths-ignore:
      - 'web/**'
  pull_request:
    paths-ignore:
      - 'web/**'

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      OLLAMA_MODELS: ${{ github.workspace }}/.cache/ollama-models
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Use Node.js 20
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Lint
        run: npm run lint

      - name: Build
        run: npm run build

      - name: Cache Ollama models
        id: cache-ollama
        uses: actions/cache@v4
        with:
          path: ${{ github.workspace }}/.cache/ollama-models
          key: ollama-models-${{ runner.os }}-llama3.2-3b-v3
          restore-keys: |
            ollama-models-${{ runner.os }}-llama3.2-3b-
            ollama-models-${{ runner.os }}-

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Setup Ollama with cached models
        run: |
          mkdir -p "$OLLAMA_MODELS"
          echo "Cache hit: ${{ steps.cache-ollama.outputs.cache-hit }}"
          
          # Start Ollama with custom model directory
          echo "Starting Ollama server..."
          nohup env OLLAMA_MODELS="$OLLAMA_MODELS" ollama serve > ollama.log 2>&1 &
          
          # Wait for Ollama to start
          echo "Waiting for Ollama to start..."
          for i in {1..15}; do
            if curl -s http://127.0.0.1:11434/api/tags >/dev/null 2>&1; then
              echo "‚úÖ Ollama server started successfully"
              break
            fi
            echo "Waiting for Ollama... ($i/15)"
            sleep 2
          done
          
          # Check if server started
          if ! curl -s http://127.0.0.1:11434/api/tags >/dev/null 2>&1; then
            echo "‚ùå Ollama failed to start. Server logs:"
            cat ollama.log || echo "No logs available"
            exit 1
          fi
          
          # List available models for debugging
          echo "Available models in Ollama:"
          env OLLAMA_MODELS="$OLLAMA_MODELS" ollama list || echo "No models found"
          
          # Check if model exists or download it
          echo "Checking for llama3.2:3b model..."
          if env OLLAMA_MODELS="$OLLAMA_MODELS" ollama show llama3.2:3b >/dev/null 2>&1; then
            echo "‚úÖ Model llama3.2:3b found!"
          else
            echo "üì• Model not found, downloading llama3.2:3b..."
            echo "This will take 2-3 minutes on first run, but will be cached for future runs."
            env OLLAMA_MODELS="$OLLAMA_MODELS" ollama pull llama3.2:3b
            echo "‚úÖ Model download complete!"
          fi
          
          # Verify model is available
          echo "Final model verification:"
          env OLLAMA_MODELS="$OLLAMA_MODELS" ollama list
          
          # Verify model is working
          echo "Testing Ollama functionality..."
          test_response=$(curl -s http://127.0.0.1:11434/api/generate -d '{
            "model": "llama3.2:3b",
            "prompt": "Say OK",
            "stream": false
          }' -H "Content-Type: application/json")
          
          if echo "$test_response" | grep -q '"response"'; then
            echo "‚úÖ Ollama is working correctly"
            echo "Test response: $test_response"
          else
            echo "‚ùå Ollama test failed"
            echo "Response: $test_response"
            exit 1
          fi

      - name: Run tests (unit/e2e)
        run: npm test
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_BASE_URL: https://api.openai.com/v1
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
          LLAMA_BASE_URL: http://127.0.0.1:11434
          LLAMA_MODEL: llama3.2:3b
          AWS_BEARER_TOKEN_BEDROCK: ${{ secrets.AWS_BEARER_TOKEN_BEDROCK }}
          AWS_REGION: us-east-1
          BEDROCK_MODEL: amazon.nova-micro-v1:0
          GCP_VERTEX_API_KEY: ${{ secrets.GCP_VERTEX_API_KEY }}
          VERTEX_MODEL: gemini-2.5-flash-lite
          AZURE_AI_API_KEY: ${{ secrets.AZURE_AI_API_KEY }}
          AZURE_AI_ENDPOINT: https://volcano-sdk.openai.azure.com/openai/responses
          AZURE_AI_MODEL: gpt-5-mini
