name: Publish to npm

on:
  push:
    tags:
      - 'v*'  # Trigger on version tags like v1.0.0, v0.1.0, etc.

jobs:
  publish:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write  # Required for npm provenance
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Use Node.js 20
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          registry-url: 'https://registry.npmjs.org'

      - name: Install dependencies
        run: npm ci

      - name: Lint
        run: npm run lint

      - name: Build
        run: npm run build

      - name: Cache Ollama models
        id: cache-ollama
        uses: actions/cache@v4
        with:
          path: ${{ github.workspace }}/.cache/ollama-models
          key: ollama-models-${{ runner.os }}-llama3.2-3b-v3
          restore-keys: |
            ollama-models-${{ runner.os }}-llama3.2-3b-
            ollama-models-${{ runner.os }}-

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Setup Ollama with cached models
        env:
          OLLAMA_MODELS: ${{ github.workspace }}/.cache/ollama-models
        run: |
          mkdir -p "$OLLAMA_MODELS"
          echo "Cache hit: ${{ steps.cache-ollama.outputs.cache-hit }}"
          
          # Start Ollama with custom model directory
          echo "Starting Ollama server..."
          nohup env OLLAMA_MODELS="$OLLAMA_MODELS" ollama serve > ollama.log 2>&1 &
          
          # Wait for Ollama to start
          echo "Waiting for Ollama to start..."
          for i in {1..15}; do
            if curl -s http://127.0.0.1:11434/api/tags >/dev/null 2>&1; then
              echo "âœ… Ollama server started successfully"
              break
            fi
            echo "Waiting for Ollama... ($i/15)"
            sleep 2
          done
          
          # Check if server started
          if ! curl -s http://127.0.0.1:11434/api/tags >/dev/null 2>&1; then
            echo "âŒ Ollama failed to start. Server logs:"
            cat ollama.log || echo "No logs available"
            exit 1
          fi
          
          # Check if model exists or download it
          echo "Checking for llama3.2:3b model..."
          if env OLLAMA_MODELS="$OLLAMA_MODELS" ollama show llama3.2:3b >/dev/null 2>&1; then
            echo "âœ… Model llama3.2:3b found!"
          else
            echo "ðŸ“¥ Model not found, downloading llama3.2:3b..."
            env OLLAMA_MODELS="$OLLAMA_MODELS" ollama pull llama3.2:3b
            echo "âœ… Model download complete!"
          fi

      - name: Run tests
        run: npm test
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_BASE_URL: https://api.openai.com/v1
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          LLAMA_BASE_URL: http://127.0.0.1:11434
          LLAMA_MODEL: llama3.2:3b
          AWS_BEARER_TOKEN_BEDROCK: ${{ secrets.AWS_BEARER_TOKEN_BEDROCK }}
          AWS_REGION: us-east-1
          BEDROCK_MODEL: amazon.nova-micro-v1:0
          GCP_VERTEX_API_KEY: ${{ secrets.GCP_VERTEX_API_KEY }}
          VERTEX_MODEL: gemini-2.5-flash-lite
          AZURE_AI_API_KEY: ${{ secrets.AZURE_AI_API_KEY }}
          AZURE_AI_ENDPOINT: https://volcano-sdk.openai.azure.com/openai/responses
          AZURE_AI_MODEL: gpt-5-mini

      - name: Publish to npm
        run: npm publish --provenance --access public
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}

