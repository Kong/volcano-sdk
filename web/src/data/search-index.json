[
  {
    "id": "Documentation-providers--anthropic",
    "title": "✅ Anthropic",
    "description": "✅ Anthropic",
    "content": "✅ Anthropic",
    "headings": ["✅ Anthropic"],
    "path": "/docs/providers#-anthropic",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["anthropic"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "-anthropic",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers--aws-bedrock",
    "title": "✅ AWS Bedrock",
    "description": "✅ AWS Bedrock",
    "content": "✅ AWS Bedrock",
    "headings": ["✅ AWS Bedrock"],
    "path": "/docs/providers#-aws-bedrock",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["bedrock"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "-aws-bedrock",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers--azure-ai",
    "title": "✅ Azure AI",
    "description": "✅ Azure AI",
    "content": "✅ Azure AI",
    "headings": ["✅ Azure AI"],
    "path": "/docs/providers#-azure-ai",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["azure"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "-azure-ai",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers--google-vertex",
    "title": "✅ Google Vertex",
    "description": "✅ Google Vertex",
    "content": "✅ Google Vertex",
    "headings": ["✅ Google Vertex"],
    "path": "/docs/providers#-google-vertex",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["google", "vertex"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "-google-vertex",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers--llama",
    "title": "✅ Llama",
    "description": "✅ Llama",
    "content": "✅ Llama",
    "headings": ["✅ Llama"],
    "path": "/docs/providers#-llama",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["llama"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "-llama",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers--mistral",
    "title": "✅ Mistral",
    "description": "✅ Mistral",
    "content": "✅ Mistral",
    "headings": ["✅ Mistral"],
    "path": "/docs/providers#-mistral",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["mistral"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "-mistral",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers--openai",
    "title": "✅ OpenAI",
    "description": "✅ OpenAI",
    "content": "✅ OpenAI",
    "headings": ["✅ OpenAI"],
    "path": "/docs/providers#-openai",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["openai"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "-openai",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-observability-advanced-configuration",
    "title": "Advanced Configuration",
    "description": "Advanced Configuration",
    "content": "Advanced Configuration",
    "headings": ["Advanced Configuration"],
    "path": "/docs/observability#advanced-configuration",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["advanced", "configuration"],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "anchor": "advanced-configuration",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-examples-advanced-patterns",
    "title": "Advanced Patterns",
    "description": "Advanced Patterns",
    "content": "Advanced Patterns",
    "headings": ["Advanced Patterns"],
    "path": "/docs/examples#advanced-patterns",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["advanced", "patterns"],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "anchor": "advanced-patterns",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-patterns",
    "title": "Advanced Patterns - Volcano SDK",
    "content": "Advanced Patterns Control flow patterns for building multi-step agents: parallel execution, conditional branching, loops, sub-agent composition, and multi-LLM workflows. Multi-LLM Workflows One of Volcano's most powerful features: use different LLM providers for different steps in the same workflow. Mix and match to leverage each model's strengths. Why use multiple LLMs? Different models excel at different tasks. GPT-4 for complex reasoning, Claude for detailed analysis, Mistral for creative writing, local Llama for cost-effective preprocessing. Volcano makes it seamless. Basic Multi-Provider Workflow [code block] Cost-Optimized Pipeline Use local models for preprocessing, expensive models for critical tasks: [code block] Multi-Provider with MCP Tools Combine different LLMs with automatic MCP tool selection: [code block] Conditional Provider Switching Route to different LLMs based on complexity: [code block] Global Default with Per-Step Overrides [code block] Benefits Best-of-breed: Use the best model for each specific task Cost optimization: Expensive models only where needed Automatic context: Results flow between providers seamlessly A/B testing: Compare model outputs in production Fallback strategies: Switch to backup provider if primary fails Parallel Execution Execute multiple steps simultaneously for faster workflows. Array Mode Run multiple tasks in parallel and get results as an array: [code block] Named Dictionary Mode Access results by key for better organization: [code block] Benefits Speed: Tasks run simultaneously, reducing total execution time Independence: Suitable for independent analysis tasks Organization: Named mode provides structured result access Conditional Branching Route workflows based on conditions. If/Else Branching Binary decision based on a condition: [code block] Switch/Case Branching Handle multiple branches with a default fallback: [code block] Use Cases Email triage and routing Content moderation with different actions Customer support ticket classification Approval workflows Loops Iterate until conditions are met. While Loop Continue until a condition becomes false: [code block] For-Each Loop Process an array of items: [code block] Retry Until Success Self-correcting agents that retry until a success condition is met: [code block] Use Cases Batch processing of items Data pagination and processing Email campaigns Self-correcting content generation Iterative refinement Sub-Agent Composition Build reusable agent components and compose them into larger workflows. Defining Reusable Sub-Agents [code block] Composing Sub-Agents [code block] Complex Composition Mix sub-agents with other patterns: [code block] Benefits Reusability: Define once, use many times Modularity: Each sub-agent has a clear responsibility Testing: Test sub-agents in isolation Maintainability: Changes in one place affect all uses Clarity: High-level workflows read like documentation Combined Patterns Mix and match patterns for powerful workflows: [code block]",
    "headings": [
      "Advanced Patterns",
      "Multi-LLM Workflows",
      "Basic Multi-Provider Workflow",
      "Cost-Optimized Pipeline",
      "Multi-Provider with MCP Tools",
      "Conditional Provider Switching",
      "Global Default with Per-Step Overrides",
      "Benefits",
      "Parallel Execution",
      "Array Mode",
      "Named Dictionary Mode",
      "Benefits",
      "Conditional Branching",
      "If/Else Branching",
      "Switch/Case Branching",
      "Use Cases",
      "Loops",
      "While Loop",
      "For-Each Loop",
      "Retry Until Success",
      "Use Cases",
      "Sub-Agent Composition",
      "Defining Reusable Sub-Agents",
      "Composing Sub-Agents",
      "Complex Composition",
      "Benefits",
      "Combined Patterns"
    ],
    "path": "/docs/patterns",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "advanced",
      "patterns",
      "volcano",
      "[code",
      "block]",
      "different",
      "workflows",
      "multiple",
      "models",
      "until",
      "sub-agents",
      "parallel"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "import { agent, llmOpenAI, llmAnthropic, llmMistral } from \"volcano-sdk\";\n\nconst gpt = llmOpenAI({\n  apiKey: process.env.OPENAI_API_KEY!,\n  model: \"gpt-4o-mini\",\n});\n\nconst claude = llmAnthropic({\n  apiKey: process.env.ANTHROPIC_API_KEY!,\n  model: \"claude-3-5-haiku-20241022\",\n});\n\nconst mistral = llmMistral({\n  apiKey: process.env.MISTRAL_API_KEY!,\n  model: \"mistral-small-latest\",\n});\n\n// Each step uses a different LLM\nawait agent()\n  .then({ llm: gpt, prompt: \"Extract key data from this report...\" })\n  .then({ llm: claude, prompt: \"Analyze the extracted data for patterns\" })\n  .then({ llm: mistral, prompt: \"Write a creative summary in French\" })\n  .run();\n\n// Context flows automatically between steps, regardless of provider\n",
      "import { agent, llmLlama, llmOpenAI } from \"volcano-sdk\";\n\nconst llama = llmLlama({\n  baseURL: \"http://127.0.0.1:11434\",\n  model: \"llama3.2:3b\", // Free, local\n});\n\nconst gpt4 = llmOpenAI({\n  apiKey: process.env.OPENAI_API_KEY!,\n  model: \"gpt-4o\", // Expensive, high-quality\n});\n\n// Process 100 documents\nawait agent()\n  .forEach(\n    documents,\n    (doc, a) => a.then({ llm: llama, prompt: `Summarize: ${doc}` }) // Cheap preprocessing\n  )\n  .then({ llm: gpt4, prompt: \"Analyze all summaries and create final report\" }) // Quality output\n  .run();\n",
      "const database = mcp(\"http://localhost:5000/mcp\");\nconst analytics = mcp(\"http://localhost:5001/mcp\");\n\nawait agent()\n  .then({\n    llm: gpt,\n    prompt: \"Query user data from database\",\n    mcps: [database],\n  })\n  .then({\n    llm: claude,\n    prompt: \"Perform statistical analysis\",\n    mcps: [analytics],\n  })\n  .then({\n    llm: mistral,\n    prompt: \"Generate executive summary\",\n  })\n  .run();\n",
      "await agent()\n  .then({ llm: gpt, prompt: \"Classify this task as SIMPLE or COMPLEX\" })\n  .switch((h) => (h[0].llmOutput?.includes(\"COMPLEX\") ? \"complex\" : \"simple\"), {\n    complex: (a) =>\n      a\n        .then({ llm: gpt4, prompt: \"Handle complex reasoning\" })\n        .then({ llm: claude, prompt: \"Verify the analysis\" }),\n\n    simple: (a) => a.then({ llm: llama, prompt: \"Handle simple task quickly\" }),\n  })\n  .run();\n",
      "// Set default LLM at agent level\nawait agent({ llm: gpt })\n  .then({ prompt: \"Step 1 uses default GPT\" })\n  .then({ llm: claude, prompt: \"Step 2 overrides with Claude\" })\n  .then({ prompt: \"Step 3 back to default GPT\" })\n  .then({ llm: mistral, prompt: \"Step 4 uses Mistral\" })\n  .run();\n",
      "await agent({ llm })\n  .parallel([\n    { prompt: \"Analyze sentiment\" },\n    { prompt: \"Extract entities\" },\n    { prompt: \"Categorize topic\" },\n  ])\n  .then({ prompt: \"Combine all analysis results\" })\n  .run();\n",
      "await agent({ llm })\n  .parallel({\n    sentiment: { prompt: \"What's the sentiment?\" },\n    entities: { prompt: \"Extract key entities\" },\n    summary: { prompt: \"Summarize in 5 words\" },\n  })\n  .then((history) => {\n    const results = history[0].parallel;\n    // Access: results.sentiment, results.entities, results.summary\n    return { prompt: \"Generate report based on analysis\" };\n  })\n  .run();\n",
      "await agent({ llm })\n  .then({ prompt: \"Is this email spam? Reply YES or NO\" })\n  .branch((history) => history[0].llmOutput?.includes(\"YES\") || false, {\n    true: (a) =>\n      a\n        .then({ prompt: \"Categorize spam type\" })\n        .then({ mcp: notifications, tool: \"alert\" }),\n    false: (a) =>\n      a\n        .then({ prompt: \"Extract action items\" })\n        .then({ prompt: \"Draft reply\" }),\n  })\n  .run();\n",
      "await agent({ llm })\n  .then({ prompt: \"Classify ticket priority: HIGH, MEDIUM, or LOW\" })\n  .switch((history) => history[0].llmOutput?.toUpperCase().trim() || \"\", {\n    HIGH: (a) => a.then({ mcp: pagerduty, tool: \"create_incident\" }),\n    MEDIUM: (a) => a.then({ mcp: jira, tool: \"create_ticket\" }),\n    LOW: (a) => a.then({ mcp: email, tool: \"queue_for_review\" }),\n    default: (a) => a.then({ prompt: \"Escalate unknown priority\" }),\n  })\n  .run();\n",
      "await agent({ llm })\n  .while(\n    (history) => {\n      if (history.length === 0) return true;\n      const last = history[history.length - 1];\n      return !last.llmOutput?.includes(\"COMPLETE\");\n    },\n    (a) => a.then({ prompt: \"Process next chunk\", mcps: [database] }),\n    { maxIterations: 10 }\n  )\n  .then({ prompt: \"Generate final summary\" })\n  .run();\n",
      "const customers = [\n  \"alice@example.com\",\n  \"bob@example.com\",\n  \"charlie@example.com\",\n];\n\nawait agent({ llm })\n  .forEach(customers, (email, a) =>\n    a\n      .then({ prompt: `Generate personalized email for ${email}` })\n      .then({ mcp: sendgrid, tool: \"send\", args: { to: email } })\n  )\n  .then({ prompt: \"Summarize campaign results\" })\n  .run();\n",
      "await agent({ llm })\n  .retryUntil(\n    (a) => a.then({ prompt: \"Generate a haiku about AI\" }),\n    (result) => {\n      // Validate 5-7-5 syllable structure\n      const lines = result.llmOutput?.split(\"\\n\") || [];\n      return lines.length === 3; // Simple validation\n    },\n    { maxAttempts: 5, backoff: 1.5 }\n  )\n  .run();\n",
      "// Define specialized sub-agents\nconst emailAnalyzer = agent({ llm: claude })\n  .then({ prompt: \"Extract sender intent\" })\n  .then({ prompt: \"Classify urgency level\" });\n\nconst responseGenerator = agent({ llm: openai })\n  .then({ prompt: \"Draft professional response\" })\n  .then({ prompt: \"Add signature\" });\n\nconst qualityChecker = agent({ llm: mistral })\n  .then({ prompt: \"Check response quality\" })\n  .then({ prompt: \"Suggest improvements\" });\n",
      "// Compose them in a larger workflow\nawait agent({ llm })\n  .then({ mcp: gmail, tool: \"fetch_unread\" })\n  .runAgent(emailAnalyzer) // Run first sub-agent\n  .runAgent(responseGenerator) // Run second sub-agent\n  .runAgent(qualityChecker) // Run third sub-agent\n  .then({ mcp: gmail, tool: \"send_reply\" })\n  .run();\n",
      "const contentAnalyzer = agent({ llm }).parallel({\n  sentiment: { prompt: \"Analyze sentiment\" },\n  topics: { prompt: \"Extract main topics\" },\n  tone: { prompt: \"Determine tone\" },\n});\n\nconst contentModerator = agent({ llm })\n  .then({ prompt: \"Check for policy violations\" })\n  .branch((h) => h[0].llmOutput?.includes(\"VIOLATION\") || false, {\n    true: (a) => a.then({ mcp: moderation, tool: \"flag_content\" }),\n    false: (a) => a.then({ prompt: \"Approve content\" }),\n  });\n\n// Main workflow\nawait agent({ llm })\n  .then({ mcp: cms, tool: \"fetch_pending_posts\" })\n  .forEach(posts, (post, a) =>\n    a.runAgent(contentAnalyzer).runAgent(contentModerator)\n  )\n  .then({ prompt: \"Generate moderation report\" })\n  .run();\n",
      "await agent({ llm })\n  // Parallel analysis\n  .parallel({\n    sentiment: { prompt: \"Analyze sentiment\" },\n    intent: { prompt: \"Extract intent\" },\n    priority: { prompt: \"Determine priority\" },\n  })\n\n  // Route based on priority\n  .switch((h) => h[0].parallel?.priority.llmOutput?.trim() || \"\", {\n    URGENT: (a) =>\n      a.then({ mcp: slack, tool: \"alert_team\" }).runAgent(escalationAgent),\n\n    NORMAL: (a) =>\n      a.forEach(responders, (person, ag) =>\n        ag.then({ prompt: `Assign to ${person}` })\n      ),\n\n    default: (a) => a.then({ prompt: \"Queue for review\" }),\n  })\n\n  // Final step\n  .then({ prompt: \"Log outcome\" })\n  .run();\n"
    ]
  },
  {
    "id": "Documentation-examples-advanced-patternsts",
    "title": "advanced-patterns.ts",
    "description": "advanced-patterns.ts Combine parallel execution, branching, and loops with automatic MCP tool selection throughout. [code block] Demonstrates: Complex",
    "content": "advanced-patterns.ts Combine parallel execution, branching, and loops with automatic MCP tool selection throughout. [code block] Demonstrates: Complex workflows mixing all control flow patterns with intelligent tool selection View source on GitHub →",
    "headings": ["advanced-patterns.ts"],
    "path": "/docs/examples#advanced-patternsts",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "advanced-patterns.ts",
      "selection",
      "combine",
      "parallel",
      "execution,",
      "branching,",
      "loops",
      "automatic",
      "throughout.",
      "[code"
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "codeBlocks": ["npx tsx examples/advanced-patterns.ts\n"],
    "anchor": "advanced-patternsts",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-agent-level-authentication",
    "title": "Agent-Level Authentication",
    "description": "Agent-Level Authentication Configure auth centrally at the agent level for cleaner code when using multiple authenticated servers: [code block] Preced",
    "content": "Agent-Level Authentication Configure auth centrally at the agent level for cleaner code when using multiple authenticated servers: [code block] Precedence: Handle-level auth takes precedence over agent-level auth. This allows you to set defaults at the agent level and override for specific handles.",
    "headings": ["Agent-Level Authentication"],
    "path": "/docs/mcp-tools#agent-level-authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "agent-level",
      "authentication",
      "agent",
      "level",
      "configure",
      "centrally",
      "cleaner",
      "using",
      "multiple",
      "authenticated"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "// MCP handles without auth\nconst mcp1 = mcp(\"https://api.example.com/mcp\");\nconst mcp2 = mcp(\"https://api.other.com/mcp\");\n\n// Auth configured at agent level\nawait agent({\n  llm,\n  mcpAuth: {\n    \"https://api.example.com/mcp\": {\n      type: \"oauth\",\n      clientId: process.env.CLIENT_ID_1!,\n      clientSecret: process.env.CLIENT_SECRET_1!,\n      tokenEndpoint: \"https://api.example.com/oauth/token\",\n    },\n    \"https://api.other.com/mcp\": {\n      type: \"bearer\",\n      token: process.env.TOKEN_2!,\n    },\n  },\n})\n  .then({ prompt: \"Use tools from both servers\", mcps: [mcp1, mcp2] })\n  .run();\n"
    ],
    "anchor": "agent-level-authentication",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-api-agentoptions-agentbuilder",
    "title": "agent(options?): AgentBuilder",
    "description": "agent(options?): AgentBuilder Create an agent workflow builder. [code block]",
    "content": "agent(options?): AgentBuilder Create an agent workflow builder. [code block]",
    "headings": ["agent(options?): AgentBuilder"],
    "path": "/docs/api#agentoptions-agentbuilder",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "agent(options?):",
      "agentbuilder",
      "create",
      "agent",
      "workflow",
      "builder.",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "import { agent, llmOpenAI } from \"volcano-sdk\";\n\nconst llm = llmOpenAI({ apiKey: process.env.OPENAI_API_KEY! });\n\nconst myAgent = agent({\n  llm,\n  instructions: \"You are a helpful assistant\",\n  timeout: 60,\n  retry: { retries: 3 },\n});\n"
    ],
    "anchor": "agentoptions-agentbuilder",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-providers-anthropic-claude-provider",
    "title": "Anthropic (Claude) Provider",
    "description": "Anthropic (Claude) Provider Native support for Claude models with tool calling. [code block]",
    "content": "Anthropic (Claude) Provider Native support for Claude models with tool calling. [code block]",
    "headings": ["Anthropic (Claude) Provider"],
    "path": "/docs/providers#anthropic-claude-provider",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "anthropic",
      "(claude)",
      "provider",
      "native",
      "support",
      "claude",
      "models",
      "calling.",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "import { llmAnthropic } from \"volcano-sdk\";\n\nconst claude = llmAnthropic({\n  apiKey: process.env.ANTHROPIC_API_KEY!,\n  model: \"claude-3-5-sonnet-20241022\",\n  baseURL: \"https://api.anthropic.com\", // Optional\n  version: \"2023-06-01\", // Optional\n  options: {\n    temperature: 0.7,\n    max_tokens: 2000,\n    top_k: 50,\n    top_p: 0.9,\n    stop_sequences: [\"\\n\\n\"],\n  },\n});\n"
    ],
    "anchor": "anthropic-claude-provider",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-api",
    "title": "API Reference - Volcano SDK",
    "content": "API Reference API documentation for Volcano SDK: agent creation, step types, results, and utility functions. agent(options?): AgentBuilder Create an agent workflow builder. [code block] Options Option Type Default Description ----------------------- ----------- ---------------- ---------------------------------------- llm LLMHandle - Default LLM provider for all steps instructions string - Global system instructions timeout number 60 Default timeout per step (seconds) retry RetryConfig Retry configuration contextMaxChars number 20480 Soft cap for injected context size contextMaxToolResults number 8 Number of recent tool results to include Methods Basic Methods Method Description ----------------- ------------------------------------------------ .then(step) Add a sequential step to the workflow .resetHistory() Clear context/history for subsequent steps .run(log?) Execute workflow and return all results .stream(log?) Execute workflow and yield results incrementally Advanced Pattern Methods Method Description ----------------------------------- ----------------------------------------------- .parallel(steps) Execute steps concurrently (array or dict mode) .branch(condition, branches) Conditional if/else routing .switch(selector, cases) Multi-way branching with default .while(condition, body, opts?) Loop until condition is false .forEach(items, body) Iterate over array of items .retryUntil(body, success, opts?) Retry until success condition is met .runAgent(subAgent) Compose and run sub-agents Step Types Volcano SDK supports three types of steps: LLM-Only Step Generate text with an LLM without tools: [code block] Automatic MCP Tool Selection Let the LLM automatically choose which tools to call: [code block] Explicit MCP Tool Call Call a specific tool directly: [code block] Common Step Fields Field Type Description -------------- ----------- ---------------------------------------------------- llm LLMHandle Override the LLM for this step instructions string Override system instructions for this step timeout number Step timeout in seconds (overrides agent default) retry RetryConfig Retry config for this step (overrides agent default) pre () => void Hook called before step execution post () => void Hook called after successful step execution Step Results Each step returns a StepResult object with execution details: [code block] Example Usage [code block] RetryConfig Configuration for retry behavior: [code block] Examples [code block] Utility Functions mcp(url, options?): MCPHandle Create an MCP handle for a tool server with optional authentication: [code block] MCP Specification: Per the official MCP spec, authentication uses OAuth 2.1. Volcano supports OAuth (automatic token acquisition) and Bearer (custom tokens). discoverTools(handles): Promise\\<ToolDefinition\\[\\]\\> Manually discover tools from MCP servers: [code block] Type Reference LLMHandle [code block] MCPHandle [code block] ToolDefinition [code block]",
    "headings": [
      "API Reference",
      "agent(options?): AgentBuilder",
      "Options",
      "Methods",
      "Basic Methods",
      "Advanced Pattern Methods",
      "Step Types",
      "LLM-Only Step",
      "Automatic MCP Tool Selection",
      "Explicit MCP Tool Call",
      "Common Step Fields",
      "Step Results",
      "Example Usage",
      "RetryConfig",
      "Examples",
      "Utility Functions",
      "mcp(url, options?): MCPHandle",
      "discoverTools(handles): Promise\\<ToolDefinition\\[\\]\\>",
      "Type Reference",
      "LLMHandle",
      "MCPHandle",
      "ToolDefinition"
    ],
    "path": "/docs/api",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "reference",
      "volcano",
      "[code",
      "block]",
      "retry",
      "number",
      "agent",
      "workflow",
      "default",
      "description",
      "instructions",
      "timeout"
    ],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "import { agent, llmOpenAI } from \"volcano-sdk\";\n\nconst llm = llmOpenAI({ apiKey: process.env.OPENAI_API_KEY! });\n\nconst myAgent = agent({\n  llm,\n  instructions: \"You are a helpful assistant\",\n  timeout: 60,\n  retry: { retries: 3 },\n});\n",
      "{\n  prompt: string;\n  llm?: LLMHandle;\n  instructions?: string;\n  timeout?: number;\n  retry?: RetryConfig;\n  contextMaxChars?: number;\n  contextMaxToolResults?: number;\n  pre?: () => void;\n  post?: () => void;\n}\n",
      "{\n  prompt: string;\n  mcps: MCPHandle[];\n  llm?: LLMHandle;\n  instructions?: string;\n  timeout?: number;\n  retry?: RetryConfig;\n  contextMaxChars?: number;\n  contextMaxToolResults?: number;\n  pre?: () => void;\n  post?: () => void;\n}\n",
      "{\n  mcp: MCPHandle;\n  tool: string;\n  args?: Record<string, any>;\n  prompt?: string;         // Optional LLM step before tool\n  llm?: LLMHandle;\n  instructions?: string;\n  timeout?: number;\n  retry?: RetryConfig;\n  contextMaxChars?: number;\n  contextMaxToolResults?: number;\n  pre?: () => void;\n  post?: () => void;\n}\n",
      "type StepResult = {\n  // Basic fields\n  prompt?: string;\n  llmOutput?: string;\n\n  // Timing metrics (milliseconds)\n  durationMs?: number; // Total step wall time\n  llmMs?: number; // Time spent in LLM calls\n\n  // MCP tool results (explicit call)\n  mcp?: {\n    endpoint: string;\n    tool: string;\n    result: any;\n    ms?: number; // Tool execution time\n  };\n\n  // MCP tool calls (automatic selection)\n  toolCalls?: Array<{\n    name: string;\n    endpoint: string;\n    result: any;\n    ms?: number; // Tool execution time\n  }>;\n\n  // Parallel execution results\n  parallel?: Record<string, StepResult>;\n  parallelResults?: StepResult[];\n\n  // Aggregated metrics (on final step only)\n  totalDurationMs?: number;\n  totalLlmMs?: number;\n  totalMcpMs?: number;\n};\n",
      "const results = await agent({ llm })\n  .then({ prompt: \"Analyze sentiment\", mcps: [analytics] })\n  .then({ prompt: \"Generate summary\" })\n  .run();\n\n// Access first step results\nconsole.log(results[0].prompt); // \"Analyze sentiment\"\nconsole.log(results[0].llmOutput); // LLM response\nconsole.log(results[0].toolCalls); // Tools that were called\nconsole.log(results[0].durationMs); // Step duration\nconsole.log(results[0].llmMs); // LLM time\n\n// Access aggregated metrics (on last step)\nconst last = results[results.length - 1];\nconsole.log(last.totalDurationMs); // Total workflow time\nconsole.log(last.totalLlmMs); // Total LLM time\nconsole.log(last.totalMcpMs); // Total MCP time\n",
      "type RetryConfig = {\n  delay?: number; // Seconds to wait between retries (mutually exclusive with backoff)\n  backoff?: number; // Exponential factor (waits 1s, factor^n each retry)\n  retries?: number; // Total attempts including first (default: 3)\n};\n",
      "// Immediate retry\n{ retries: 3 }\n\n// Delayed retry (wait 20s between attempts)\n{ delay: 20, retries: 3 }\n\n// Exponential backoff (1s, 2s, 4s, 8s)\n{ backoff: 2, retries: 4 }\n",
      "import { mcp } from \"volcano-sdk\";\n\n// No authentication\nconst astro = mcp(\"http://localhost:3211/mcp\");\n\n// OAuth authentication\nconst protectedMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"oauth\",\n    clientId: \"your-client-id\",\n    clientSecret: \"your-client-secret\",\n    tokenEndpoint: \"https://api.example.com/oauth/token\",\n  },\n});\n\n// Bearer token\nconst bearerMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"bearer\",\n    token: \"your-bearer-token\",\n  },\n});\n",
      "import { discoverTools, mcp } from \"volcano-sdk\";\n\nconst handles = [\n  mcp(\"http://localhost:3211/mcp\"),\n  mcp(\"http://localhost:3212/mcp\"),\n];\n\nconst tools = await discoverTools(handles);\nconsole.log(tools); // Array of tool definitions\n",
      "type LLMHandle = {\n  model: string;\n  gen(prompt: string): Promise<string>;\n  genWithTools(prompt: string, tools: ToolDefinition[]): Promise<LLMToolResult>;\n  genStream?(prompt: string): AsyncGenerator<string, void, unknown>;\n};\n",
      "type MCPHandle = {\n  id: string;\n  url: string;\n};\n",
      "type ToolDefinition = {\n  name: string;\n  description: string;\n  parameters: any; // JSON Schema\n  mcpHandle?: MCPHandle;\n};\n"
    ]
  },
  {
    "id": "Documentation-patterns-array-mode",
    "title": "Array Mode",
    "description": "Array Mode Run multiple tasks in parallel and get results as an array: [code block]",
    "content": "Array Mode Run multiple tasks in parallel and get results as an array: [code block]",
    "headings": ["Array Mode"],
    "path": "/docs/patterns#array-mode",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "array",
      "mode",
      "multiple",
      "tasks",
      "parallel",
      "results",
      "array:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "await agent({ llm })\n  .parallel([\n    { prompt: \"Analyze sentiment\" },\n    { prompt: \"Extract entities\" },\n    { prompt: \"Categorize topic\" },\n  ])\n  .then({ prompt: \"Combine all analysis results\" })\n  .run();\n"
    ],
    "anchor": "array-mode",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-providers-authentication",
    "title": "Authentication",
    "description": "Authentication Method Required Description --------- ------------ -------------------------------------------------------- apiKey Required OpenAI API ",
    "content": "Authentication Method Required Description --------- ------------ -------------------------------------------------------- apiKey Required OpenAI API key from platform.openai.com baseURL Optional Custom API endpoint (default: https://api.openai.com/v1)",
    "headings": ["Authentication"],
    "path": "/docs/providers#authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "authentication",
      "required",
      "method",
      "description",
      "---------",
      "------------",
      "--------------------------------------------------------",
      "apikey",
      "openai",
      "platform.openai.com"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "authentication",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-authentication",
    "title": "Authentication",
    "description": "Authentication Method Required Description --------- ------------ -------------------------------------------------------- apiKey Required Anthropic A",
    "content": "Authentication Method Required Description --------- ------------ -------------------------------------------------------- apiKey Required Anthropic API key from console.anthropic.com baseURL Optional Custom API endpoint (default: https://api.anthropic.com) version Optional API version header (default: 2023-06-01)",
    "headings": ["Authentication"],
    "path": "/docs/providers#authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "authentication",
      "required",
      "optional",
      "(default:",
      "version",
      "method",
      "description",
      "---------",
      "------------",
      "--------------------------------------------------------"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "authentication",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-authentication",
    "title": "Authentication",
    "description": "Authentication Method Required Description --------- ------------ ----------------------------------------------------- apiKey Required Mistral API ke",
    "content": "Authentication Method Required Description --------- ------------ ----------------------------------------------------- apiKey Required Mistral API key from console.mistral.ai baseURL Optional Custom API endpoint (default: https://api.mistral.ai)",
    "headings": ["Authentication"],
    "path": "/docs/providers#authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "authentication",
      "required",
      "method",
      "description",
      "---------",
      "------------",
      "-----------------------------------------------------",
      "apikey",
      "mistral",
      "console.mistral.ai"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "authentication",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-authentication",
    "title": "Authentication",
    "description": "Authentication Method Required Description --------- ---------- ----------------------------------------------------------------------- baseURL Option",
    "content": "Authentication Method Required Description --------- ---------- ----------------------------------------------------------------------- baseURL Optional OpenAI-compatible endpoint (default: http://localhost:11434 for Ollama) apiKey Optional API key if your endpoint requires authentication",
    "headings": ["Authentication"],
    "path": "/docs/providers#authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "authentication",
      "optional",
      "endpoint",
      "method",
      "required",
      "description",
      "---------",
      "----------",
      "-----------------------------------------------------------------------",
      "baseurl"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "authentication",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-authentication",
    "title": "Authentication",
    "description": "Authentication AWS Bedrock supports multiple authentication methods (in priority order): Method Parameters Description -------------------- ----------",
    "content": "Authentication AWS Bedrock supports multiple authentication methods (in priority order): Method Parameters Description -------------------- ------------------------------------------------ ------------------------------------------------------- Explicit Credentials accessKeyId, secretAccessKey, sessionToken Directly provide AWS credentials Bearer Token bearerToken Use bearer token authentication AWS Profile profile Use credentials from ~/.aws/credentials IAM Role roleArn Assume an IAM role Default Chain (none) Environment variables, instance profiles, ECS/EKS roles",
    "headings": ["Authentication"],
    "path": "/docs/providers#authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "authentication",
      "credentials",
      "bearer",
      "token",
      "profile",
      "bedrock",
      "supports",
      "multiple",
      "methods",
      "priority"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "authentication",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-authentication",
    "title": "Authentication",
    "description": "Authentication Method Required Description --------- ------------ ------------------------------------------------------------------- apiKey Required ",
    "content": "Authentication Method Required Description --------- ------------ ------------------------------------------------------------------- apiKey Required Google AI Studio API key baseURL Optional Custom API endpoint (default: https://aiplatform.googleapis.com/v1)",
    "headings": ["Authentication"],
    "path": "/docs/providers#authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "authentication",
      "required",
      "method",
      "description",
      "---------",
      "------------",
      "-------------------------------------------------------------------",
      "apikey",
      "google",
      "studio"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "authentication",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-authentication",
    "title": "Authentication",
    "description": "Authentication Azure AI supports three authentication methods (in priority order): Method Parameters Description ------------------------ ------------",
    "content": "Authentication Azure AI supports three authentication methods (in priority order): Method Parameters Description ------------------------ ------------- -------------------------------------------------------------------- API Key apiKey Simplest method - use your Azure resource key Entra ID Token accessToken Use Microsoft Entra ID (Azure AD) access token Default Credential Chain (none) Uses Azure SDK: Managed Identity, Service Principal, CLI credentials",
    "headings": ["Authentication"],
    "path": "/docs/providers#authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "authentication",
      "azure",
      "method",
      "entra",
      "token",
      "supports",
      "three",
      "methods",
      "priority",
      "order):"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "authentication",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-authentication-examples",
    "title": "Authentication Examples",
    "description": "Authentication Examples [code block]",
    "content": "Authentication Examples [code block]",
    "headings": ["Authentication Examples"],
    "path": "/docs/providers#authentication-examples",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["authentication", "examples", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "// 1. Explicit credentials\nconst bedrock1 = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  accessKeyId: process.env.AWS_ACCESS_KEY_ID!,\n  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,\n  sessionToken: process.env.AWS_SESSION_TOKEN, // Optional\n});\n\n// 2. Bearer token\nconst bedrock2 = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  bearerToken: process.env.AWS_BEARER_TOKEN_BEDROCK!,\n});\n\n// 3. AWS Profile\nconst bedrock3 = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  profile: \"my-aws-profile\",\n});\n\n// 4. IAM Role\nconst bedrock4 = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  roleArn: \"arn:aws:iam::123456789012:role/my-bedrock-role\",\n});\n\n// 5. Default chain (recommended)\nconst bedrock5 = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  // Automatically uses environment, instance profiles, etc.\n});\n"
    ],
    "anchor": "authentication-examples",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-authentication-examples",
    "title": "Authentication Examples",
    "description": "Authentication Examples [code block]",
    "content": "Authentication Examples [code block]",
    "headings": ["Authentication Examples"],
    "path": "/docs/providers#authentication-examples",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["authentication", "examples", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "// 1. API Key (simplest)\nconst azure1 = llmAzure({\n  model: \"gpt-4o-mini\",\n  endpoint: \"https://your-resource.openai.azure.com/openai/responses\",\n  apiKey: process.env.AZURE_AI_API_KEY!,\n});\n\n// 2. Entra ID Access Token\nconst azure2 = llmAzure({\n  model: \"gpt-4o-mini\",\n  endpoint: \"https://your-resource.openai.azure.com/openai/responses\",\n  accessToken: process.env.AZURE_ACCESS_TOKEN!,\n});\n\n// 3. Azure Default Credential Chain\nconst azure3 = llmAzure({\n  model: \"gpt-4o-mini\",\n  endpoint: \"https://your-resource.openai.azure.com/openai/responses\",\n  // Uses Azure SDK credential providers automatically\n});\n"
    ],
    "anchor": "authentication-examples",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-authentication-features",
    "title": "Authentication Features",
    "description": "Authentication Features OAuth token caching: Tokens are cached and reused until expiration (60s buffer) Automatic refresh: Expired tokens are refreshe",
    "content": "Authentication Features OAuth token caching: Tokens are cached and reused until expiration (60s buffer) Automatic refresh: Expired tokens are refreshed automatically Per-endpoint configuration: Each MCP server can have different auth Connection pooling: Authenticated connections are pooled separately Spec compliant: Follows MCP OAuth 2.1 authentication standard",
    "headings": ["Authentication Features"],
    "path": "/docs/mcp-tools#authentication-features",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "authentication",
      "features",
      "oauth",
      "tokens",
      "token",
      "caching:",
      "cached",
      "reused",
      "until",
      "expiration"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "anchor": "authentication-features",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-api-automatic-mcp-tool-selection",
    "title": "Automatic MCP Tool Selection",
    "description": "Automatic MCP Tool Selection Let the LLM automatically choose which tools to call: [code block]",
    "content": "Automatic MCP Tool Selection Let the LLM automatically choose which tools to call: [code block]",
    "headings": ["Automatic MCP Tool Selection"],
    "path": "/docs/api#automatic-mcp-tool-selection",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "automatic",
      "tool",
      "selection",
      "automatically",
      "choose",
      "tools",
      "call:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "{\n  prompt: string;\n  mcps: MCPHandle[];\n  llm?: LLMHandle;\n  instructions?: string;\n  timeout?: number;\n  retry?: RetryConfig;\n  contextMaxChars?: number;\n  contextMaxToolResults?: number;\n  pre?: () => void;\n  post?: () => void;\n}\n"
    ],
    "anchor": "automatic-mcp-tool-selection",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-automatic-tool-selection",
    "title": "Automatic Tool Selection",
    "description": "Automatic Tool Selection Let the LLM intelligently choose which tools to call based on the prompt. This is the recommended approach for most use cases",
    "content": "Automatic Tool Selection Let the LLM intelligently choose which tools to call based on the prompt. This is the recommended approach for most use cases. [code block]",
    "headings": ["Automatic Tool Selection"],
    "path": "/docs/mcp-tools#automatic-tool-selection",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "automatic",
      "tool",
      "selection",
      "intelligently",
      "choose",
      "tools",
      "based",
      "prompt.",
      "recommended",
      "approach",
      "cases."
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "import { agent, llmOpenAI, mcp } from \"volcano-sdk\";\n\nconst weather = mcp(\"http://localhost:3000/mcp\");\nconst notifications = mcp(\"http://localhost:4000/mcp\");\nconst llm = llmOpenAI({ apiKey: process.env.OPENAI_API_KEY! });\n\nawait agent({ llm })\n  .then({\n    prompt: \"Check SF weather for tomorrow and send me a notification.\",\n    mcps: [weather, notifications],\n  })\n  .run();\n\n// The LLM automatically:\n// 1. Discovers available tools from both servers\n// 2. Selects the appropriate tools\n// 3. Calls them with correct arguments\n// 4. Returns the results\n"
    ],
    "anchor": "automatic-tool-selection",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-examples-automaticts",
    "title": "automatic.ts",
    "description": "automatic.ts Automatic MCP tool selection - let the LLM choose which tools to call based on the prompt. [code block] Demonstrates: Automatic tool disc",
    "content": "automatic.ts Automatic MCP tool selection - let the LLM choose which tools to call based on the prompt. [code block] Demonstrates: Automatic tool discovery and selection with mcps: [mcp1, mcp2] View source on GitHub →",
    "headings": ["automatic.ts"],
    "path": "/docs/examples#automaticts",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "automatic.ts",
      "automatic",
      "selection",
      "choose",
      "tools",
      "based",
      "prompt.",
      "[code",
      "block]",
      "demonstrates:"
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "codeBlocks": ["npx tsx examples/automatic.ts\n"],
    "anchor": "automaticts",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-observability-available-metrics",
    "title": "Available Metrics",
    "description": "Available Metrics Metric Type Description ------------------------- --------- --------------------------------------------------------- volcano.agent.",
    "content": "Available Metrics Metric Type Description ------------------------- --------- --------------------------------------------------------- volcano.agent.duration Histogram Total agent execution time (ms) volcano.step.duration Histogram Individual step duration (ms), labeled by type volcano.llm.calls.total Counter Total LLM API calls, labeled by provider and error status volcano.mcp.calls.total Counter Total MCP tool calls volcano.errors.total Counter Total errors by type and provider",
    "headings": ["Available Metrics"],
    "path": "/docs/observability#available-metrics",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "available",
      "metrics",
      "total",
      "counter",
      "histogram",
      "labeled",
      "provider",
      "metric",
      "description",
      "-------------------------"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "anchor": "available-metrics",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-providers-aws-bedrock-provider",
    "title": "AWS Bedrock Provider",
    "description": "AWS Bedrock Provider Access foundation models via AWS Bedrock with native tool calling support using the Converse API. [code block]",
    "content": "AWS Bedrock Provider Access foundation models via AWS Bedrock with native tool calling support using the Converse API. [code block]",
    "headings": ["AWS Bedrock Provider"],
    "path": "/docs/providers#aws-bedrock-provider",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "bedrock",
      "provider",
      "access",
      "foundation",
      "models",
      "native",
      "calling",
      "support",
      "using",
      "converse"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "import { llmBedrock } from \"volcano-sdk\";\n\nconst bedrock = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  // Uses AWS credential chain by default\n  options: {\n    temperature: 0.7,\n    max_tokens: 2000,\n    top_p: 0.9,\n    stop_sequences: [\"\\n\\n\"],\n  },\n});\n"
    ],
    "anchor": "aws-bedrock-provider",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-azure-ai-provider",
    "title": "Azure AI Provider",
    "description": "Azure AI Provider Azure OpenAI Service with enterprise authentication via the Responses API. [code block]",
    "content": "Azure AI Provider Azure OpenAI Service with enterprise authentication via the Responses API. [code block]",
    "headings": ["Azure AI Provider"],
    "path": "/docs/providers#azure-ai-provider",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "azure",
      "provider",
      "openai",
      "service",
      "enterprise",
      "authentication",
      "responses",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "import { llmAzure } from \"volcano-sdk\";\n\nconst azure = llmAzure({\n  model: \"gpt-4o-mini\",\n  endpoint: \"https://your-resource.openai.azure.com/openai/responses\",\n  apiKey: process.env.AZURE_AI_API_KEY!,\n  apiVersion: \"2025-04-01-preview\", // Optional\n});\n"
    ],
    "anchor": "azure-ai-provider",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-examples-basic-examples",
    "title": "Basic Examples",
    "description": "Basic Examples",
    "content": "Basic Examples",
    "headings": ["Basic Examples"],
    "path": "/docs/examples#basic-examples",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["basic", "examples"],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "anchor": "basic-examples",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-basic-multi-provider-workflow",
    "title": "Basic Multi-Provider Workflow",
    "description": "Basic Multi-Provider Workflow [code block]",
    "content": "Basic Multi-Provider Workflow [code block]",
    "headings": ["Basic Multi-Provider Workflow"],
    "path": "/docs/patterns#basic-multi-provider-workflow",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["basic", "multi-provider", "workflow", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "import { agent, llmOpenAI, llmAnthropic, llmMistral } from \"volcano-sdk\";\n\nconst gpt = llmOpenAI({\n  apiKey: process.env.OPENAI_API_KEY!,\n  model: \"gpt-4o-mini\",\n});\n\nconst claude = llmAnthropic({\n  apiKey: process.env.ANTHROPIC_API_KEY!,\n  model: \"claude-3-5-haiku-20241022\",\n});\n\nconst mistral = llmMistral({\n  apiKey: process.env.MISTRAL_API_KEY!,\n  model: \"mistral-small-latest\",\n});\n\n// Each step uses a different LLM\nawait agent()\n  .then({ llm: gpt, prompt: \"Extract key data from this report...\" })\n  .then({ llm: claude, prompt: \"Analyze the extracted data for patterns\" })\n  .then({ llm: mistral, prompt: \"Write a creative summary in French\" })\n  .run();\n\n// Context flows automatically between steps, regardless of provider\n"
    ],
    "anchor": "basic-multi-provider-workflow",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-examples-basicts",
    "title": "basic.ts",
    "description": "basic.ts Simple single-step agent with LLM-only generation. [code block] Demonstrates: Agent creation, basic .then() and .run() View source on GitHub ",
    "content": "basic.ts Simple single-step agent with LLM-only generation. [code block] Demonstrates: Agent creation, basic .then() and .run() View source on GitHub →",
    "headings": ["basic.ts"],
    "path": "/docs/examples#basicts",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "basic.ts",
      "agent",
      "simple",
      "single-step",
      "llm-only",
      "generation.",
      "[code",
      "block]",
      "demonstrates:",
      "creation,"
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "codeBlocks": ["npx tsx examples/basic.ts\n"],
    "anchor": "basicts",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-bearer-token-authentication",
    "title": "Bearer Token Authentication",
    "description": "Bearer Token Authentication For pre-acquired tokens or custom authentication flows: [code block]",
    "content": "Bearer Token Authentication For pre-acquired tokens or custom authentication flows: [code block]",
    "headings": ["Bearer Token Authentication"],
    "path": "/docs/mcp-tools#bearer-token-authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "bearer",
      "token",
      "authentication",
      "pre-acquired",
      "tokens",
      "custom",
      "flows:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "const authMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"bearer\",\n    token: process.env.MCP_BEARER_TOKEN!,\n  },\n});\n\nawait agent({ llm })\n  .then({ mcp: authMcp, tool: \"secure_action\", args: {} })\n  .run();\n"
    ],
    "anchor": "bearer-token-authentication",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-benefits",
    "title": "Benefits",
    "description": "Benefits Best-of-breed: Use the best model for each specific task Cost optimization: Expensive models only where needed Automatic context: Results flo",
    "content": "Benefits Best-of-breed: Use the best model for each specific task Cost optimization: Expensive models only where needed Automatic context: Results flow between providers seamlessly A/B testing: Compare model outputs in production Fallback strategies: Switch to backup provider if primary fails",
    "headings": ["Benefits"],
    "path": "/docs/patterns#benefits",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "benefits",
      "model",
      "best-of-breed:",
      "specific",
      "optimization:",
      "expensive",
      "models",
      "needed",
      "automatic",
      "context:"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "anchor": "benefits",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-benefits",
    "title": "Benefits",
    "description": "Benefits Speed: Tasks run simultaneously, reducing total execution time Independence: Suitable for independent analysis tasks Organization: Named mode",
    "content": "Benefits Speed: Tasks run simultaneously, reducing total execution time Independence: Suitable for independent analysis tasks Organization: Named mode provides structured result access",
    "headings": ["Benefits"],
    "path": "/docs/patterns#benefits",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "benefits",
      "tasks",
      "speed:",
      "simultaneously,",
      "reducing",
      "total",
      "execution",
      "independence:",
      "suitable",
      "independent"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "anchor": "benefits",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-benefits",
    "title": "Benefits",
    "description": "Benefits Reusability: Define once, use many times Modularity: Each sub-agent has a clear responsibility Testing: Test sub-agents in isolation Maintain",
    "content": "Benefits Reusability: Define once, use many times Modularity: Each sub-agent has a clear responsibility Testing: Test sub-agents in isolation Maintainability: Changes in one place affect all uses Clarity: High-level workflows read like documentation",
    "headings": ["Benefits"],
    "path": "/docs/patterns#benefits",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "benefits",
      "reusability:",
      "define",
      "once,",
      "times",
      "modularity:",
      "sub-agent",
      "clear",
      "responsibility",
      "testing:"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "anchor": "benefits",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-observability-best-practices",
    "title": "Best Practices",
    "description": "Best Practices Production only: Enable telemetry in production, disable in development for faster iteration Sampling: Use OTEL sampling for high-traff",
    "content": "Best Practices Production only: Enable telemetry in production, disable in development for faster iteration Sampling: Use OTEL sampling for high-traffic applications to reduce costs Service naming: Use descriptive service names for easier filtering Label cardinality: Be mindful of high-cardinality labels (user IDs, etc.) Monitor costs: Observability backends charge by data volume - sample appropriately Performance: Telemetry adds minimal overhead (~1-5ms per workflow) but can increase network traffic to your observability backend. Use sampling for high-throughput applications.",
    "headings": ["Best Practices"],
    "path": "/docs/observability#best-practices",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "best",
      "practices",
      "telemetry",
      "sampling",
      "service",
      "observability",
      "production",
      "only:",
      "enable",
      "production,",
      "disable"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "anchor": "best-practices",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-examples-branchingts-branching-simplets",
    "title": "branching.ts & branching-simple.ts",
    "description": "branching.ts & branching-simple.ts Conditional routing with .branch() and .switch() using automatic MCP tool selection. [code block] Demonstrates: If/",
    "content": "branching.ts & branching-simple.ts Conditional routing with .branch() and .switch() using automatic MCP tool selection. [code block] Demonstrates: If/else branching and switch/case patterns, LLM-driven tool selection View source on GitHub →",
    "headings": ["branching.ts & branching-simple.ts"],
    "path": "/docs/examples#branchingts-branching-simplets",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "branching.ts",
      "branching-simple.ts",
      "conditional",
      "routing",
      ".branch()",
      ".switch()",
      "using",
      "automatic",
      "selection.",
      "[code"
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "codeBlocks": [
      "npx tsx examples/branching-simple.ts\nnpx tsx examples/branching.ts\n"
    ],
    "anchor": "branchingts-branching-simplets",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-features-characteristics",
    "title": "Characteristics",
    "description": "Characteristics Waits for completion: Returns only after all steps finish Aggregated metrics: Final step includes total duration, LLM time, and MCP ti",
    "content": "Characteristics Waits for completion: Returns only after all steps finish Aggregated metrics: Final step includes total duration, LLM time, and MCP time Error handling: Throws on first failure (use try/catch) Sequential execution: Steps run in order, one after another Full results: Access all step data for analysis or debugging",
    "headings": ["Characteristics"],
    "path": "/docs/features#characteristics",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "characteristics",
      "after",
      "steps",
      "waits",
      "completion:",
      "returns",
      "finish",
      "aggregated",
      "metrics:",
      "final"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "characteristics",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-features-characteristics",
    "title": "Characteristics",
    "description": "Characteristics Immediate feedback: First result available when first step completes Memory efficient: Results can be processed and discarded incremen",
    "content": "Characteristics Immediate feedback: First result available when first step completes Memory efficient: Results can be processed and discarded incrementally Progressive display: Results appear as they complete Same execution model: Retries, timeouts, and validation work identically to run()",
    "headings": ["Characteristics"],
    "path": "/docs/features#characteristics",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "characteristics",
      "first",
      "results",
      "immediate",
      "feedback:",
      "result",
      "available",
      "completes",
      "memory",
      "efficient:"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "characteristics",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-combined-patterns",
    "title": "Combined Patterns",
    "description": "Combined Patterns Mix and match patterns for powerful workflows: [code block]",
    "content": "Combined Patterns Mix and match patterns for powerful workflows: [code block]",
    "headings": ["Combined Patterns"],
    "path": "/docs/patterns#combined-patterns",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "combined",
      "patterns",
      "match",
      "powerful",
      "workflows:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "await agent({ llm })\n  // Parallel analysis\n  .parallel({\n    sentiment: { prompt: \"Analyze sentiment\" },\n    intent: { prompt: \"Extract intent\" },\n    priority: { prompt: \"Determine priority\" },\n  })\n\n  // Route based on priority\n  .switch((h) => h[0].parallel?.priority.llmOutput?.trim() || \"\", {\n    URGENT: (a) =>\n      a.then({ mcp: slack, tool: \"alert_team\" }).runAgent(escalationAgent),\n\n    NORMAL: (a) =>\n      a.forEach(responders, (person, ag) =>\n        ag.then({ prompt: `Assign to ${person}` })\n      ),\n\n    default: (a) => a.then({ prompt: \"Queue for review\" }),\n  })\n\n  // Final step\n  .then({ prompt: \"Log outcome\" })\n  .run();\n"
    ],
    "anchor": "combined-patterns",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-api-common-step-fields",
    "title": "Common Step Fields",
    "description": "Common Step Fields Field Type Description -------------- ----------- ---------------------------------------------------- llm LLMHandle Override the L",
    "content": "Common Step Fields Field Type Description -------------- ----------- ---------------------------------------------------- llm LLMHandle Override the LLM for this step instructions string Override system instructions for this step timeout number Step timeout in seconds (overrides agent default) retry RetryConfig Retry config for this step (overrides agent default) pre () => void Hook called before step execution post () => void Hook called after successful step execution",
    "headings": ["Common Step Fields"],
    "path": "/docs/api#common-step-fields",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "common",
      "step",
      "fields",
      "override",
      "instructions",
      "timeout",
      "(overrides",
      "agent",
      "default)",
      "retry",
      "called",
      "execution"
    ],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "anchor": "common-step-fields",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-complex-composition",
    "title": "Complex Composition",
    "description": "Complex Composition Mix sub-agents with other patterns: [code block]",
    "content": "Complex Composition Mix sub-agents with other patterns: [code block]",
    "headings": ["Complex Composition"],
    "path": "/docs/patterns#complex-composition",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "complex",
      "composition",
      "sub-agents",
      "other",
      "patterns:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "const contentAnalyzer = agent({ llm }).parallel({\n  sentiment: { prompt: \"Analyze sentiment\" },\n  topics: { prompt: \"Extract main topics\" },\n  tone: { prompt: \"Determine tone\" },\n});\n\nconst contentModerator = agent({ llm })\n  .then({ prompt: \"Check for policy violations\" })\n  .branch((h) => h[0].llmOutput?.includes(\"VIOLATION\") || false, {\n    true: (a) => a.then({ mcp: moderation, tool: \"flag_content\" }),\n    false: (a) => a.then({ prompt: \"Approve content\" }),\n  });\n\n// Main workflow\nawait agent({ llm })\n  .then({ mcp: cms, tool: \"fetch_pending_posts\" })\n  .forEach(posts, (post, a) =>\n    a.runAgent(contentAnalyzer).runAgent(contentModerator)\n  )\n  .then({ prompt: \"Generate moderation report\" })\n  .run();\n"
    ],
    "anchor": "complex-composition",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-composing-sub-agents",
    "title": "Composing Sub-Agents",
    "description": "Composing Sub-Agents [code block]",
    "content": "Composing Sub-Agents [code block]",
    "headings": ["Composing Sub-Agents"],
    "path": "/docs/patterns#composing-sub-agents",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["composing", "sub-agents", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "// Compose them in a larger workflow\nawait agent({ llm })\n  .then({ mcp: gmail, tool: \"fetch_unread\" })\n  .runAgent(emailAnalyzer) // Run first sub-agent\n  .runAgent(responseGenerator) // Run second sub-agent\n  .runAgent(qualityChecker) // Run third sub-agent\n  .then({ mcp: gmail, tool: \"send_reply\" })\n  .run();\n"
    ],
    "anchor": "composing-sub-agents",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-conditional-branching",
    "title": "Conditional Branching",
    "description": "Conditional Branching Route workflows based on conditions.",
    "content": "Conditional Branching Route workflows based on conditions.",
    "headings": ["Conditional Branching"],
    "path": "/docs/patterns#conditional-branching",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "conditional",
      "branching",
      "route",
      "workflows",
      "based",
      "conditions."
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "anchor": "conditional-branching",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-conditional-provider-switching",
    "title": "Conditional Provider Switching",
    "description": "Conditional Provider Switching Route to different LLMs based on complexity: [code block]",
    "content": "Conditional Provider Switching Route to different LLMs based on complexity: [code block]",
    "headings": ["Conditional Provider Switching"],
    "path": "/docs/patterns#conditional-provider-switching",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "conditional",
      "provider",
      "switching",
      "route",
      "different",
      "based",
      "complexity:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "await agent()\n  .then({ llm: gpt, prompt: \"Classify this task as SIMPLE or COMPLEX\" })\n  .switch((h) => (h[0].llmOutput?.includes(\"COMPLEX\") ? \"complex\" : \"simple\"), {\n    complex: (a) =>\n      a\n        .then({ llm: gpt4, prompt: \"Handle complex reasoning\" })\n        .then({ llm: claude, prompt: \"Verify the analysis\" }),\n\n    simple: (a) => a.then({ llm: llama, prompt: \"Handle simple task quickly\" }),\n  })\n  .run();\n"
    ],
    "anchor": "conditional-provider-switching",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-configuration",
    "title": "Configuration",
    "description": "Configuration Advanced configuration for testing or special scenarios: [code block] Note: These are internal APIs for advanced use cases and testing. ",
    "content": "Configuration Advanced configuration for testing or special scenarios: [code block] Note: These are internal APIs for advanced use cases and testing. The default configuration works well for most applications.",
    "headings": ["Configuration"],
    "path": "/docs/mcp-tools#configuration",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "configuration",
      "advanced",
      "testing",
      "special",
      "scenarios:",
      "[code",
      "block]",
      "note:",
      "internal",
      "cases"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "import {\n  __internal_setPoolConfig,\n  __internal_setDiscoveryTtl,\n  __internal_clearDiscoveryCache,\n  __internal_clearOAuthTokenCache,\n} from \"volcano-sdk\";\n\n// Configure connection pool\n__internal_setPoolConfig(32, 60_000); // max 32 connections, 60s idle\n\n// Configure tool discovery cache TTL\n__internal_setDiscoveryTtl(120_000); // 120s cache\n\n// Clear caches (useful for testing)\n__internal_clearDiscoveryCache();\n__internal_clearOAuthTokenCache();\n"
    ],
    "anchor": "configuration",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-providers-configuration",
    "title": "Configuration",
    "description": "Configuration Parameter Required Description --------- ------------ -------------------------------------------- model Required Model identifier (e.g.",
    "content": "Configuration Parameter Required Description --------- ------------ -------------------------------------------- model Required Model identifier (e.g., gpt-4o-mini, gpt-4o) apiKey Required OpenAI API key baseURL Optional Custom API endpoint options Optional Model-specific parameters (see below)",
    "headings": ["Configuration"],
    "path": "/docs/providers#configuration",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "configuration",
      "required",
      "model",
      "optional",
      "parameter",
      "description",
      "---------",
      "------------",
      "--------------------------------------------",
      "identifier"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "configuration",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-configuration",
    "title": "Configuration",
    "description": "Configuration Parameter Required Description --------- ------------ --------------------------------------------------- model Required Model identifie",
    "content": "Configuration Parameter Required Description --------- ------------ --------------------------------------------------- model Required Model identifier (e.g., claude-3-5-sonnet-20241022) apiKey Required Anthropic API key options Optional Model-specific parameters",
    "headings": ["Configuration"],
    "path": "/docs/providers#configuration",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "configuration",
      "required",
      "model",
      "parameter",
      "description",
      "---------",
      "------------",
      "---------------------------------------------------",
      "identifier",
      "(e.g.,"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "configuration",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-configuration",
    "title": "Configuration",
    "description": "Configuration Parameter Required Description --------- ------------ ------------------------- model Required Model identifier apiKey Required Mistral ",
    "content": "Configuration Parameter Required Description --------- ------------ ------------------------- model Required Model identifier apiKey Required Mistral API key options Optional Model-specific parameters",
    "headings": ["Configuration"],
    "path": "/docs/providers#configuration",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "configuration",
      "required",
      "model",
      "parameter",
      "description",
      "---------",
      "------------",
      "-------------------------",
      "identifier",
      "apikey"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "configuration",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-configuration",
    "title": "Configuration",
    "description": "Configuration Parameter Required Description --------- ------------ ----------------------------------------------- model Required Model identifier (e",
    "content": "Configuration Parameter Required Description --------- ------------ ----------------------------------------------- model Required Model identifier (e.g., llama3.2:3b for Ollama) baseURL Optional Server endpoint options Optional Model-specific parameters",
    "headings": ["Configuration"],
    "path": "/docs/providers#configuration",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "configuration",
      "required",
      "model",
      "optional",
      "parameter",
      "description",
      "---------",
      "------------",
      "-----------------------------------------------",
      "identifier"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "configuration",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-configuration",
    "title": "Configuration",
    "description": "Configuration Parameter Required Description --------- ------------ ------------------------------- model Required Bedrock model identifier region Opt",
    "content": "Configuration Parameter Required Description --------- ------------ ------------------------------- model Required Bedrock model identifier region Optional AWS region (default: us-east-1) options Optional Model-specific parameters",
    "headings": ["Configuration"],
    "path": "/docs/providers#configuration",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "configuration",
      "required",
      "model",
      "region",
      "optional",
      "parameter",
      "description",
      "---------",
      "------------",
      "-------------------------------"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "configuration",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-configuration",
    "title": "Configuration",
    "description": "Configuration Parameter Required Description --------- ------------ ------------------------- model Required Gemini model identifier apiKey Required G",
    "content": "Configuration Parameter Required Description --------- ------------ ------------------------- model Required Gemini model identifier apiKey Required Google AI Studio API key options Optional Model-specific parameters",
    "headings": ["Configuration"],
    "path": "/docs/providers#configuration",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "configuration",
      "required",
      "model",
      "parameter",
      "description",
      "---------",
      "------------",
      "-------------------------",
      "gemini",
      "identifier"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "configuration",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-configuration",
    "title": "Configuration",
    "description": "Configuration Parameter Required Description ------------ ------------ ----------------------------------------- model Required Deployment model name ",
    "content": "Configuration Parameter Required Description ------------ ------------ ----------------------------------------- model Required Deployment model name endpoint Required Azure resource URL apiVersion Optional API version (default: 2025-04-01-preview)",
    "headings": ["Configuration"],
    "path": "/docs/providers#configuration",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "configuration",
      "required",
      "------------",
      "model",
      "parameter",
      "description",
      "-----------------------------------------",
      "deployment",
      "endpoint",
      "azure"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "configuration",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-connection-pooling",
    "title": "Connection Pooling",
    "description": "Connection Pooling Automatic pooling: TCP sessions are reused across steps LRU eviction: Idle connections evicted when pool is full Per-endpoint pools",
    "content": "Connection Pooling Automatic pooling: TCP sessions are reused across steps LRU eviction: Idle connections evicted when pool is full Per-endpoint pools: Each MCP server has its own pool Auth-aware: Authenticated connections pooled separately Configurable limits: Max 16 connections, 30s idle timeout (default)",
    "headings": ["Connection Pooling"],
    "path": "/docs/mcp-tools#connection-pooling",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "connection",
      "pooling",
      "connections",
      "automatic",
      "pooling:",
      "sessions",
      "reused",
      "across",
      "steps",
      "eviction:"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "anchor": "connection-pooling",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-connection-pooling-performance",
    "title": "Connection Pooling & Performance",
    "description": "Connection Pooling & Performance Volcano SDK automatically manages MCP connections for optimal performance.",
    "content": "Connection Pooling & Performance Volcano SDK automatically manages MCP connections for optimal performance.",
    "headings": ["Connection Pooling & Performance"],
    "path": "/docs/mcp-tools#connection-pooling-performance",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "connection",
      "pooling",
      "performance",
      "volcano",
      "automatically",
      "manages",
      "connections",
      "optimal",
      "performance."
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "anchor": "connection-pooling-performance",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-examples-contributing-examples",
    "title": "Contributing Examples",
    "description": "Contributing Examples Have a great use case? Submit a PR with your example to help other developers!",
    "content": "Contributing Examples Have a great use case? Submit a PR with your example to help other developers!",
    "headings": ["Contributing Examples"],
    "path": "/docs/examples#contributing-examples",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "contributing",
      "examples",
      "great",
      "case?",
      "submit",
      "example",
      "other",
      "developers!"
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "anchor": "contributing-examples",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-controlling-tool-iterations",
    "title": "Controlling Tool Iterations",
    "description": "Controlling Tool Iterations For complex tasks, the LLM may need multiple tool calls. Configure how many iterations are allowed: [code block]",
    "content": "Controlling Tool Iterations For complex tasks, the LLM may need multiple tool calls. Configure how many iterations are allowed: [code block]",
    "headings": ["Controlling Tool Iterations"],
    "path": "/docs/mcp-tools#controlling-tool-iterations",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "controlling",
      "tool",
      "iterations",
      "complex",
      "tasks,",
      "multiple",
      "calls.",
      "configure",
      "allowed:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "// Agent-level (default for all steps)\nawait agent({\n  llm,\n  maxToolIterations: 2, // Limit to 2 iterations (faster)\n})\n  .then({ prompt: \"Task\", mcps: [tools] })\n  .run();\n\n// Per-step override\nawait agent({ llm, maxToolIterations: 4 }) // Default 4\n  .then({\n    prompt: \"Simple task\",\n    mcps: [tools],\n    maxToolIterations: 1, // Fast: only 1 tool call\n  })\n  .then({\n    prompt: \"Complex task\",\n    mcps: [tools],\n    maxToolIterations: 4, // More iterations for complexity\n  })\n  .run();\n"
    ],
    "anchor": "controlling-tool-iterations",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-cost-optimized-pipeline",
    "title": "Cost-Optimized Pipeline",
    "description": "Cost-Optimized Pipeline Use local models for preprocessing, expensive models for critical tasks: [code block]",
    "content": "Cost-Optimized Pipeline Use local models for preprocessing, expensive models for critical tasks: [code block]",
    "headings": ["Cost-Optimized Pipeline"],
    "path": "/docs/patterns#cost-optimized-pipeline",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "cost-optimized",
      "pipeline",
      "models",
      "local",
      "preprocessing,",
      "expensive",
      "critical",
      "tasks:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "import { agent, llmLlama, llmOpenAI } from \"volcano-sdk\";\n\nconst llama = llmLlama({\n  baseURL: \"http://127.0.0.1:11434\",\n  model: \"llama3.2:3b\", // Free, local\n});\n\nconst gpt4 = llmOpenAI({\n  apiKey: process.env.OPENAI_API_KEY!,\n  model: \"gpt-4o\", // Expensive, high-quality\n});\n\n// Process 100 documents\nawait agent()\n  .forEach(\n    documents,\n    (doc, a) => a.then({ llm: llama, prompt: `Summarize: ${doc}` }) // Cheap preprocessing\n  )\n  .then({ llm: gpt4, prompt: \"Analyze all summaries and create final report\" }) // Quality output\n  .run();\n"
    ],
    "anchor": "cost-optimized-pipeline",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-providers-creating-a-custom-provider",
    "title": "Creating a Custom Provider",
    "description": "Creating a Custom Provider You can create your own LLM provider by implementing the LLMHandle interface:",
    "content": "Creating a Custom Provider You can create your own LLM provider by implementing the LLMHandle interface:",
    "headings": ["Creating a Custom Provider"],
    "path": "/docs/providers#creating-a-custom-provider",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "creating",
      "custom",
      "provider",
      "create",
      "implementing",
      "llmhandle",
      "interface:"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "creating-a-custom-provider",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-observability-custom-tracer-and-meter",
    "title": "Custom Tracer and Meter",
    "description": "Custom Tracer and Meter Provide your own OTEL tracer and meter instances: [code block]",
    "content": "Custom Tracer and Meter Provide your own OTEL tracer and meter instances: [code block]",
    "headings": ["Custom Tracer and Meter"],
    "path": "/docs/observability#custom-tracer-and-meter",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "custom",
      "tracer",
      "meter",
      "provide",
      "instances:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "codeBlocks": [
      "import { trace, metrics } from \"@opentelemetry/api\";\nimport { createVolcanoTelemetry } from \"volcano-sdk\";\n\nconst tracer = trace.getTracer(\"my-custom-tracer\");\nconst meter = metrics.getMeter(\"my-custom-meter\");\n\nconst telemetry = createVolcanoTelemetry({\n  serviceName: \"my-agent\",\n  tracer,\n  meter,\n});\n"
    ],
    "anchor": "custom-tracer-and-meter",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-observability-datadog-newrelic-grafana-cloud",
    "title": "DataDog / NewRelic / Grafana Cloud",
    "description": "DataDog / NewRelic / Grafana Cloud [code block]",
    "content": "DataDog / NewRelic / Grafana Cloud [code block]",
    "headings": ["DataDog / NewRelic / Grafana Cloud"],
    "path": "/docs/observability#datadog-newrelic-grafana-cloud",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["datadog", "newrelic", "grafana", "cloud", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "codeBlocks": [
      "import { OTLPTraceExporter } from \"@opentelemetry/exporter-otlp-http\";\nimport { NodeSDK } from \"@opentelemetry/sdk-node\";\n\nconst sdk = new NodeSDK({\n  serviceName: \"my-agent\",\n  traceExporter: new OTLPTraceExporter({\n    url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT,\n    headers: {\n      \"DD-API-KEY\": process.env.DD_API_KEY, // DataDog\n      // or 'X-License-Key': process.env.NEW_RELIC_KEY\n      // or 'Authorization': `Bearer ${process.env.GRAFANA_TOKEN}`\n    },\n  }),\n});\n\nsdk.start();\n"
    ],
    "anchor": "datadog-newrelic-grafana-cloud",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-defining-reusable-sub-agents",
    "title": "Defining Reusable Sub-Agents",
    "description": "Defining Reusable Sub-Agents [code block]",
    "content": "Defining Reusable Sub-Agents [code block]",
    "headings": ["Defining Reusable Sub-Agents"],
    "path": "/docs/patterns#defining-reusable-sub-agents",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["defining", "reusable", "sub-agents", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "// Define specialized sub-agents\nconst emailAnalyzer = agent({ llm: claude })\n  .then({ prompt: \"Extract sender intent\" })\n  .then({ prompt: \"Classify urgency level\" });\n\nconst responseGenerator = agent({ llm: openai })\n  .then({ prompt: \"Draft professional response\" })\n  .then({ prompt: \"Add signature\" });\n\nconst qualityChecker = agent({ llm: mistral })\n  .then({ prompt: \"Check response quality\" })\n  .then({ prompt: \"Suggest improvements\" });\n"
    ],
    "anchor": "defining-reusable-sub-agents",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-observability-disable-traces-or-metrics",
    "title": "Disable Traces or Metrics",
    "description": "Disable Traces or Metrics [code block]",
    "content": "Disable Traces or Metrics [code block]",
    "headings": ["Disable Traces or Metrics"],
    "path": "/docs/observability#disable-traces-or-metrics",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["disable", "traces", "metrics", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "codeBlocks": [
      "// Only traces, no metrics\nconst telemetry = createVolcanoTelemetry({\n  serviceName: \"my-agent\",\n  traces: true,\n  metrics: false,\n});\n\n// Only metrics, no traces\nconst telemetry = createVolcanoTelemetry({\n  serviceName: \"my-agent\",\n  traces: false,\n  metrics: true,\n});\n"
    ],
    "anchor": "disable-traces-or-metrics",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-api-discovertoolshandles-promisetooldefinition",
    "title": "discoverTools(handles): Promise\\<ToolDefinition\\[\\]\\>",
    "description": "discoverTools(handles): Promise\\<ToolDefinition\\[\\]\\> Manually discover tools from MCP servers: [code block]",
    "content": "discoverTools(handles): Promise\\<ToolDefinition\\[\\]\\> Manually discover tools from MCP servers: [code block]",
    "headings": ["discoverTools(handles): Promise\\<ToolDefinition\\[\\]\\>"],
    "path": "/docs/api#discovertoolshandles-promisetooldefinition",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "discovertools(handles):",
      "promise\\<tooldefinition\\[\\]\\>",
      "manually",
      "discover",
      "tools",
      "servers:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "import { discoverTools, mcp } from \"volcano-sdk\";\n\nconst handles = [\n  mcp(\"http://localhost:3211/mcp\"),\n  mcp(\"http://localhost:3212/mcp\"),\n];\n\nconst tools = await discoverTools(handles);\nconsole.log(tools); // Array of tool definitions\n"
    ],
    "anchor": "discovertoolshandles-promisetooldefinition",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-observability-distributed-tracing",
    "title": "Distributed Tracing",
    "description": "Distributed Tracing Volcano creates hierarchical traces showing the complete execution flow of your agent workflows.",
    "content": "Distributed Tracing Volcano creates hierarchical traces showing the complete execution flow of your agent workflows.",
    "headings": ["Distributed Tracing"],
    "path": "/docs/observability#distributed-tracing",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "distributed",
      "tracing",
      "volcano",
      "creates",
      "hierarchical",
      "traces",
      "showing",
      "complete",
      "execution",
      "agent"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "anchor": "distributed-tracing",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-observability-environment-variables",
    "title": "Environment Variables",
    "description": "Environment Variables OTEL supports standard environment variables for configuration: ```bash",
    "content": "Environment Variables OTEL supports standard environment variables for configuration: ```bash",
    "headings": ["Environment Variables"],
    "path": "/docs/observability#environment-variables",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "environment",
      "variables",
      "supports",
      "standard",
      "configuration:",
      "```bash"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "anchor": "environment-variables",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-features-error-handling",
    "title": "Error Handling",
    "description": "Error Handling Volcano surfaces typed errors with rich metadata for easy debugging.",
    "content": "Error Handling Volcano surfaces typed errors with rich metadata for easy debugging.",
    "headings": ["Error Handling"],
    "path": "/docs/features#error-handling",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "error",
      "handling",
      "volcano",
      "surfaces",
      "typed",
      "errors",
      "metadata",
      "debugging."
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "error-handling",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-features-error-metadata",
    "title": "Error Metadata",
    "description": "Error Metadata All Volcano errors include metadata for debugging: [code block]",
    "content": "Error Metadata All Volcano errors include metadata for debugging: [code block]",
    "headings": ["Error Metadata"],
    "path": "/docs/features#error-metadata",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "error",
      "metadata",
      "volcano",
      "errors",
      "include",
      "debugging:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "codeBlocks": [
      "try {\n  await agent({ llm, retry: { backoff: 2, retries: 4 }, timeout: 30 })\n    .then({ prompt: \"auto\", mcps: [mcp(\"http://localhost:3211/mcp\")] })\n    .run();\n} catch (err) {\n  if (err && typeof err === \"object\" && \"meta\" in err) {\n    const e = err as VolcanoError;\n\n    console.error(e.name, e.message);\n    console.error(\"Metadata:\", {\n      stepId: e.meta.stepId, // 0-based step index\n      provider: e.meta.provider, // llm:openai or mcp:localhost\n      requestId: e.meta.requestId, // Upstream request ID\n      retryable: e.meta.retryable, // Should retry?\n    });\n\n    if (e.meta?.retryable) {\n      // Maybe enqueue for retry later\n    }\n  }\n}\n"
    ],
    "anchor": "error-metadata",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-observability-error-tracking",
    "title": "Error Tracking",
    "description": "Error Tracking When errors occur, spans include exception details: [code block]",
    "content": "Error Tracking When errors occur, spans include exception details: [code block]",
    "headings": ["Error Tracking"],
    "path": "/docs/observability#error-tracking",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "error",
      "tracking",
      "errors",
      "occur,",
      "spans",
      "include",
      "exception",
      "details:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "codeBlocks": [
      "// Errors are automatically recorded in spans\ntry {\n  await agent({ llm, telemetry }).then({ prompt: \"This might fail\" }).run();\n} catch (error) {\n  // Span will have:\n  // - status: ERROR\n  // - exception details\n  // - stack trace\n}\n"
    ],
    "anchor": "error-tracking",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-features-error-types",
    "title": "Error Types",
    "description": "Error Types Error Type Description ----------------------- ----------------------------------- AgentConcurrencyError run() called twice on same instan",
    "content": "Error Types Error Type Description ----------------------- ----------------------------------- AgentConcurrencyError run() called twice on same instance TimeoutError Step exceeded timeout limit ValidationError Tool args failed schema validation RetryExhaustedError Final failure after all retries LLMError LLM provider error MCPToolError MCP tool execution error MCPConnectionError MCP connection error",
    "headings": ["Error Types"],
    "path": "/docs/features#error-types",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "error",
      "types",
      "description",
      "-----------------------",
      "-----------------------------------",
      "agentconcurrencyerror",
      "run()",
      "called",
      "twice",
      "instance"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "error-types",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-api-example-usage",
    "title": "Example Usage",
    "description": "Example Usage [code block]",
    "content": "Example Usage [code block]",
    "headings": ["Example Usage"],
    "path": "/docs/api#example-usage",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["example", "usage", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "const results = await agent({ llm })\n  .then({ prompt: \"Analyze sentiment\", mcps: [analytics] })\n  .then({ prompt: \"Generate summary\" })\n  .run();\n\n// Access first step results\nconsole.log(results[0].prompt); // \"Analyze sentiment\"\nconsole.log(results[0].llmOutput); // LLM response\nconsole.log(results[0].toolCalls); // Tools that were called\nconsole.log(results[0].durationMs); // Step duration\nconsole.log(results[0].llmMs); // LLM time\n\n// Access aggregated metrics (on last step)\nconst last = results[results.length - 1];\nconsole.log(last.totalDurationMs); // Total workflow time\nconsole.log(last.totalLlmMs); // Total LLM time\nconsole.log(last.totalMcpMs); // Total MCP time\n"
    ],
    "anchor": "example-usage",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-providers-example-custom-provider",
    "title": "Example: Custom Provider",
    "description": "Example: Custom Provider [code block]",
    "content": "Example: Custom Provider [code block]",
    "headings": ["Example: Custom Provider"],
    "path": "/docs/providers#example-custom-provider",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["example:", "custom", "provider", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "import type { LLMHandle, ToolDefinition, LLMToolResult } from \"volcano-sdk\";\n\nexport function llmCustom(config: {\n  apiKey: string;\n  model: string;\n  baseURL: string;\n}): LLMHandle {\n  return {\n    id: \"custom\",\n    model: config.model,\n    client: null, // Your HTTP client or SDK instance\n\n    async gen(prompt: string): Promise<string> {\n      // Call your LLM API\n      const response = await fetch(`${config.baseURL}/generate`, {\n        method: \"POST\",\n        headers: {\n          Authorization: `Bearer ${config.apiKey}`,\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify({\n          model: config.model,\n          prompt: prompt,\n        }),\n      });\n\n      const data = await response.json();\n      return data.text;\n    },\n\n    async genWithTools(\n      prompt: string,\n      tools: ToolDefinition[]\n    ): Promise<LLMToolResult> {\n      // Call your LLM API with tool definitions\n      const response = await fetch(`${config.baseURL}/generate-with-tools`, {\n        method: \"POST\",\n        headers: {\n          Authorization: `Bearer ${config.apiKey}`,\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify({\n          model: config.model,\n          prompt: prompt,\n          tools: tools.map((t) => ({\n            name: t.name,\n            description: t.description,\n            parameters: t.parameters,\n          })),\n        }),\n      });\n\n      const data = await response.json();\n\n      // Map your API response to LLMToolResult format\n      return {\n        content: data.text,\n        toolCalls:\n          data.tool_calls?.map((tc: any) => ({\n            name: tc.name,\n            arguments: tc.arguments,\n            mcpHandle: tools.find((t) => t.name === tc.name)?.mcpHandle,\n          })) || [],\n      };\n    },\n\n    async *genStream(prompt: string): AsyncGenerator<string, void, unknown> {\n      // Implement streaming if your provider supports it\n      const response = await fetch(`${config.baseURL}/stream`, {\n        method: \"POST\",\n        headers: {\n          Authorization: `Bearer ${config.apiKey}`,\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify({\n          model: config.model,\n          prompt: prompt,\n        }),\n      });\n\n      const reader = response.body?.getReader();\n      if (!reader) return;\n\n      const decoder = new TextDecoder();\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        yield decoder.decode(value);\n      }\n    },\n  };\n}\n"
    ],
    "anchor": "example-custom-provider",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-api-examples",
    "title": "Examples",
    "description": "Examples [code block]",
    "content": "Examples [code block]",
    "headings": ["Examples"],
    "path": "/docs/api#examples",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["examples", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "// Immediate retry\n{ retries: 3 }\n\n// Delayed retry (wait 20s between attempts)\n{ delay: 20, retries: 3 }\n\n// Exponential backoff (1s, 2s, 4s, 8s)\n{ backoff: 2, retries: 4 }\n"
    ],
    "anchor": "examples",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-examples",
    "title": "Examples - Volcano SDK",
    "content": "Examples Ready-to-run examples demonstrating Volcano SDK features. View all examples on GitHub → All examples showcase automatic MCP tool selection where the LLM intelligently chooses which tools to call based on the prompt. Basic Examples basic.ts Simple single-step agent with LLM-only generation. [code block] Demonstrates: Agent creation, basic .then() and .run() View source on GitHub → automatic.ts Automatic MCP tool selection - let the LLM choose which tools to call based on the prompt. [code block] Demonstrates: Automatic tool discovery and selection with mcps: [mcp1, mcp2] View source on GitHub → providers.ts Multi-provider workflow using OpenAI, Anthropic, Mistral, and Llama with automatic MCP tool selection. [code block] Demonstrates: Per-step LLM provider switching, automatic tool selection, context flow between providers View source on GitHub → Advanced Patterns parallel.ts Execute multiple steps in parallel with automatic MCP tool selection in each parallel branch. [code block] Demonstrates: .parallel() with array and dictionary modes, automatic tool selection View source on GitHub → branching.ts & branching-simple.ts Conditional routing with .branch() and .switch() using automatic MCP tool selection. [code block] Demonstrates: If/else branching and switch/case patterns, LLM-driven tool selection View source on GitHub → loops.ts & loops-simple.ts Iterative workflows with automatic tool selection in each iteration. [code block] Demonstrates: .while(), .forEach(), .retryUntil() with MCP tools View source on GitHub → sub-agents.ts & composition.ts Build reusable sub-agents with automatic MCP tool integration. [code block] Demonstrates: .runAgent() for modular agent design with tool selection View source on GitHub → advanced-patterns.ts Combine parallel execution, branching, and loops with automatic MCP tool selection throughout. [code block] Demonstrates: Complex workflows mixing all control flow patterns with intelligent tool selection View source on GitHub → Special Features streaming.ts Stream step results in real-time with automatic MCP tool selection. [code block] Demonstrates: .stream() method for progressive result delivery, tool selection in streaming mode View source on GitHub → Running Examples Prerequisites [code block] Set Environment Variables Most examples require API keys. Set them in your environment: [code block] Run an Example [code block] Tip: Start with basic.ts and automatic.ts to understand core concepts, then explore advanced patterns. More Examples Find more examples in the GitHub repository: All examples include automatic MCP tool selection Production-ready code you can copy and adapt Comments explaining key concepts Environment variable setup instructions Contributing Examples Have a great use case? Submit a PR with your example to help other developers!",
    "headings": [
      "Examples",
      "Basic Examples",
      "basic.ts",
      "automatic.ts",
      "providers.ts",
      "Advanced Patterns",
      "parallel.ts",
      "branching.ts & branching-simple.ts",
      "loops.ts & loops-simple.ts",
      "sub-agents.ts & composition.ts",
      "advanced-patterns.ts",
      "Special Features",
      "streaming.ts",
      "Running Examples",
      "Prerequisites",
      "Clone the repository",
      "Install dependencies",
      "Build the SDK",
      "Set Environment Variables",
      "For Bedrock examples",
      "For Vertex examples",
      "For Azure examples",
      "Run an Example",
      "Run any example with tsx",
      "More Examples",
      "Contributing Examples"
    ],
    "path": "/docs/examples",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "examples",
      "volcano",
      "automatic",
      "selection",
      "[code",
      "block]",
      "github",
      "demonstrates:",
      "source",
      "tools",
      "agent"
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "codeBlocks": [
      "npx tsx examples/basic.ts\n",
      "npx tsx examples/automatic.ts\n",
      "npx tsx examples/providers.ts\n",
      "npx tsx examples/parallel.ts\n",
      "npx tsx examples/branching-simple.ts\nnpx tsx examples/branching.ts\n",
      "npx tsx examples/loops-simple.ts\nnpx tsx examples/loops.ts\n",
      "npx tsx examples/sub-agents.ts\nnpx tsx examples/composition.ts\n",
      "npx tsx examples/advanced-patterns.ts\n",
      "npx tsx examples/streaming.ts\n",
      "# Clone the repository\ngit clone https://github.com/Kong/volcano-sdk.git\ncd volcano-sdk\n\n# Install dependencies\nnpm install\n\n# Build the SDK\nnpm run build\n",
      "export OPENAI_API_KEY=\"your-openai-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\nexport MISTRAL_API_KEY=\"your-mistral-key\"\n\n# For Bedrock examples\nexport AWS_REGION=\"us-east-1\"\nexport AWS_ACCESS_KEY_ID=\"your-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret\"\n\n# For Vertex examples\nexport GCP_VERTEX_API_KEY=\"your-google-key\"\n\n# For Azure examples\nexport AZURE_AI_API_KEY=\"your-azure-key\"\nexport AZURE_AI_ENDPOINT=\"https://your-resource.openai.azure.com/openai/responses\"\n",
      "# Run any example with tsx\nnpx tsx examples/basic.ts\nnpx tsx examples/parallel.ts\nnpx tsx examples/streaming.ts\n"
    ]
  },
  {
    "id": "Documentation-api-explicit-mcp-tool-call",
    "title": "Explicit MCP Tool Call",
    "description": "Explicit MCP Tool Call Call a specific tool directly: [code block]",
    "content": "Explicit MCP Tool Call Call a specific tool directly: [code block]",
    "headings": ["Explicit MCP Tool Call"],
    "path": "/docs/api#explicit-mcp-tool-call",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "explicit",
      "tool",
      "call",
      "specific",
      "directly:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "{\n  mcp: MCPHandle;\n  tool: string;\n  args?: Record<string, any>;\n  prompt?: string;         // Optional LLM step before tool\n  llm?: LLMHandle;\n  instructions?: string;\n  timeout?: number;\n  retry?: RetryConfig;\n  contextMaxChars?: number;\n  contextMaxToolResults?: number;\n  pre?: () => void;\n  post?: () => void;\n}\n"
    ],
    "anchor": "explicit-mcp-tool-call",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-explicit-tool-calling",
    "title": "Explicit Tool Calling",
    "description": "Explicit Tool Calling Call specific MCP tools directly when you know exactly which tool to use and with what arguments. [code block]",
    "content": "Explicit Tool Calling Call specific MCP tools directly when you know exactly which tool to use and with what arguments. [code block]",
    "headings": ["Explicit Tool Calling"],
    "path": "/docs/mcp-tools#explicit-tool-calling",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "explicit",
      "tool",
      "calling",
      "specific",
      "tools",
      "directly",
      "exactly",
      "arguments.",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "const cafe = mcp(\"http://localhost:3000/mcp\");\n\nawait agent({ llm })\n  .then({ prompt: \"Recommend a coffee for Ava from Naples\" })\n  .then({\n    mcp: cafe,\n    tool: \"order_item\",\n    args: { item_id: \"espresso\", quantity: 2 },\n  })\n  .run();\n"
    ],
    "anchor": "explicit-tool-calling",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-features",
    "title": "Features - Volcano SDK",
    "content": "Features Core features for building AI agents: execution methods, streaming, retries, timeouts, hooks, error handling, and MCP tool integration. run() Method Execute the complete agent workflow and return all step results at once. [code block] With Logging Callback Pass a callback function to log each step as it completes: [code block] Return Value Returns Promise<StepResult[]> with all step results: [code block] Characteristics Waits for completion: Returns only after all steps finish Aggregated metrics: Final step includes total duration, LLM time, and MCP time Error handling: Throws on first failure (use try/catch) Sequential execution: Steps run in order, one after another Full results: Access all step data for analysis or debugging When to Use run() Batch processing where you need complete results Scripts that can wait for full completion Analysis workflows needing aggregated metrics APIs returning complete responses Testing and debugging (inspect all steps together) stream() Method Stream step results in real-time as they complete using the stream() method. [code block] Streaming with Progress Tracking [code block] When to Use Streaming Use stream() for: Interactive applications needing live updates Long-running workflows (>5 seconds) Real-time dashboards WebSocket/SSE applications Memory-sensitive environments Early termination scenarios Use run() for: Batch processing Simple scripts Analysis workflows APIs returning complete responses Testing and debugging When you need aggregated metrics Characteristics Immediate feedback: First result available when first step completes Memory efficient: Results can be processed and discarded incrementally Progressive display: Results appear as they complete Same execution model: Retries, timeouts, and validation work identically to run() Retries & Timeouts Timeouts Set per-step or global timeouts (in seconds): [code block] Retry Strategies Immediate Retry (Default) Retry immediately without waiting: [code block] Delayed Retry Wait a fixed duration between attempts: [code block] Exponential Backoff Progressively longer waits between retries: [code block] Per-Step Override [code block] Retry Semantics Non-retryable errors (like ValidationError) abort immediately Retryable errors include: timeouts, 429, 5xx, network errors On retry exhaustion, the last error is thrown You cannot set both delay and backoff Step Hooks Add pre and post hooks for fine-grained control over execution flow: [code block] Hook Execution Order pre() hook (before step execution) Step execution (LLM/MCP calls) post() hook (after step completion) run() callback (with step result and index) Hook Characteristics Hooks are synchronous functions (() => void) Hook errors are caught and logged but don't fail the step Hooks execute on every retry attempt (pre) or only on success (post) Hooks have access to closure variables for state management Note: Hook errors are caught and logged but do not fail the step. Pre-hooks execute on every retry attempt. Post-hooks execute only on successful completion. Use Cases Performance monitoring and timing Logging and debugging State management Notifications and alerts Metrics collection Error Handling Volcano surfaces typed errors with rich metadata for easy debugging. Error Types Error Type Description ----------------------- ----------------------------------- AgentConcurrencyError run() called twice on same instance TimeoutError Step exceeded timeout limit ValidationError Tool args failed schema validation RetryExhaustedError Final failure after all retries LLMError LLM provider error MCPToolError MCP tool execution error MCPConnectionError MCP connection error Error Metadata All Volcano errors include metadata for debugging: [code block] Metadata Fields stepId: 0-based index of the failing step provider: llm:<id model> or mcp:<host> requestId: Upstream provider request ID when available retryable: Volcano's hint (true for 429/5xx/timeouts; false for validation/4xx)",
    "headings": [
      "Features",
      "run() Method",
      "With Logging Callback",
      "Return Value",
      "Characteristics",
      "When to Use run()",
      "stream() Method",
      "Streaming with Progress Tracking",
      "When to Use Streaming",
      "Use `stream()` for:",
      "Use `run()` for:",
      "Characteristics",
      "Retries & Timeouts",
      "Timeouts",
      "Retry Strategies",
      "Immediate Retry (Default)",
      "Delayed Retry",
      "Exponential Backoff",
      "Per-Step Override",
      "Retry Semantics",
      "Step Hooks",
      "Hook Execution Order",
      "Hook Characteristics",
      "Use Cases",
      "Error Handling",
      "Error Types",
      "Error Metadata",
      "Metadata Fields"
    ],
    "path": "/docs/features",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "features",
      "volcano",
      "[code",
      "block]",
      "error",
      "retry",
      "errors",
      "execution",
      "run()",
      "complete",
      "results",
      "hooks"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "codeBlocks": [
      "const results = await agent({ llm })\n  .then({ prompt: \"Analyze user data\", mcps: [analytics] })\n  .then({ prompt: \"Generate insights\" })\n  .then({ prompt: \"Create recommendations\" })\n  .run();\n\n// All steps complete before results are returned\nconsole.log(results); // Array of all StepResult objects\nconsole.log(results[0].llmOutput); // First step output\nconsole.log(results[1].llmOutput); // Second step output\nconsole.log(results[results.length - 1].totalDurationMs); // Total time\n",
      "const results = await agent({ llm })\n  .then({ prompt: \"Step 1\" })\n  .then({ prompt: \"Step 2\" })\n  .then({ prompt: \"Step 3\" })\n  .run((stepResult, stepIndex) => {\n    console.log(`Step ${stepIndex + 1} completed`);\n    console.log(`Duration: ${stepResult.durationMs}ms`);\n    console.log(`Output: ${stepResult.llmOutput}`);\n  });\n\n// Callback is called for each step as it completes\n// Final results array is returned when all steps finish\n",
      "type StepResult = {\n  prompt?: string;\n  llmOutput?: string;\n  durationMs?: number;\n  llmMs?: number;\n  toolCalls?: Array<{ name: string; result: any; ms?: number }>;\n  // Aggregated metrics (on final step only):\n  totalDurationMs?: number;\n  totalLlmMs?: number;\n  totalMcpMs?: number;\n};\n",
      "for await (const stepResult of agent({ llm })\n  .then({ prompt: \"Analyze user data\", mcps: [analytics] })\n  .then({ prompt: \"Generate insights\" })\n  .then({ prompt: \"Create recommendations\" })\n  .stream()) {\n  console.log(`Step completed: ${stepResult.prompt}`);\n  console.log(`Duration: ${stepResult.durationMs}ms`);\n\n  if (stepResult.llmOutput) {\n    console.log(`Result: ${stepResult.llmOutput}`);\n  }\n}\n",
      "let completedSteps = 0;\nconst totalSteps = 3;\n\nfor await (const stepResult of workflow.stream((step, stepIndex) => {\n  completedSteps++;\n  console.log(`Progress: ${completedSteps}/${totalSteps}`);\n})) {\n  updateProgressBar(completedSteps / totalSteps);\n  displayStepResult(stepResult);\n}\n",
      "await agent({ llm, timeout: 60 })\n  .then({ prompt: \"Quick check\", timeout: 5 }) // Override to 5s\n  .then({ prompt: \"Next step uses agent default (60s)\" })\n  .run();\n",
      "await agent({ llm, retry: { retries: 3 } })\n  .then({ prompt: \"hello\" })\n  .run();\n",
      "await agent({ llm, retry: { delay: 20, retries: 3 } })\n  .then({ prompt: \"unstable action\" })\n  .run();\n// Waits 20s between each retry\n",
      "await agent({ llm, retry: { backoff: 2, retries: 4 } })\n  .then({ prompt: \"might fail\" })\n  .run();\n// Waits: 1s, 2s, 4s, 8s between attempts\n",
      "await agent({ llm, retry: { delay: 20 } })\n  .then({ prompt: \"override to immediate\", retry: { delay: 0 } })\n  .run();\n",
      "await agent({ llm })\n  .then({\n    prompt: \"Analyze the user data\",\n    mcps: [analytics],\n    pre: () => {\n      console.log(\"Starting analysis...\");\n    },\n    post: () => {\n      console.log(\"Analysis complete!\");\n    },\n  })\n  .then({\n    prompt: \"Generate report\",\n    pre: () => {\n      startTimer();\n    },\n    post: () => {\n      endTimer();\n      saveMetrics();\n    },\n  })\n  .run((step, stepIndex) => {\n    console.log(`Step ${stepIndex + 1} finished`);\n  });\n",
      "try {\n  await agent({ llm, retry: { backoff: 2, retries: 4 }, timeout: 30 })\n    .then({ prompt: \"auto\", mcps: [mcp(\"http://localhost:3211/mcp\")] })\n    .run();\n} catch (err) {\n  if (err && typeof err === \"object\" && \"meta\" in err) {\n    const e = err as VolcanoError;\n\n    console.error(e.name, e.message);\n    console.error(\"Metadata:\", {\n      stepId: e.meta.stepId, // 0-based step index\n      provider: e.meta.provider, // llm:openai or mcp:localhost\n      requestId: e.meta.requestId, // Upstream request ID\n      retryable: e.meta.retryable, // Should retry?\n    });\n\n    if (e.meta?.retryable) {\n      // Maybe enqueue for retry later\n    }\n  }\n}\n"
    ]
  },
  {
    "id": "Documentation-patterns-for-each-loop",
    "title": "For-Each Loop",
    "description": "For-Each Loop Process an array of items: [code block]",
    "content": "For-Each Loop Process an array of items: [code block]",
    "headings": ["For-Each Loop"],
    "path": "/docs/patterns#for-each-loop",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "for-each",
      "loop",
      "process",
      "array",
      "items:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "const customers = [\n  \"alice@example.com\",\n  \"bob@example.com\",\n  \"charlie@example.com\",\n];\n\nawait agent({ llm })\n  .forEach(customers, (email, a) =>\n    a\n      .then({ prompt: `Generate personalized email for ${email}` })\n      .then({ mcp: sendgrid, tool: \"send\", args: { to: email } })\n  )\n  .then({ prompt: \"Summarize campaign results\" })\n  .run();\n"
    ],
    "anchor": "for-each-loop",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-global-default-with-per-step-overrides",
    "title": "Global Default with Per-Step Overrides",
    "description": "Global Default with Per-Step Overrides [code block]",
    "content": "Global Default with Per-Step Overrides [code block]",
    "headings": ["Global Default with Per-Step Overrides"],
    "path": "/docs/patterns#global-default-with-per-step-overrides",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "global",
      "default",
      "per-step",
      "overrides",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "// Set default LLM at agent level\nawait agent({ llm: gpt })\n  .then({ prompt: \"Step 1 uses default GPT\" })\n  .then({ llm: claude, prompt: \"Step 2 overrides with Claude\" })\n  .then({ prompt: \"Step 3 back to default GPT\" })\n  .then({ llm: mistral, prompt: \"Step 4 uses Mistral\" })\n  .run();\n"
    ],
    "anchor": "global-default-with-per-step-overrides",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-providers-google-vertex-studio-provider",
    "title": "Google Vertex Studio Provider",
    "description": "Google Vertex Studio Provider Google's Gemini models with function calling via AI Studio API. [code block]",
    "content": "Google Vertex Studio Provider Google's Gemini models with function calling via AI Studio API. [code block]",
    "headings": ["Google Vertex Studio Provider"],
    "path": "/docs/providers#google-vertex-studio-provider",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "google",
      "vertex",
      "studio",
      "provider",
      "google's",
      "gemini",
      "models",
      "function",
      "calling",
      "[code"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "import { llmVertexStudio } from \"volcano-sdk\";\n\nconst vertex = llmVertexStudio({\n  model: \"gemini-2.0-flash-exp\",\n  apiKey: process.env.GCP_VERTEX_API_KEY!,\n  baseURL: \"https://aiplatform.googleapis.com/v1\", // Optional\n  options: {\n    temperature: 0.8,\n    max_output_tokens: 2048,\n    top_k: 40,\n    top_p: 0.95,\n    stop_sequences: [\"\\n\\n\"],\n    candidate_count: 1,\n    response_mime_type: \"text/plain\",\n  },\n});\n"
    ],
    "anchor": "google-vertex-studio-provider",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-handle-level-authentication",
    "title": "Handle-Level Authentication",
    "description": "Handle-Level Authentication Configure auth directly on the MCP handle: [code block]",
    "content": "Handle-Level Authentication Configure auth directly on the MCP handle: [code block]",
    "headings": ["Handle-Level Authentication"],
    "path": "/docs/mcp-tools#handle-level-authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "handle-level",
      "authentication",
      "configure",
      "directly",
      "handle:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "// OAuth on handle\nconst protectedMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"oauth\",\n    clientId: process.env.MCP_CLIENT_ID!,\n    clientSecret: process.env.MCP_CLIENT_SECRET!,\n    tokenEndpoint: \"https://api.example.com/oauth/token\",\n  },\n});\n\n// Bearer token on handle\nconst bearerMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"bearer\",\n    token: process.env.MCP_BEARER_TOKEN!,\n  },\n});\n"
    ],
    "anchor": "handle-level-authentication",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-features-hook-characteristics",
    "title": "Hook Characteristics",
    "description": "Hook Characteristics Hooks are synchronous functions (() => void) Hook errors are caught and logged but don't fail the step Hooks execute on every ret",
    "content": "Hook Characteristics Hooks are synchronous functions (() => void) Hook errors are caught and logged but don't fail the step Hooks execute on every retry attempt (pre) or only on success (post) Hooks have access to closure variables for state management Note: Hook errors are caught and logged but do not fail the step. Pre-hooks execute on every retry attempt. Post-hooks execute only on successful completion.",
    "headings": ["Hook Characteristics"],
    "path": "/docs/features#hook-characteristics",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "hook",
      "characteristics",
      "hooks",
      "execute",
      "errors",
      "caught",
      "logged",
      "every",
      "retry",
      "synchronous",
      "functions"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "hook-characteristics",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-features-hook-execution-order",
    "title": "Hook Execution Order",
    "description": "Hook Execution Order pre() hook (before step execution) Step execution (LLM/MCP calls) post() hook (after step completion) run() callback (with step r",
    "content": "Hook Execution Order pre() hook (before step execution) Step execution (LLM/MCP calls) post() hook (after step completion) run() callback (with step result and index)",
    "headings": ["Hook Execution Order"],
    "path": "/docs/features#hook-execution-order",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "hook",
      "execution",
      "order",
      "pre()",
      "(before",
      "execution)",
      "(llm/mcp",
      "calls)",
      "post()",
      "(after",
      "completion)"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "hook-execution-order",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-how-it-works",
    "title": "How It Works",
    "description": "How It Works Tool Discovery: Volcano fetches available tools from MCP servers (cached with TTL) LLM Selection: The LLM analyzes the prompt and chooses",
    "content": "How It Works Tool Discovery: Volcano fetches available tools from MCP servers (cached with TTL) LLM Selection: The LLM analyzes the prompt and chooses relevant tools Schema Validation: Arguments are validated against JSON Schema before execution Iterative Calling: The LLM can make multiple tool calls in sequence (default: 4 iterations) Context Flow: Tool results are automatically included in subsequent steps",
    "headings": ["How It Works"],
    "path": "/docs/mcp-tools#how-it-works",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "works",
      "tools",
      "schema",
      "discovery:",
      "volcano",
      "fetches",
      "available",
      "servers",
      "(cached",
      "selection:"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "anchor": "how-it-works",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-ifelse-branching",
    "title": "If/Else Branching",
    "description": "If/Else Branching Binary decision based on a condition: [code block]",
    "content": "If/Else Branching Binary decision based on a condition: [code block]",
    "headings": ["If/Else Branching"],
    "path": "/docs/patterns#ifelse-branching",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "if/else",
      "branching",
      "binary",
      "decision",
      "based",
      "condition:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "await agent({ llm })\n  .then({ prompt: \"Is this email spam? Reply YES or NO\" })\n  .branch((history) => history[0].llmOutput?.includes(\"YES\") || false, {\n    true: (a) =>\n      a\n        .then({ prompt: \"Categorize spam type\" })\n        .then({ mcp: notifications, tool: \"alert\" }),\n    false: (a) =>\n      a\n        .then({ prompt: \"Extract action items\" })\n        .then({ prompt: \"Draft reply\" }),\n  })\n  .run();\n"
    ],
    "anchor": "ifelse-branching",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-observability-installation",
    "title": "Installation",
    "description": "Installation Install the OpenTelemetry API (peer dependency): ```bash",
    "content": "Installation Install the OpenTelemetry API (peer dependency): ```bash",
    "headings": ["Installation"],
    "path": "/docs/observability#installation",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "installation",
      "install",
      "opentelemetry",
      "(peer",
      "dependency):",
      "```bash"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "anchor": "installation",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-iteration-trade-offs",
    "title": "Iteration Trade-offs",
    "description": "Iteration Trade-offs maxToolIterations Speed Capability Use Case ----------------- ----------- -------------------- --------------------- 1 ⚡ Fastest ",
    "content": "Iteration Trade-offs maxToolIterations Speed Capability Use Case ----------------- ----------- -------------------- --------------------- 1 ⚡ Fastest Single tool call Simple, direct tasks 2 🏃 Fast Tool + follow-up Two-step operations 4 (default) ⏱️ Moderate Multi-step workflows Complex orchestration Performance tip: Start with maxToolIterations: 1 for simple tasks. Increase only if the LLM needs multiple tool calls to complete the task.",
    "headings": ["Iteration Trade-offs"],
    "path": "/docs/mcp-tools#iteration-trade-offs",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "iteration",
      "trade-offs",
      "maxtooliterations",
      "speed",
      "capability",
      "-----------------",
      "-----------",
      "--------------------",
      "---------------------",
      "fastest"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "anchor": "iteration-trade-offs",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-observability-jaeger-distributed-tracing",
    "title": "Jaeger (Distributed Tracing)",
    "description": "Jaeger (Distributed Tracing) [code block]",
    "content": "Jaeger (Distributed Tracing) [code block]",
    "headings": ["Jaeger (Distributed Tracing)"],
    "path": "/docs/observability#jaeger-distributed-tracing",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["jaeger", "(distributed", "tracing)", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "codeBlocks": [
      "import { NodeSDK } from \"@opentelemetry/sdk-node\";\nimport { JaegerExporter } from \"@opentelemetry/exporter-jaeger\";\nimport { createVolcanoTelemetry } from \"volcano-sdk\";\n\n// Setup OTEL SDK\nconst sdk = new NodeSDK({\n  serviceName: \"my-agent\",\n  traceExporter: new JaegerExporter({\n    endpoint: \"http://localhost:14268/api/traces\",\n  }),\n});\n\nsdk.start();\n\n// Use with Volcano\nconst telemetry = createVolcanoTelemetry({\n  serviceName: \"my-agent\",\n});\n\nawait agent({ llm, telemetry }).then({ prompt: \"...\" }).run();\n\n// View traces in Jaeger UI at http://localhost:16686\n"
    ],
    "anchor": "jaeger-distributed-tracing",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-providers-llama-provider",
    "title": "Llama Provider",
    "description": "Llama Provider Run Llama models locally with Ollama or via OpenAI-compatible endpoints. [code block] Setup: Install Ollama from ollama.ai and run olla",
    "content": "Llama Provider Run Llama models locally with Ollama or via OpenAI-compatible endpoints. [code block] Setup: Install Ollama from ollama.ai and run ollama pull llama3.2:3b",
    "headings": ["Llama Provider"],
    "path": "/docs/providers#llama-provider",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "llama",
      "provider",
      "ollama",
      "models",
      "locally",
      "openai-compatible",
      "endpoints.",
      "[code",
      "block]",
      "setup:"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "import { llmLlama } from \"volcano-sdk\";\n\n// Local Ollama setup\nconst llama = llmLlama({\n  baseURL: \"http://127.0.0.1:11434\",\n  model: \"llama3.2:3b\",\n  apiKey: \"\", // Optional, not needed for local Ollama\n  options: {\n    temperature: 0.7,\n    max_tokens: 2000,\n    top_p: 0.9,\n    top_k: 40,\n    repeat_penalty: 1.1,\n    seed: 42,\n    num_predict: 2000,\n  },\n});\n"
    ],
    "anchor": "llama-provider",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers",
    "title": "LLM Providers - Volcano SDK",
    "content": "LLM Providers Volcano SDK supports 100s of models from 7 providers with function calling and MCP integration. Providers can be mixed within a single workflow. Provider Support Matrix Provider Basic Generation Function Calling Streaming MCP Integration ------------------------ ---------------- ---------------------------- --------- --------------- OpenAI ✅ Full ✅ Native ✅ Native ✅ Complete Anthropic ✅ Full ✅ Native (tooluse) ✅ Native ✅ Complete Mistral ✅ Full ✅ Native ✅ Native ✅ Complete Llama ✅ Full ✅ Via Ollama ✅ Native ✅ Complete AWS Bedrock ✅ Full ✅ Native (Converse API) ✅ Native ✅ Complete Google Vertex Studio ✅ Full ✅ Native (Function calling) ✅ Native ✅ Complete Azure AI ✅ Full ✅ Native (Responses API) ✅ Native ✅ Complete All providers support automatic tool selection and multi-step workflows. ✅ OpenAI ✅ Anthropic ✅ Mistral ✅ Llama ✅ AWS Bedrock ✅ Google Vertex ✅ Azure AI OpenAI Provider Full support for OpenAI's GPT models with function calling and streaming. [code block] Authentication Method Required Description --------- ------------ -------------------------------------------------------- apiKey Required OpenAI API key from platform.openai.com baseURL Optional Custom API endpoint (default: https://api.openai.com/v1) Configuration Parameter Required Description --------- ------------ -------------------------------------------- model Required Model identifier (e.g., gpt-4o-mini, gpt-4o) apiKey Required OpenAI API key baseURL Optional Custom API endpoint options Optional Model-specific parameters (see below) Options Parameters Parameter Type Description ----------------------- -------------------- ----------------------------------------------------------------- temperature 0-2 Controls randomness. Higher = more creative, lower = more focused maxcompletiontokens number Maximum tokens to generate (recommended for all models) maxtokens number Legacy parameter (use maxcompletiontokens instead) topp 0-1 Nucleus sampling - alternative to temperature frequencypenalty -2 to 2 Reduces repetition based on token frequency presencepenalty -2 to 2 Encourages topic diversity stop string \\ string\\[\\] Stop sequences to end generation seed number For deterministic outputs (same seed = same output) responseformat object Force JSON output: Structured Outputs (JSON Schema Validation) Use llmOpenAIResponses() for guaranteed JSON output that matches your schema exactly. Unlike basic JSON mode, structured outputs enforce strict schema compliance. [code block] Why Use Structured Outputs? Guaranteed schema compliance: Model output always matches your JSON schema No hallucinated fields: Only defined fields appear in output Type safety: Numbers are numbers, strings are strings, arrays are arrays Required fields enforced: All required properties always present Reliable parsing: No unexpected JSON structures or parsing errors Supported Models Structured outputs work with: gpt-4o-mini, gpt-4o, gpt-4o-2024-08-06 and later, o1-mini, o1-preview, o3-mini In Agent Workflows [code block] Structured Outputs vs Regular JSON Mode Feature llmOpenAI() llmOpenAIResponses() ----------------- ----------------- --------------------------------------- Output format Free text or JSON Guaranteed valid JSON Schema validation None Strict JSON Schema Use case General purpose Structured data extraction Required fields Not enforced Always present Extra fields May appear Never appear (strict mode) Best for Conversational AI APIs, data extraction, structured tasks Tip: Use llmOpenAIResponses() when building APIs or extracting structured data. Use regular llmOpenAI() for conversational responses and free-form text. Anthropic (Claude) Provider Native support for Claude models with tool calling. [code block] Authentication Method Required Description --------- ------------ -------------------------------------------------------- apiKey Required Anthropic API key from console.anthropic.com baseURL Optional Custom API endpoint (default: https://api.anthropic.com) version Optional API version header (default: 2023-06-01) Configuration Parameter Required Description --------- ------------ --------------------------------------------------- model Required Model identifier (e.g., claude-3-5-sonnet-20241022) apiKey Required Anthropic API key options Optional Model-specific parameters Options Parameters Parameter Type Description ---------------- ---------- ------------------------------------------------- temperature 0-1 Controls randomness (note: range is 0-1, not 0-2) maxtokens number Maximum tokens to generate topp 0-1 Nucleus sampling topk number Sample from top K options stopsequences string\\[\\] Array of stop sequences thinking object Extended thinking configuration (Claude-specific) Mistral Provider Support for Mistral's open and proprietary models via OpenAI-compatible API. [code block] Authentication Method Required Description --------- ------------ ----------------------------------------------------- apiKey Required Mistral API key from console.mistral.ai baseURL Optional Custom API endpoint (default: https://api.mistral.ai) Configuration Parameter Required Description --------- ------------ ------------------------- model Required Model identifier apiKey Required Mistral API key options Optional Model-specific parameters Options Parameters Parameter Type Description ----------------- -------------------- ---------------------------------------- temperature 0-1 Controls randomness maxtokens number Maximum tokens to generate topp 0-1 Nucleus sampling stop string \\ string\\[\\] Stop sequences safeprompt boolean Enable safety mode randomseed number For deterministic outputs responseformat object For JSON mode: Llama Provider Run Llama models locally with Ollama or via OpenAI-compatible endpoints. [code block] Setup: Install Ollama from ollama.ai and run ollama pull llama3.2:3b Authentication Method Required Description --------- ---------- ----------------------------------------------------------------------- baseURL Optional OpenAI-compatible endpoint (default: http://localhost:11434 for Ollama) apiKey Optional API key if your endpoint requires authentication Configuration Parameter Required Description --------- ------------ ----------------------------------------------- model Required Model identifier (e.g., llama3.2:3b for Ollama) baseURL Optional Server endpoint options Optional Model-specific parameters Options Parameters Parameter Type Description ---------------- -------------------- ---------------------------------------------------- temperature 0-2 Controls randomness maxtokens number Maximum tokens to generate topp 0-1 Nucleus sampling topk number Sample from top K options stop string \\ string\\[\\] Stop sequences repeatpenalty number Penalize repetitions (Ollama-specific, default: 1.1) seed number For deterministic outputs numpredict number Number of tokens to predict (Ollama-specific) AWS Bedrock Provider Access foundation models via AWS Bedrock with native tool calling support using the Converse API. [code block] Authentication AWS Bedrock supports multiple authentication methods (in priority order): Method Parameters Description -------------------- ------------------------------------------------ ------------------------------------------------------- Explicit Credentials accessKeyId, secretAccessKey, sessionToken Directly provide AWS credentials Bearer Token bearerToken Use bearer token authentication AWS Profile profile Use credentials from ~/.aws/credentials IAM Role roleArn Assume an IAM role Default Chain (none) Environment variables, instance profiles, ECS/EKS roles Authentication Examples [code block] Configuration Parameter Required Description --------- ------------ ------------------------------- model Required Bedrock model identifier region Optional AWS region (default: us-east-1) options Optional Model-specific parameters Options Parameters Parameter Type Description ---------------- ---------- -------------------------- temperature 0-1 Controls randomness maxtokens number Maximum tokens to generate topp 0-1 Nucleus sampling stopsequences string\\[\\] Array of stop sequences Google Vertex Studio Provider Google's Gemini models with function calling via AI Studio API. [code block] Authentication Method Required Description --------- ------------ ------------------------------------------------------------------- apiKey Required Google AI Studio API key baseURL Optional Custom API endpoint (default: https://aiplatform.googleapis.com/v1) Configuration Parameter Required Description --------- ------------ ------------------------- model Required Gemini model identifier apiKey Required Google AI Studio API key options Optional Model-specific parameters Options Parameters Parameter Type Description -------------------- ---------- ----------------------------------------- temperature 0-2 Controls randomness maxoutputtokens number Maximum tokens to generate topp 0-1 Nucleus sampling topk number Sample from top K options stopsequences string\\[\\] Array of stop sequences candidatecount number Number of response variations (usually 1) responsemimetype string For JSON mode: \"application/json\" Function calling limitations: Multiple tools per call only supported for search tools. Use multi-step workflows for complex tool orchestration. Azure AI Provider Azure OpenAI Service with enterprise authentication via the Responses API. [code block] Authentication Azure AI supports three authentication methods (in priority order): Method Parameters Description ------------------------ ------------- -------------------------------------------------------------------- API Key apiKey Simplest method - use your Azure resource key Entra ID Token accessToken Use Microsoft Entra ID (Azure AD) access token Default Credential Chain (none) Uses Azure SDK: Managed Identity, Service Principal, CLI credentials Authentication Examples [code block] Configuration Parameter Required Description ------------ ------------ ----------------------------------------- model Required Deployment model name endpoint Required Azure resource URL apiVersion Optional API version (default: 2025-04-01-preview) Options Parameters Important: Azure Responses API currently does not support optional configuration parameters. All inference parameters (temperature, maxtokens, seed, etc.) are rejected with HTTP 400 errors. This is a limitation of Azure's Responses API endpoint. The AzureOptions type is defined for API consistency but parameters cannot be used in practice. Creating a Custom Provider You can create your own LLM provider by implementing the LLMHandle interface: LLMHandle Interface [code block] Example: Custom Provider [code block] Using Your Custom Provider [code block] Requirements gen(): Basic text generation method (required) genWithTools(): Function calling support (required for MCP tools) genStream(): Streaming support (optional) id & model: Identifiers for logging and error messages Tip: Look at existing providers in src/llms/ for reference implementations. The OpenAI provider is a good starting point.",
    "headings": [
      "LLM Providers",
      "Provider Support Matrix",
      "✅ OpenAI",
      "✅ Anthropic",
      "✅ Mistral",
      "✅ Llama",
      "✅ AWS Bedrock",
      "✅ Google Vertex",
      "✅ Azure AI",
      "OpenAI Provider",
      "Authentication",
      "Configuration",
      "Options Parameters",
      "Structured Outputs (JSON Schema Validation)",
      "Why Use Structured Outputs?",
      "Supported Models",
      "In Agent Workflows",
      "Structured Outputs vs Regular JSON Mode",
      "Anthropic (Claude) Provider",
      "Authentication",
      "Configuration",
      "Options Parameters",
      "Mistral Provider",
      "Authentication",
      "Configuration",
      "Options Parameters",
      "Llama Provider",
      "Authentication",
      "Configuration",
      "Options Parameters",
      "AWS Bedrock Provider",
      "Authentication",
      "Authentication Examples",
      "Configuration",
      "Options Parameters",
      "Google Vertex Studio Provider",
      "Authentication",
      "Configuration",
      "Options Parameters",
      "Azure AI Provider",
      "Authentication",
      "Authentication Examples",
      "Configuration",
      "Options Parameters",
      "Creating a Custom Provider",
      "LLMHandle Interface",
      "Example: Custom Provider",
      "Using Your Custom Provider",
      "Requirements"
    ],
    "path": "/docs/providers",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "providers",
      "volcano",
      "required",
      "description",
      "optional",
      "number",
      "parameters",
      "options",
      "native",
      "model",
      "provider",
      "[code"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "import { llmOpenAI } from \"volcano-sdk\";\n\nconst openai = llmOpenAI({\n  apiKey: process.env.OPENAI_API_KEY!,\n  model: \"gpt-4o-mini\",\n  baseURL: \"https://api.openai.com/v1\", // Optional\n  options: {\n    temperature: 0.7,\n    max_completion_tokens: 2000,\n    top_p: 0.9,\n    seed: 42,\n  },\n});\n",
      "import { llmOpenAIResponses } from \"volcano-sdk\";\n\nconst llm = llmOpenAIResponses({\n  apiKey: process.env.OPENAI_API_KEY!,\n  model: \"gpt-4o-mini\",\n  options: {\n    jsonSchema: {\n      name: \"order_response\",\n      description: \"Order information\",\n      schema: {\n        type: \"object\",\n        properties: {\n          item: { type: \"string\" },\n          price: { type: \"number\" },\n          category: { type: \"string\" },\n        },\n        required: [\"item\", \"price\", \"category\"],\n        additionalProperties: false,\n      },\n    },\n  },\n});\n\nconst response = await llm.gen(\"Info for Espresso: $5.25, Coffee\");\nconst data = JSON.parse(response);\n// Guaranteed: { item: \"Espresso\", price: 5.25, category: \"Coffee\" }\n",
      "const llm = llmOpenAIResponses({\n  apiKey: process.env.OPENAI_API_KEY!,\n  model: \"gpt-4o-mini\",\n  options: {\n    jsonSchema: {\n      name: \"analysis\",\n      schema: {\n        type: \"object\",\n        properties: {\n          summary: { type: \"string\" },\n          sentiment: {\n            type: \"string\",\n            enum: [\"positive\", \"negative\", \"neutral\"],\n          },\n          confidence: { type: \"number\" },\n        },\n        required: [\"summary\", \"sentiment\", \"confidence\"],\n        additionalProperties: false,\n      },\n    },\n  },\n});\n\nconst results = await agent({ llm })\n  .then({ prompt: 'Analyze: \"The product is amazing!\"' })\n  .then({ prompt: 'Analyze: \"This is terrible.\"' })\n  .run();\n\n// Each step returns guaranteed valid JSON\nresults.forEach((step) => {\n  const analysis = JSON.parse(step.llmOutput!);\n  console.log(analysis.sentiment, analysis.confidence);\n});\n",
      "import { llmAnthropic } from \"volcano-sdk\";\n\nconst claude = llmAnthropic({\n  apiKey: process.env.ANTHROPIC_API_KEY!,\n  model: \"claude-3-5-sonnet-20241022\",\n  baseURL: \"https://api.anthropic.com\", // Optional\n  version: \"2023-06-01\", // Optional\n  options: {\n    temperature: 0.7,\n    max_tokens: 2000,\n    top_k: 50,\n    top_p: 0.9,\n    stop_sequences: [\"\\n\\n\"],\n  },\n});\n",
      "import { llmMistral } from \"volcano-sdk\";\n\nconst mistral = llmMistral({\n  apiKey: process.env.MISTRAL_API_KEY!,\n  model: \"mistral-small-latest\",\n  baseURL: \"https://api.mistral.ai\", // Optional\n  options: {\n    temperature: 0.7,\n    max_tokens: 2000,\n    top_p: 0.9,\n    safe_prompt: true,\n    random_seed: 42,\n  },\n});\n",
      "import { llmLlama } from \"volcano-sdk\";\n\n// Local Ollama setup\nconst llama = llmLlama({\n  baseURL: \"http://127.0.0.1:11434\",\n  model: \"llama3.2:3b\",\n  apiKey: \"\", // Optional, not needed for local Ollama\n  options: {\n    temperature: 0.7,\n    max_tokens: 2000,\n    top_p: 0.9,\n    top_k: 40,\n    repeat_penalty: 1.1,\n    seed: 42,\n    num_predict: 2000,\n  },\n});\n",
      "import { llmBedrock } from \"volcano-sdk\";\n\nconst bedrock = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  // Uses AWS credential chain by default\n  options: {\n    temperature: 0.7,\n    max_tokens: 2000,\n    top_p: 0.9,\n    stop_sequences: [\"\\n\\n\"],\n  },\n});\n",
      "// 1. Explicit credentials\nconst bedrock1 = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  accessKeyId: process.env.AWS_ACCESS_KEY_ID!,\n  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,\n  sessionToken: process.env.AWS_SESSION_TOKEN, // Optional\n});\n\n// 2. Bearer token\nconst bedrock2 = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  bearerToken: process.env.AWS_BEARER_TOKEN_BEDROCK!,\n});\n\n// 3. AWS Profile\nconst bedrock3 = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  profile: \"my-aws-profile\",\n});\n\n// 4. IAM Role\nconst bedrock4 = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  roleArn: \"arn:aws:iam::123456789012:role/my-bedrock-role\",\n});\n\n// 5. Default chain (recommended)\nconst bedrock5 = llmBedrock({\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\",\n  region: \"us-east-1\",\n  // Automatically uses environment, instance profiles, etc.\n});\n",
      "import { llmVertexStudio } from \"volcano-sdk\";\n\nconst vertex = llmVertexStudio({\n  model: \"gemini-2.0-flash-exp\",\n  apiKey: process.env.GCP_VERTEX_API_KEY!,\n  baseURL: \"https://aiplatform.googleapis.com/v1\", // Optional\n  options: {\n    temperature: 0.8,\n    max_output_tokens: 2048,\n    top_k: 40,\n    top_p: 0.95,\n    stop_sequences: [\"\\n\\n\"],\n    candidate_count: 1,\n    response_mime_type: \"text/plain\",\n  },\n});\n",
      "import { llmAzure } from \"volcano-sdk\";\n\nconst azure = llmAzure({\n  model: \"gpt-4o-mini\",\n  endpoint: \"https://your-resource.openai.azure.com/openai/responses\",\n  apiKey: process.env.AZURE_AI_API_KEY!,\n  apiVersion: \"2025-04-01-preview\", // Optional\n});\n",
      "// 1. API Key (simplest)\nconst azure1 = llmAzure({\n  model: \"gpt-4o-mini\",\n  endpoint: \"https://your-resource.openai.azure.com/openai/responses\",\n  apiKey: process.env.AZURE_AI_API_KEY!,\n});\n\n// 2. Entra ID Access Token\nconst azure2 = llmAzure({\n  model: \"gpt-4o-mini\",\n  endpoint: \"https://your-resource.openai.azure.com/openai/responses\",\n  accessToken: process.env.AZURE_ACCESS_TOKEN!,\n});\n\n// 3. Azure Default Credential Chain\nconst azure3 = llmAzure({\n  model: \"gpt-4o-mini\",\n  endpoint: \"https://your-resource.openai.azure.com/openai/responses\",\n  // Uses Azure SDK credential providers automatically\n});\n",
      "type LLMHandle = {\n  id: string;\n  model: string;\n  gen: (prompt: string) => Promise<string>;\n  genWithTools: (\n    prompt: string,\n    tools: ToolDefinition[]\n  ) => Promise<LLMToolResult>;\n  genStream: (prompt: string) => AsyncGenerator<string, void, unknown>;\n  client: any; // Your provider-specific client\n};\n",
      "import type { LLMHandle, ToolDefinition, LLMToolResult } from \"volcano-sdk\";\n\nexport function llmCustom(config: {\n  apiKey: string;\n  model: string;\n  baseURL: string;\n}): LLMHandle {\n  return {\n    id: \"custom\",\n    model: config.model,\n    client: null, // Your HTTP client or SDK instance\n\n    async gen(prompt: string): Promise<string> {\n      // Call your LLM API\n      const response = await fetch(`${config.baseURL}/generate`, {\n        method: \"POST\",\n        headers: {\n          Authorization: `Bearer ${config.apiKey}`,\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify({\n          model: config.model,\n          prompt: prompt,\n        }),\n      });\n\n      const data = await response.json();\n      return data.text;\n    },\n\n    async genWithTools(\n      prompt: string,\n      tools: ToolDefinition[]\n    ): Promise<LLMToolResult> {\n      // Call your LLM API with tool definitions\n      const response = await fetch(`${config.baseURL}/generate-with-tools`, {\n        method: \"POST\",\n        headers: {\n          Authorization: `Bearer ${config.apiKey}`,\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify({\n          model: config.model,\n          prompt: prompt,\n          tools: tools.map((t) => ({\n            name: t.name,\n            description: t.description,\n            parameters: t.parameters,\n          })),\n        }),\n      });\n\n      const data = await response.json();\n\n      // Map your API response to LLMToolResult format\n      return {\n        content: data.text,\n        toolCalls:\n          data.tool_calls?.map((tc: any) => ({\n            name: tc.name,\n            arguments: tc.arguments,\n            mcpHandle: tools.find((t) => t.name === tc.name)?.mcpHandle,\n          })) || [],\n      };\n    },\n\n    async *genStream(prompt: string): AsyncGenerator<string, void, unknown> {\n      // Implement streaming if your provider supports it\n      const response = await fetch(`${config.baseURL}/stream`, {\n        method: \"POST\",\n        headers: {\n          Authorization: `Bearer ${config.apiKey}`,\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify({\n          model: config.model,\n          prompt: prompt,\n        }),\n      });\n\n      const reader = response.body?.getReader();\n      if (!reader) return;\n\n      const decoder = new TextDecoder();\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        yield decoder.decode(value);\n      }\n    },\n  };\n}\n",
      "import { agent } from \"volcano-sdk\";\nimport { llmCustom } from \"./my-custom-provider\";\n\nconst customLLM = llmCustom({\n  apiKey: process.env.CUSTOM_API_KEY!,\n  model: \"my-model-v1\",\n  baseURL: \"https://api.myservice.com/v1\",\n});\n\nconst results = await agent({ llm: customLLM })\n  .then({ prompt: \"Hello from custom provider!\" })\n  .run();\n\nconsole.log(results[0].llmOutput);\n"
    ]
  },
  {
    "id": "Documentation-api-llm-only-step",
    "title": "LLM-Only Step",
    "description": "LLM-Only Step Generate text with an LLM without tools: [code block]",
    "content": "LLM-Only Step Generate text with an LLM without tools: [code block]",
    "headings": ["LLM-Only Step"],
    "path": "/docs/api#llm-only-step",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "llm-only",
      "step",
      "generate",
      "without",
      "tools:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "{\n  prompt: string;\n  llm?: LLMHandle;\n  instructions?: string;\n  timeout?: number;\n  retry?: RetryConfig;\n  contextMaxChars?: number;\n  contextMaxToolResults?: number;\n  pre?: () => void;\n  post?: () => void;\n}\n"
    ],
    "anchor": "llm-only-step",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-api-llmhandle",
    "title": "LLMHandle",
    "description": "LLMHandle [code block]",
    "content": "LLMHandle [code block]",
    "headings": ["LLMHandle"],
    "path": "/docs/api#llmhandle",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["llmhandle", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "type LLMHandle = {\n  model: string;\n  gen(prompt: string): Promise<string>;\n  genWithTools(prompt: string, tools: ToolDefinition[]): Promise<LLMToolResult>;\n  genStream?(prompt: string): AsyncGenerator<string, void, unknown>;\n};\n"
    ],
    "anchor": "llmhandle",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-providers-llmhandle-interface",
    "title": "LLMHandle Interface",
    "description": "LLMHandle Interface [code block]",
    "content": "LLMHandle Interface [code block]",
    "headings": ["LLMHandle Interface"],
    "path": "/docs/providers#llmhandle-interface",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["llmhandle", "interface", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "type LLMHandle = {\n  id: string;\n  model: string;\n  gen: (prompt: string) => Promise<string>;\n  genWithTools: (\n    prompt: string,\n    tools: ToolDefinition[]\n  ) => Promise<LLMToolResult>;\n  genStream: (prompt: string) => AsyncGenerator<string, void, unknown>;\n  client: any; // Your provider-specific client\n};\n"
    ],
    "anchor": "llmhandle-interface",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-loops",
    "title": "Loops",
    "description": "Loops Iterate until conditions are met.",
    "content": "Loops Iterate until conditions are met.",
    "headings": ["Loops"],
    "path": "/docs/patterns#loops",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["loops", "iterate", "until", "conditions"],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "anchor": "loops",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-examples-loopsts-loops-simplets",
    "title": "loops.ts & loops-simple.ts",
    "description": "loops.ts & loops-simple.ts Iterative workflows with automatic tool selection in each iteration. [code block] Demonstrates: .while(), .forEach(), .retr",
    "content": "loops.ts & loops-simple.ts Iterative workflows with automatic tool selection in each iteration. [code block] Demonstrates: .while(), .forEach(), .retryUntil() with MCP tools View source on GitHub →",
    "headings": ["loops.ts & loops-simple.ts"],
    "path": "/docs/examples#loopsts-loops-simplets",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "loops.ts",
      "loops-simple.ts",
      "iterative",
      "workflows",
      "automatic",
      "selection",
      "iteration.",
      "[code",
      "block]",
      "demonstrates:"
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "codeBlocks": [
      "npx tsx examples/loops-simple.ts\nnpx tsx examples/loops.ts\n"
    ],
    "anchor": "loopsts-loops-simplets",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-mcp-authentication",
    "title": "MCP Authentication",
    "description": "MCP Authentication Volcano SDK supports OAuth 2.1 and Bearer token authentication per the MCP specification. Authentication can be configured at the M",
    "content": "MCP Authentication Volcano SDK supports OAuth 2.1 and Bearer token authentication per the MCP specification. Authentication can be configured at the MCP handle level or centrally at the agent level.",
    "headings": ["MCP Authentication"],
    "path": "/docs/mcp-tools#mcp-authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "authentication",
      "volcano",
      "supports",
      "oauth",
      "bearer",
      "token",
      "specification.",
      "configured",
      "handle",
      "level"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "anchor": "mcp-authentication",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools",
    "title": "MCP Tools - Volcano SDK",
    "content": "MCP Tools The Model Context Protocol (MCP) enables AI agents to securely interact with external tools and data sources. Volcano SDK provides first-class MCP integration with automatic tool discovery, selection, and authentication. What is MCP? The Model Context Protocol is an open standard for connecting AI systems to external data sources and tools. It enables LLMs to safely interact with databases, APIs, file systems, and other services through a unified interface. Automatic Tool Selection Let the LLM intelligently choose which tools to call based on the prompt. This is the recommended approach for most use cases. [code block] How It Works Tool Discovery: Volcano fetches available tools from MCP servers (cached with TTL) LLM Selection: The LLM analyzes the prompt and chooses relevant tools Schema Validation: Arguments are validated against JSON Schema before execution Iterative Calling: The LLM can make multiple tool calls in sequence (default: 4 iterations) Context Flow: Tool results are automatically included in subsequent steps Controlling Tool Iterations For complex tasks, the LLM may need multiple tool calls. Configure how many iterations are allowed: [code block] Iteration Trade-offs maxToolIterations Speed Capability Use Case ----------------- ----------- -------------------- --------------------- 1 ⚡ Fastest Single tool call Simple, direct tasks 2 🏃 Fast Tool + follow-up Two-step operations 4 (default) ⏱️ Moderate Multi-step workflows Complex orchestration Performance tip: Start with maxToolIterations: 1 for simple tasks. Increase only if the LLM needs multiple tool calls to complete the task. Multiple MCP Servers Provide multiple MCP servers and the LLM will use tools from any of them: [code block] Explicit Tool Calling Call specific MCP tools directly when you know exactly which tool to use and with what arguments. [code block] With LLM Step Combine LLM reasoning with explicit tool calls: [code block] When to Use Explicit Calling You know exactly which tool to call The arguments are predetermined You want fine-grained control over execution order You're building deterministic workflows MCP Authentication Volcano SDK supports OAuth 2.1 and Bearer token authentication per the MCP specification. Authentication can be configured at the MCP handle level or centrally at the agent level. Handle-Level Authentication Configure auth directly on the MCP handle: [code block] Agent-Level Authentication Configure auth centrally at the agent level for cleaner code when using multiple authenticated servers: [code block] Precedence: Handle-level auth takes precedence over agent-level auth. This allows you to set defaults at the agent level and override for specific handles. OAuth Authentication (Client Credentials) Recommended for production MCP servers. Volcano automatically acquires and refreshes tokens: [code block] Bearer Token Authentication For pre-acquired tokens or custom authentication flows: [code block] Mixed Authentication Combine authenticated and non-authenticated MCP servers in the same workflow: [code block] MCP Spec Compliance: Per the official MCP specification, servers use OAuth 2.1 for authentication. Volcano supports both OAuth (automatic token acquisition) and Bearer (bring your own token). Authentication Features OAuth token caching: Tokens are cached and reused until expiration (60s buffer) Automatic refresh: Expired tokens are refreshed automatically Per-endpoint configuration: Each MCP server can have different auth Connection pooling: Authenticated connections are pooled separately Spec compliant: Follows MCP OAuth 2.1 authentication standard Connection Pooling & Performance Volcano SDK automatically manages MCP connections for optimal performance. Connection Pooling Automatic pooling: TCP sessions are reused across steps LRU eviction: Idle connections evicted when pool is full Per-endpoint pools: Each MCP server has its own pool Auth-aware: Authenticated connections pooled separately Configurable limits: Max 16 connections, 30s idle timeout (default) Tool Discovery Cache Cached with TTL: listTools() results cached for 60s Per-endpoint cache: Each MCP server cached independently Invalidation on failure: Cache cleared if server becomes unavailable Reduced latency: Subsequent requests use cached tool definitions Schema Validation Tool arguments are validated against JSON Schema before execution: [code block] Configuration Advanced configuration for testing or special scenarios: [code block] Note: These are internal APIs for advanced use cases and testing. The default configuration works well for most applications.",
    "headings": [
      "MCP Tools",
      "Automatic Tool Selection",
      "How It Works",
      "Controlling Tool Iterations",
      "Iteration Trade-offs",
      "Multiple MCP Servers",
      "Explicit Tool Calling",
      "With LLM Step",
      "When to Use Explicit Calling",
      "MCP Authentication",
      "Handle-Level Authentication",
      "Agent-Level Authentication",
      "OAuth Authentication (Client Credentials)",
      "Bearer Token Authentication",
      "Mixed Authentication",
      "Authentication Features",
      "Connection Pooling & Performance",
      "Connection Pooling",
      "Tool Discovery Cache",
      "Schema Validation",
      "Configuration"
    ],
    "path": "/docs/mcp-tools",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "tools",
      "volcano",
      "[code",
      "block]",
      "authentication",
      "multiple",
      "oauth",
      "servers",
      "cached",
      "automatic"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "import { agent, llmOpenAI, mcp } from \"volcano-sdk\";\n\nconst weather = mcp(\"http://localhost:3000/mcp\");\nconst notifications = mcp(\"http://localhost:4000/mcp\");\nconst llm = llmOpenAI({ apiKey: process.env.OPENAI_API_KEY! });\n\nawait agent({ llm })\n  .then({\n    prompt: \"Check SF weather for tomorrow and send me a notification.\",\n    mcps: [weather, notifications],\n  })\n  .run();\n\n// The LLM automatically:\n// 1. Discovers available tools from both servers\n// 2. Selects the appropriate tools\n// 3. Calls them with correct arguments\n// 4. Returns the results\n",
      "// Agent-level (default for all steps)\nawait agent({\n  llm,\n  maxToolIterations: 2, // Limit to 2 iterations (faster)\n})\n  .then({ prompt: \"Task\", mcps: [tools] })\n  .run();\n\n// Per-step override\nawait agent({ llm, maxToolIterations: 4 }) // Default 4\n  .then({\n    prompt: \"Simple task\",\n    mcps: [tools],\n    maxToolIterations: 1, // Fast: only 1 tool call\n  })\n  .then({\n    prompt: \"Complex task\",\n    mcps: [tools],\n    maxToolIterations: 4, // More iterations for complexity\n  })\n  .run();\n",
      "const database = mcp(\"http://localhost:5000/mcp\");\nconst email = mcp(\"http://localhost:5001/mcp\");\nconst analytics = mcp(\"http://localhost:5002/mcp\");\n\nawait agent({ llm })\n  .then({\n    prompt:\n      \"Query user data, analyze it, and email the report to admin@example.com\",\n    mcps: [database, email, analytics],\n  })\n  .run();\n\n// The LLM automatically orchestrates tools across all three servers\n",
      "const cafe = mcp(\"http://localhost:3000/mcp\");\n\nawait agent({ llm })\n  .then({ prompt: \"Recommend a coffee for Ava from Naples\" })\n  .then({\n    mcp: cafe,\n    tool: \"order_item\",\n    args: { item_id: \"espresso\", quantity: 2 },\n  })\n  .run();\n",
      "await agent({ llm })\n  .then({\n    mcp: database,\n    tool: \"query_users\",\n    args: { status: \"active\" },\n    prompt: \"Analyze the results and summarize\", // LLM processes tool output\n  })\n  .run();\n",
      "// OAuth on handle\nconst protectedMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"oauth\",\n    clientId: process.env.MCP_CLIENT_ID!,\n    clientSecret: process.env.MCP_CLIENT_SECRET!,\n    tokenEndpoint: \"https://api.example.com/oauth/token\",\n  },\n});\n\n// Bearer token on handle\nconst bearerMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"bearer\",\n    token: process.env.MCP_BEARER_TOKEN!,\n  },\n});\n",
      "// MCP handles without auth\nconst mcp1 = mcp(\"https://api.example.com/mcp\");\nconst mcp2 = mcp(\"https://api.other.com/mcp\");\n\n// Auth configured at agent level\nawait agent({\n  llm,\n  mcpAuth: {\n    \"https://api.example.com/mcp\": {\n      type: \"oauth\",\n      clientId: process.env.CLIENT_ID_1!,\n      clientSecret: process.env.CLIENT_SECRET_1!,\n      tokenEndpoint: \"https://api.example.com/oauth/token\",\n    },\n    \"https://api.other.com/mcp\": {\n      type: \"bearer\",\n      token: process.env.TOKEN_2!,\n    },\n  },\n})\n  .then({ prompt: \"Use tools from both servers\", mcps: [mcp1, mcp2] })\n  .run();\n",
      "const protectedMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"oauth\",\n    clientId: process.env.MCP_CLIENT_ID!,\n    clientSecret: process.env.MCP_CLIENT_SECRET!,\n    tokenEndpoint: \"https://api.example.com/oauth/token\",\n  },\n});\n\nawait agent({ llm })\n  .then({\n    prompt: \"Use protected tools\",\n    mcps: [protectedMcp],\n  })\n  .run();\n\n// Volcano automatically:\n// 1. Requests OAuth token from tokenEndpoint\n// 2. Caches the token until expiration\n// 3. Refreshes automatically when needed\n// 4. Includes Authorization header in all requests\n",
      "const authMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"bearer\",\n    token: process.env.MCP_BEARER_TOKEN!,\n  },\n});\n\nawait agent({ llm })\n  .then({ mcp: authMcp, tool: \"secure_action\", args: {} })\n  .run();\n",
      "const publicMcp = mcp(\"http://localhost:3000/mcp\"); // No auth\nconst privateMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"oauth\",\n    clientId: process.env.CLIENT_ID!,\n    clientSecret: process.env.CLIENT_SECRET!,\n    tokenEndpoint: \"https://api.example.com/oauth/token\",\n  },\n});\n\nawait agent({ llm })\n  .then({ mcp: publicMcp, tool: \"get_public_data\", args: {} })\n  .then({ mcp: privateMcp, tool: \"store_private_data\", args: {} })\n  .run();\n\n// Each server uses its own authentication (or none)\n",
      "// If the MCP tool has this schema:\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"city\": { \"type\": \"string\" },\n    \"units\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"] }\n  },\n  \"required\": [\"city\"]\n}\n\n// This call would fail validation:\nawait agent({ llm })\n  .then({\n    mcp: weather,\n    tool: \"get_weather\",\n    args: { units: \"kelvin\" }  // ❌ Missing required \"city\", invalid enum\n  })\n  .run();\n\n// Error: ValidationError\n// Message: \"arguments failed schema validation: city is required; units must be one of...\"\n",
      "import {\n  __internal_setPoolConfig,\n  __internal_setDiscoveryTtl,\n  __internal_clearDiscoveryCache,\n  __internal_clearOAuthTokenCache,\n} from \"volcano-sdk\";\n\n// Configure connection pool\n__internal_setPoolConfig(32, 60_000); // max 32 connections, 60s idle\n\n// Configure tool discovery cache TTL\n__internal_setDiscoveryTtl(120_000); // 120s cache\n\n// Clear caches (useful for testing)\n__internal_clearDiscoveryCache();\n__internal_clearOAuthTokenCache();\n"
    ]
  },
  {
    "id": "Documentation-api-mcpurl-options-mcphandle",
    "title": "mcp(url, options?): MCPHandle",
    "description": "mcp(url, options?): MCPHandle Create an MCP handle for a tool server with optional authentication: [code block] MCP Specification: Per the official MC",
    "content": "mcp(url, options?): MCPHandle Create an MCP handle for a tool server with optional authentication: [code block] MCP Specification: Per the official MCP spec, authentication uses OAuth 2.1. Volcano supports OAuth (automatic token acquisition) and Bearer (custom tokens).",
    "headings": ["mcp(url, options?): MCPHandle"],
    "path": "/docs/api#mcpurl-options-mcphandle",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "mcp(url,",
      "options?):",
      "mcphandle",
      "oauth",
      "create",
      "handle",
      "server",
      "optional",
      "authentication:",
      "[code"
    ],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "import { mcp } from \"volcano-sdk\";\n\n// No authentication\nconst astro = mcp(\"http://localhost:3211/mcp\");\n\n// OAuth authentication\nconst protectedMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"oauth\",\n    clientId: \"your-client-id\",\n    clientSecret: \"your-client-secret\",\n    tokenEndpoint: \"https://api.example.com/oauth/token\",\n  },\n});\n\n// Bearer token\nconst bearerMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"bearer\",\n    token: \"your-bearer-token\",\n  },\n});\n"
    ],
    "anchor": "mcpurl-options-mcphandle",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-api-mcphandle",
    "title": "MCPHandle",
    "description": "MCPHandle [code block]",
    "content": "MCPHandle [code block]",
    "headings": ["MCPHandle"],
    "path": "/docs/api#mcphandle",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["mcphandle", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": ["type MCPHandle = {\n  id: string;\n  url: string;\n};\n"],
    "anchor": "mcphandle",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-features-metadata-fields",
    "title": "Metadata Fields",
    "description": "Metadata Fields stepId: 0-based index of the failing step provider: llm:<id model> or mcp:<host> requestId: Upstream provider request ID when availabl",
    "content": "Metadata Fields stepId: 0-based index of the failing step provider: llm:<id model> or mcp:<host> requestId: Upstream provider request ID when available retryable: Volcano's hint (true for 429/5xx/timeouts; false for validation/4xx)",
    "headings": ["Metadata Fields"],
    "path": "/docs/features#metadata-fields",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "metadata",
      "fields",
      "stepid:",
      "0-based",
      "index",
      "failing",
      "provider:",
      "llm:<id",
      "model>",
      "mcp:<host>"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "metadata-fields",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-api-methods",
    "title": "Methods",
    "description": "Methods",
    "content": "Methods",
    "headings": ["Methods"],
    "path": "/docs/api#methods",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["methods"],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "anchor": "methods",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-observability-metric-labels",
    "title": "Metric Labels",
    "description": "Metric Labels Metrics include labels for filtering and grouping: provider - LLM provider (openai, anthropic, etc.) model - Specific model used type - ",
    "content": "Metric Labels Metrics include labels for filtering and grouping: provider - LLM provider (openai, anthropic, etc.) model - Specific model used type - Step type (llm, mcpauto, mcpexplicit) error - Boolean indicating success/failure",
    "headings": ["Metric Labels"],
    "path": "/docs/observability#metric-labels",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "metric",
      "labels",
      "provider",
      "model",
      "metrics",
      "include",
      "filtering",
      "grouping:",
      "(openai,",
      "anthropic,"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "anchor": "metric-labels",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-observability-metrics",
    "title": "Metrics",
    "description": "Metrics Volcano records metrics for dashboards and alerting.",
    "content": "Metrics Volcano records metrics for dashboards and alerting.",
    "headings": ["Metrics"],
    "path": "/docs/observability#metrics",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["metrics", "volcano", "records", "dashboards", "alerting."],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "anchor": "metrics",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-providers-mistral-provider",
    "title": "Mistral Provider",
    "description": "Mistral Provider Support for Mistral's open and proprietary models via OpenAI-compatible API. [code block]",
    "content": "Mistral Provider Support for Mistral's open and proprietary models via OpenAI-compatible API. [code block]",
    "headings": ["Mistral Provider"],
    "path": "/docs/providers#mistral-provider",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "mistral",
      "provider",
      "support",
      "mistral's",
      "proprietary",
      "models",
      "openai-compatible",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "import { llmMistral } from \"volcano-sdk\";\n\nconst mistral = llmMistral({\n  apiKey: process.env.MISTRAL_API_KEY!,\n  model: \"mistral-small-latest\",\n  baseURL: \"https://api.mistral.ai\", // Optional\n  options: {\n    temperature: 0.7,\n    max_tokens: 2000,\n    top_p: 0.9,\n    safe_prompt: true,\n    random_seed: 42,\n  },\n});\n"
    ],
    "anchor": "mistral-provider",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-mixed-authentication",
    "title": "Mixed Authentication",
    "description": "Mixed Authentication Combine authenticated and non-authenticated MCP servers in the same workflow: [code block] MCP Spec Compliance: Per the official ",
    "content": "Mixed Authentication Combine authenticated and non-authenticated MCP servers in the same workflow: [code block] MCP Spec Compliance: Per the official MCP specification, servers use OAuth 2.1 for authentication. Volcano supports both OAuth (automatic token acquisition) and Bearer (bring your own token).",
    "headings": ["Mixed Authentication"],
    "path": "/docs/mcp-tools#mixed-authentication",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "mixed",
      "authentication",
      "servers",
      "oauth",
      "combine",
      "authenticated",
      "non-authenticated",
      "workflow:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "const publicMcp = mcp(\"http://localhost:3000/mcp\"); // No auth\nconst privateMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"oauth\",\n    clientId: process.env.CLIENT_ID!,\n    clientSecret: process.env.CLIENT_SECRET!,\n    tokenEndpoint: \"https://api.example.com/oauth/token\",\n  },\n});\n\nawait agent({ llm })\n  .then({ mcp: publicMcp, tool: \"get_public_data\", args: {} })\n  .then({ mcp: privateMcp, tool: \"store_private_data\", args: {} })\n  .run();\n\n// Each server uses its own authentication (or none)\n"
    ],
    "anchor": "mixed-authentication",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-examples-more-examples",
    "title": "More Examples",
    "description": "More Examples Find more examples in the GitHub repository: All examples include automatic MCP tool selection Production-ready code you can copy and ad",
    "content": "More Examples Find more examples in the GitHub repository: All examples include automatic MCP tool selection Production-ready code you can copy and adapt Comments explaining key concepts Environment variable setup instructions",
    "headings": ["More Examples"],
    "path": "/docs/examples#more-examples",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "more",
      "examples",
      "github",
      "repository:",
      "include",
      "automatic",
      "selection",
      "production-ready",
      "adapt",
      "comments",
      "explaining"
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "anchor": "more-examples",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-multi-llm-workflows",
    "title": "Multi-LLM Workflows",
    "description": "Multi-LLM Workflows One of Volcano's most powerful features: use different LLM providers for different steps in the same workflow. Mix and match to le",
    "content": "Multi-LLM Workflows One of Volcano's most powerful features: use different LLM providers for different steps in the same workflow. Mix and match to leverage each model's strengths. Why use multiple LLMs? Different models excel at different tasks. GPT-4 for complex reasoning, Claude for detailed analysis, Mistral for creative writing, local Llama for cost-effective preprocessing. Volcano makes it seamless.",
    "headings": ["Multi-LLM Workflows"],
    "path": "/docs/patterns#multi-llm-workflows",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "multi-llm",
      "workflows",
      "different",
      "volcano's",
      "powerful",
      "features:",
      "providers",
      "steps",
      "workflow.",
      "match"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "anchor": "multi-llm-workflows",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-multi-provider-with-mcp-tools",
    "title": "Multi-Provider with MCP Tools",
    "description": "Multi-Provider with MCP Tools Combine different LLMs with automatic MCP tool selection: [code block]",
    "content": "Multi-Provider with MCP Tools Combine different LLMs with automatic MCP tool selection: [code block]",
    "headings": ["Multi-Provider with MCP Tools"],
    "path": "/docs/patterns#multi-provider-with-mcp-tools",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "multi-provider",
      "tools",
      "combine",
      "different",
      "automatic",
      "selection:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "const database = mcp(\"http://localhost:5000/mcp\");\nconst analytics = mcp(\"http://localhost:5001/mcp\");\n\nawait agent()\n  .then({\n    llm: gpt,\n    prompt: \"Query user data from database\",\n    mcps: [database],\n  })\n  .then({\n    llm: claude,\n    prompt: \"Perform statistical analysis\",\n    mcps: [analytics],\n  })\n  .then({\n    llm: mistral,\n    prompt: \"Generate executive summary\",\n  })\n  .run();\n"
    ],
    "anchor": "multi-provider-with-mcp-tools",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-multiple-mcp-servers",
    "title": "Multiple MCP Servers",
    "description": "Multiple MCP Servers Provide multiple MCP servers and the LLM will use tools from any of them: [code block]",
    "content": "Multiple MCP Servers Provide multiple MCP servers and the LLM will use tools from any of them: [code block]",
    "headings": ["Multiple MCP Servers"],
    "path": "/docs/mcp-tools#multiple-mcp-servers",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "multiple",
      "servers",
      "provide",
      "tools",
      "them:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "const database = mcp(\"http://localhost:5000/mcp\");\nconst email = mcp(\"http://localhost:5001/mcp\");\nconst analytics = mcp(\"http://localhost:5002/mcp\");\n\nawait agent({ llm })\n  .then({\n    prompt:\n      \"Query user data, analyze it, and email the report to admin@example.com\",\n    mcps: [database, email, analytics],\n  })\n  .run();\n\n// The LLM automatically orchestrates tools across all three servers\n"
    ],
    "anchor": "multiple-mcp-servers",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-named-dictionary-mode",
    "title": "Named Dictionary Mode",
    "description": "Named Dictionary Mode Access results by key for better organization: [code block]",
    "content": "Named Dictionary Mode Access results by key for better organization: [code block]",
    "headings": ["Named Dictionary Mode"],
    "path": "/docs/patterns#named-dictionary-mode",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "named",
      "dictionary",
      "mode",
      "access",
      "results",
      "better",
      "organization:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "await agent({ llm })\n  .parallel({\n    sentiment: { prompt: \"What's the sentiment?\" },\n    entities: { prompt: \"Extract key entities\" },\n    summary: { prompt: \"Summarize in 5 words\" },\n  })\n  .then((history) => {\n    const results = history[0].parallel;\n    // Access: results.sentiment, results.entities, results.summary\n    return { prompt: \"Generate report based on analysis\" };\n  })\n  .run();\n"
    ],
    "anchor": "named-dictionary-mode",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-oauth-authentication-client-credentials",
    "title": "OAuth Authentication (Client Credentials)",
    "description": "OAuth Authentication (Client Credentials) Recommended for production MCP servers. Volcano automatically acquires and refreshes tokens: [code block]",
    "content": "OAuth Authentication (Client Credentials) Recommended for production MCP servers. Volcano automatically acquires and refreshes tokens: [code block]",
    "headings": ["OAuth Authentication (Client Credentials)"],
    "path": "/docs/mcp-tools#oauth-authentication-client-credentials",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "oauth",
      "authentication",
      "(client",
      "credentials)",
      "recommended",
      "production",
      "servers.",
      "volcano",
      "automatically",
      "acquires"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "const protectedMcp = mcp(\"https://api.example.com/mcp\", {\n  auth: {\n    type: \"oauth\",\n    clientId: process.env.MCP_CLIENT_ID!,\n    clientSecret: process.env.MCP_CLIENT_SECRET!,\n    tokenEndpoint: \"https://api.example.com/oauth/token\",\n  },\n});\n\nawait agent({ llm })\n  .then({\n    prompt: \"Use protected tools\",\n    mcps: [protectedMcp],\n  })\n  .run();\n\n// Volcano automatically:\n// 1. Requests OAuth token from tokenEndpoint\n// 2. Caches the token until expiration\n// 3. Refreshes automatically when needed\n// 4. Includes Authorization header in all requests\n"
    ],
    "anchor": "oauth-authentication-client-credentials",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-observability",
    "title": "Observability - Volcano SDK",
    "content": "Observability Production-ready observability with OpenTelemetry traces and metrics. Monitor agent performance, debug failures, and optimize costs. Opt-in and fully optional. Opt-In by Design: Observability is disabled by default. Enable it by configuring telemetry when creating your agent. No performance impact when disabled. OpenTelemetry Integration Volcano SDK uses OpenTelemetry (OTEL), the industry-standard observability framework. Export traces and metrics to any OTEL-compatible backend. Quick Start Step 1: Configure OpenTelemetry SDK (one-time setup): [code block] Step 2: Enable telemetry in your agents: [code block] That's it! Volcano spans and metrics are now being sent to your observability backend. View them in Jaeger, Grafana, DataDog, NewRelic, or any OTEL-compatible system. Installation Install the OpenTelemetry API (peer dependency): [code block] Optional Dependency: If @opentelemetry/api is not installed, telemetry becomes a no-op. Your code works normally without observability. Distributed Tracing Volcano creates hierarchical traces showing the complete execution flow of your agent workflows. Trace Hierarchy [code block] Span Attributes Each span includes rich attributes for debugging: Span Type Attributes ---------------- ----------------------------------------------------------- agent.run agent.stepcount, volcano.version step.execute step.index, step.type (llm \\ mcpauto \\ mcpexplicit) llm.generate llm.provider, llm.model, llm.promptlength mcp.\\* mcp.endpoint, mcp.operation, mcp.hasauth Error Tracking When errors occur, spans include exception details: [code block] Metrics Volcano records metrics for dashboards and alerting. Available Metrics Metric Type Description ------------------------- --------- --------------------------------------------------------- volcano.agent.duration Histogram Total agent execution time (ms) volcano.step.duration Histogram Individual step duration (ms), labeled by type volcano.llm.calls.total Counter Total LLM API calls, labeled by provider and error status volcano.mcp.calls.total Counter Total MCP tool calls volcano.errors.total Counter Total errors by type and provider Metric Labels Metrics include labels for filtering and grouping: provider - LLM provider (openai, anthropic, etc.) model - Specific model used type - Step type (llm, mcpauto, mcp_explicit) error - Boolean indicating success/failure Observability Backends Volcano works with any OpenTelemetry-compatible backend. Jaeger (Distributed Tracing) [code block] Prometheus (Metrics) [code block] DataDog / NewRelic / Grafana Cloud [code block] Environment Variables OTEL supports standard environment variables for configuration: [code block] Advanced Configuration Custom Tracer and Meter Provide your own OTEL tracer and meter instances: [code block] Disable Traces or Metrics [code block] Best Practices Production only: Enable telemetry in production, disable in development for faster iteration Sampling: Use OTEL sampling for high-traffic applications to reduce costs Service naming: Use descriptive service names for easier filtering Label cardinality: Be mindful of high-cardinality labels (user IDs, etc.) Monitor costs: Observability backends charge by data volume - sample appropriately Performance: Telemetry adds minimal overhead (~1-5ms per workflow) but can increase network traffic to your observability backend. Use sampling for high-throughput applications.",
    "headings": [
      "Observability",
      "OpenTelemetry Integration",
      "Quick Start",
      "Installation",
      "Required for telemetry",
      "Optional: SDK and exporters for backends",
      "Distributed Tracing",
      "Trace Hierarchy",
      "Span Attributes",
      "Error Tracking",
      "Metrics",
      "Available Metrics",
      "Metric Labels",
      "Observability Backends",
      "Jaeger (Distributed Tracing)",
      "Prometheus (Metrics)",
      "DataDog / NewRelic / Grafana Cloud",
      "Environment Variables",
      "OTLP endpoint (your observability backend)",
      "Service name",
      "Headers (API keys, etc.)",
      "Traces only, metrics only, or both",
      "Advanced Configuration",
      "Custom Tracer and Meter",
      "Disable Traces or Metrics",
      "Best Practices"
    ],
    "path": "/docs/observability",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "observability",
      "volcano",
      "[code",
      "block]",
      "metrics",
      "opentelemetry",
      "telemetry",
      "traces",
      "backend.",
      "total"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "codeBlocks": [
      "// app.ts or index.ts - run once at startup\nimport { NodeSDK } from \"@opentelemetry/sdk-node\";\nimport { OTLPTraceExporter } from \"@opentelemetry/exporter-trace-otlp-http\";\n\n// Configure OTEL to send to your observability backend\nconst sdk = new NodeSDK({\n  serviceName: \"my-agent-service\",\n  traceExporter: new OTLPTraceExporter({\n    url: \"http://localhost:4318/v1/traces\", // Your OTEL collector\n    // For production: configure your observability backend URL\n    // url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT\n    // headers: { 'Authorization': `Bearer ${process.env.API_KEY}` }\n  }),\n});\n\nsdk.start();\n\n// Optional: Graceful shutdown\nprocess.on(\"SIGTERM\", async () => {\n  await sdk.shutdown();\n});\n",
      "import { agent, llmOpenAI, createVolcanoTelemetry } from \"volcano-sdk\";\n\n// Create telemetry (uses global OTEL SDK configured above)\nconst telemetry = createVolcanoTelemetry({\n  serviceName: \"my-agent-service\",\n});\n\n// Pass to agent\nconst results = await agent({\n  llm: llmOpenAI({ apiKey: process.env.OPENAI_API_KEY! }),\n  telemetry, // Enable observability\n})\n  .then({ prompt: \"Analyze data\" })\n  .then({ prompt: \"Generate report\" })\n  .run();\n\n// Traces and metrics automatically sent to your configured backend!\n",
      "# Required for telemetry\nnpm install @opentelemetry/api\n\n# Optional: SDK and exporters for backends\nnpm install @opentelemetry/sdk-node\nnpm install @opentelemetry/exporter-jaeger\nnpm install @opentelemetry/exporter-prometheus\nnpm install @opentelemetry/exporter-otlp-http\n",
      "Trace: agent-run-abc123\n├── Span: agent.run (parent)\n│   ├── Span: step[0].execute (type: mcp_auto)\n│   │   ├── Span: llm.generate (provider: openai, model: gpt-4o-mini)\n│   │   ├── Span: mcp.discover_tools (endpoint: http://...)\n│   │   └── Span: mcp.call_tool (tool: get_weather)\n│   ├── Span: step[1].execute (type: llm)\n│   │   └── Span: llm.generate (provider: anthropic, model: claude-3-haiku)\n│   └── Span: step[2].execute (type: mcp_explicit)\n│       └── Span: mcp.call_tool (tool: send_notification)\n",
      "// Errors are automatically recorded in spans\ntry {\n  await agent({ llm, telemetry }).then({ prompt: \"This might fail\" }).run();\n} catch (error) {\n  // Span will have:\n  // - status: ERROR\n  // - exception details\n  // - stack trace\n}\n",
      "import { NodeSDK } from \"@opentelemetry/sdk-node\";\nimport { JaegerExporter } from \"@opentelemetry/exporter-jaeger\";\nimport { createVolcanoTelemetry } from \"volcano-sdk\";\n\n// Setup OTEL SDK\nconst sdk = new NodeSDK({\n  serviceName: \"my-agent\",\n  traceExporter: new JaegerExporter({\n    endpoint: \"http://localhost:14268/api/traces\",\n  }),\n});\n\nsdk.start();\n\n// Use with Volcano\nconst telemetry = createVolcanoTelemetry({\n  serviceName: \"my-agent\",\n});\n\nawait agent({ llm, telemetry }).then({ prompt: \"...\" }).run();\n\n// View traces in Jaeger UI at http://localhost:16686\n",
      "import { NodeSDK } from \"@opentelemetry/sdk-node\";\nimport { PrometheusExporter } from \"@opentelemetry/exporter-prometheus\";\n\nconst prometheusExporter = new PrometheusExporter({\n  port: 9464,\n});\n\nconst sdk = new NodeSDK({\n  serviceName: \"my-agent\",\n  metricReader: prometheusExporter,\n});\n\nsdk.start();\n\n// Metrics available at http://localhost:9464/metrics\n// volcano_agent_duration_bucket{le=\"100\"} 42\n// volcano_llm_calls_total{provider=\"openai\"} 156\n",
      "import { OTLPTraceExporter } from \"@opentelemetry/exporter-otlp-http\";\nimport { NodeSDK } from \"@opentelemetry/sdk-node\";\n\nconst sdk = new NodeSDK({\n  serviceName: \"my-agent\",\n  traceExporter: new OTLPTraceExporter({\n    url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT,\n    headers: {\n      \"DD-API-KEY\": process.env.DD_API_KEY, // DataDog\n      // or 'X-License-Key': process.env.NEW_RELIC_KEY\n      // or 'Authorization': `Bearer ${process.env.GRAFANA_TOKEN}`\n    },\n  }),\n});\n\nsdk.start();\n",
      "# OTLP endpoint (your observability backend)\nexport OTEL_EXPORTER_OTLP_ENDPOINT=\"http://localhost:4318\"\n\n# Service name\nexport OTEL_SERVICE_NAME=\"my-agent\"\n\n# Headers (API keys, etc.)\nexport OTEL_EXPORTER_OTLP_HEADERS=\"Authorization=Bearer your-api-key\"\n\n# Traces only, metrics only, or both\nexport OTEL_TRACES_EXPORTER=\"otlp\"\nexport OTEL_METRICS_EXPORTER=\"otlp\"\n",
      "import { trace, metrics } from \"@opentelemetry/api\";\nimport { createVolcanoTelemetry } from \"volcano-sdk\";\n\nconst tracer = trace.getTracer(\"my-custom-tracer\");\nconst meter = metrics.getMeter(\"my-custom-meter\");\n\nconst telemetry = createVolcanoTelemetry({\n  serviceName: \"my-agent\",\n  tracer,\n  meter,\n});\n",
      "// Only traces, no metrics\nconst telemetry = createVolcanoTelemetry({\n  serviceName: \"my-agent\",\n  traces: true,\n  metrics: false,\n});\n\n// Only metrics, no traces\nconst telemetry = createVolcanoTelemetry({\n  serviceName: \"my-agent\",\n  traces: false,\n  metrics: true,\n});\n"
    ]
  },
  {
    "id": "Documentation-observability-observability-backends",
    "title": "Observability Backends",
    "description": "Observability Backends Volcano works with any OpenTelemetry-compatible backend.",
    "content": "Observability Backends Volcano works with any OpenTelemetry-compatible backend.",
    "headings": ["Observability Backends"],
    "path": "/docs/observability#observability-backends",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "observability",
      "backends",
      "volcano",
      "works",
      "opentelemetry-compatible",
      "backend."
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "anchor": "observability-backends",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-providers-openai-provider",
    "title": "OpenAI Provider",
    "description": "OpenAI Provider Full support for OpenAI's GPT models with function calling and streaming. [code block]",
    "content": "OpenAI Provider Full support for OpenAI's GPT models with function calling and streaming. [code block]",
    "headings": ["OpenAI Provider"],
    "path": "/docs/providers#openai-provider",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "openai",
      "provider",
      "support",
      "openai's",
      "models",
      "function",
      "calling",
      "streaming.",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "import { llmOpenAI } from \"volcano-sdk\";\n\nconst openai = llmOpenAI({\n  apiKey: process.env.OPENAI_API_KEY!,\n  model: \"gpt-4o-mini\",\n  baseURL: \"https://api.openai.com/v1\", // Optional\n  options: {\n    temperature: 0.7,\n    max_completion_tokens: 2000,\n    top_p: 0.9,\n    seed: 42,\n  },\n});\n"
    ],
    "anchor": "openai-provider",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-observability-opentelemetry-integration",
    "title": "OpenTelemetry Integration",
    "description": "OpenTelemetry Integration Volcano SDK uses OpenTelemetry (OTEL), the industry-standard observability framework. Export traces and metrics to any OTEL-",
    "content": "OpenTelemetry Integration Volcano SDK uses OpenTelemetry (OTEL), the industry-standard observability framework. Export traces and metrics to any OTEL-compatible backend.",
    "headings": ["OpenTelemetry Integration"],
    "path": "/docs/observability#opentelemetry-integration",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "opentelemetry",
      "integration",
      "volcano",
      "(otel),",
      "industry-standard",
      "observability",
      "framework.",
      "export",
      "traces",
      "metrics"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "anchor": "opentelemetry-integration",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-api-options",
    "title": "Options",
    "description": "Options Option Type Default Description ----------------------- ----------- ---------------- ---------------------------------------- llm LLMHandle - ",
    "content": "Options Option Type Default Description ----------------------- ----------- ---------------- ---------------------------------------- llm LLMHandle - Default LLM provider for all steps instructions string - Global system instructions timeout number 60 Default timeout per step (seconds) retry RetryConfig Retry configuration contextMaxChars number 20480 Soft cap for injected context size contextMaxToolResults number 8 Number of recent tool results to include",
    "headings": ["Options"],
    "path": "/docs/api#options",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "options",
      "number",
      "default",
      "instructions",
      "timeout",
      "retry",
      "option",
      "description",
      "-----------------------",
      "-----------"
    ],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "anchor": "options",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-providers-options-parameters",
    "title": "Options Parameters",
    "description": "Options Parameters Parameter Type Description ----------------------- -------------------- -----------------------------------------------------------",
    "content": "Options Parameters Parameter Type Description ----------------------- -------------------- ----------------------------------------------------------------- temperature 0-2 Controls randomness. Higher = more creative, lower = more focused maxcompletiontokens number Maximum tokens to generate (recommended for all models) maxtokens number Legacy parameter (use maxcompletiontokens instead) topp 0-1 Nucleus sampling - alternative to temperature frequencypenalty -2 to 2 Reduces repetition based on token frequency presencepenalty -2 to 2 Encourages topic diversity stop string \\ string\\[\\] Stop sequences to end generation seed number For deterministic outputs (same seed = same output) response_format object Force JSON output:",
    "headings": ["Options Parameters"],
    "path": "/docs/providers#options-parameters",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "options",
      "parameters",
      "number",
      "parameter",
      "temperature",
      "maxcompletiontokens",
      "description",
      "-----------------------",
      "--------------------",
      "-----------------------------------------------------------------"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "options-parameters",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-options-parameters",
    "title": "Options Parameters",
    "description": "Options Parameters Parameter Type Description ---------------- ---------- ------------------------------------------------- temperature 0-1 Controls r",
    "content": "Options Parameters Parameter Type Description ---------------- ---------- ------------------------------------------------- temperature 0-1 Controls randomness (note: range is 0-1, not 0-2) maxtokens number Maximum tokens to generate topp 0-1 Nucleus sampling topk number Sample from top K options stopsequences string\\[\\] Array of stop sequences thinking object Extended thinking configuration (Claude-specific)",
    "headings": ["Options Parameters"],
    "path": "/docs/providers#options-parameters",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "options",
      "parameters",
      "number",
      "thinking",
      "parameter",
      "description",
      "----------------",
      "----------",
      "-------------------------------------------------",
      "temperature"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "options-parameters",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-options-parameters",
    "title": "Options Parameters",
    "description": "Options Parameters Parameter Type Description ----------------- -------------------- ---------------------------------------- temperature 0-1 Controls",
    "content": "Options Parameters Parameter Type Description ----------------- -------------------- ---------------------------------------- temperature 0-1 Controls randomness maxtokens number Maximum tokens to generate topp 0-1 Nucleus sampling stop string \\ string\\[\\] Stop sequences safeprompt boolean Enable safety mode randomseed number For deterministic outputs response_format object For JSON mode:",
    "headings": ["Options Parameters"],
    "path": "/docs/providers#options-parameters",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "options",
      "parameters",
      "number",
      "parameter",
      "description",
      "-----------------",
      "--------------------",
      "----------------------------------------",
      "temperature",
      "controls"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "options-parameters",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-options-parameters",
    "title": "Options Parameters",
    "description": "Options Parameters Parameter Type Description ---------------- -------------------- ---------------------------------------------------- temperature 0",
    "content": "Options Parameters Parameter Type Description ---------------- -------------------- ---------------------------------------------------- temperature 0-2 Controls randomness maxtokens number Maximum tokens to generate topp 0-1 Nucleus sampling topk number Sample from top K options stop string \\ string\\[\\] Stop sequences repeatpenalty number Penalize repetitions (Ollama-specific, default: 1.1) seed number For deterministic outputs num_predict number Number of tokens to predict (Ollama-specific)",
    "headings": ["Options Parameters"],
    "path": "/docs/providers#options-parameters",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "options",
      "parameters",
      "number",
      "tokens",
      "parameter",
      "description",
      "----------------",
      "--------------------",
      "----------------------------------------------------",
      "temperature"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "options-parameters",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-options-parameters",
    "title": "Options Parameters",
    "description": "Options Parameters Parameter Type Description ---------------- ---------- -------------------------- temperature 0-1 Controls randomness maxtokens num",
    "content": "Options Parameters Parameter Type Description ---------------- ---------- -------------------------- temperature 0-1 Controls randomness maxtokens number Maximum tokens to generate topp 0-1 Nucleus sampling stop_sequences string\\[\\] Array of stop sequences",
    "headings": ["Options Parameters"],
    "path": "/docs/providers#options-parameters",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "options",
      "parameters",
      "parameter",
      "description",
      "----------------",
      "----------",
      "--------------------------",
      "temperature",
      "controls",
      "randomness"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "options-parameters",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-options-parameters",
    "title": "Options Parameters",
    "description": "Options Parameters Parameter Type Description -------------------- ---------- ----------------------------------------- temperature 0-2 Controls rando",
    "content": "Options Parameters Parameter Type Description -------------------- ---------- ----------------------------------------- temperature 0-2 Controls randomness maxoutputtokens number Maximum tokens to generate topp 0-1 Nucleus sampling topk number Sample from top K options stopsequences string\\[\\] Array of stop sequences candidatecount number Number of response variations (usually 1) responsemimetype string For JSON mode: \"application/json\" Function calling limitations: Multiple tools per call only supported for search tools. Use multi-step workflows for complex tool orchestration.",
    "headings": ["Options Parameters"],
    "path": "/docs/providers#options-parameters",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "options",
      "parameters",
      "number",
      "parameter",
      "description",
      "--------------------",
      "----------",
      "-----------------------------------------",
      "temperature",
      "controls"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "options-parameters",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-providers-options-parameters",
    "title": "Options Parameters",
    "description": "Options Parameters Important: Azure Responses API currently does not support optional configuration parameters. All inference parameters (temperature,",
    "content": "Options Parameters Important: Azure Responses API currently does not support optional configuration parameters. All inference parameters (temperature, max_tokens, seed, etc.) are rejected with HTTP 400 errors. This is a limitation of Azure's Responses API endpoint. The AzureOptions type is defined for API consistency but parameters cannot be used in practice.",
    "headings": ["Options Parameters"],
    "path": "/docs/providers#options-parameters",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "options",
      "parameters",
      "responses",
      "important:",
      "azure",
      "currently",
      "support",
      "optional",
      "configuration",
      "parameters."
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "options-parameters",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-parallel-execution",
    "title": "Parallel Execution",
    "description": "Parallel Execution Execute multiple steps simultaneously for faster workflows.",
    "content": "Parallel Execution Execute multiple steps simultaneously for faster workflows.",
    "headings": ["Parallel Execution"],
    "path": "/docs/patterns#parallel-execution",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "parallel",
      "execution",
      "execute",
      "multiple",
      "steps",
      "simultaneously",
      "faster",
      "workflows."
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "anchor": "parallel-execution",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-examples-parallelts",
    "title": "parallel.ts",
    "description": "parallel.ts Execute multiple steps in parallel with automatic MCP tool selection in each parallel branch. [code block] Demonstrates: .parallel() with ",
    "content": "parallel.ts Execute multiple steps in parallel with automatic MCP tool selection in each parallel branch. [code block] Demonstrates: .parallel() with array and dictionary modes, automatic tool selection View source on GitHub →",
    "headings": ["parallel.ts"],
    "path": "/docs/examples#parallelts",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "parallel.ts",
      "parallel",
      "automatic",
      "selection",
      "execute",
      "multiple",
      "steps",
      "branch.",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "codeBlocks": ["npx tsx examples/parallel.ts\n"],
    "anchor": "parallelts",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-features-per-step-override",
    "title": "Per-Step Override",
    "description": "Per-Step Override [code block]",
    "content": "Per-Step Override [code block]",
    "headings": ["Per-Step Override"],
    "path": "/docs/features#per-step-override",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["per-step", "override", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "codeBlocks": [
      "await agent({ llm, retry: { delay: 20 } })\n  .then({ prompt: \"override to immediate\", retry: { delay: 0 } })\n  .run();\n"
    ],
    "anchor": "per-step-override",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-examples-prerequisites",
    "title": "Prerequisites",
    "description": "Prerequisites ```bash",
    "content": "Prerequisites ```bash",
    "headings": ["Prerequisites"],
    "path": "/docs/examples#prerequisites",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["prerequisites", "```bash"],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "anchor": "prerequisites",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-observability-prometheus-metrics",
    "title": "Prometheus (Metrics)",
    "description": "Prometheus (Metrics) [code block]",
    "content": "Prometheus (Metrics) [code block]",
    "headings": ["Prometheus (Metrics)"],
    "path": "/docs/observability#prometheus-metrics",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["prometheus", "(metrics)", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "codeBlocks": [
      "import { NodeSDK } from \"@opentelemetry/sdk-node\";\nimport { PrometheusExporter } from \"@opentelemetry/exporter-prometheus\";\n\nconst prometheusExporter = new PrometheusExporter({\n  port: 9464,\n});\n\nconst sdk = new NodeSDK({\n  serviceName: \"my-agent\",\n  metricReader: prometheusExporter,\n});\n\nsdk.start();\n\n// Metrics available at http://localhost:9464/metrics\n// volcano_agent_duration_bucket{le=\"100\"} 42\n// volcano_llm_calls_total{provider=\"openai\"} 156\n"
    ],
    "anchor": "prometheus-metrics",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-providers-provider-support-matrix",
    "title": "Provider Support Matrix",
    "description": "Provider Support Matrix Provider Basic Generation Function Calling Streaming MCP Integration ------------------------ ---------------- ---------------",
    "content": "Provider Support Matrix Provider Basic Generation Function Calling Streaming MCP Integration ------------------------ ---------------- ---------------------------- --------- --------------- OpenAI ✅ Full ✅ Native ✅ Native ✅ Complete Anthropic ✅ Full ✅ Native (tool_use) ✅ Native ✅ Complete Mistral ✅ Full ✅ Native ✅ Native ✅ Complete Llama ✅ Full ✅ Via Ollama ✅ Native ✅ Complete AWS Bedrock ✅ Full ✅ Native (Converse API) ✅ Native ✅ Complete Google Vertex Studio ✅ Full ✅ Native (Function calling) ✅ Native ✅ Complete Azure AI ✅ Full ✅ Native (Responses API) ✅ Native ✅ Complete All providers support automatic tool selection and multi-step workflows.",
    "headings": ["Provider Support Matrix"],
    "path": "/docs/providers#provider-support-matrix",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "provider",
      "support",
      "matrix",
      "native",
      "complete",
      "basic",
      "generation",
      "function",
      "calling",
      "streaming"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "provider-support-matrix",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-examples-providersts",
    "title": "providers.ts",
    "description": "providers.ts Multi-provider workflow using OpenAI, Anthropic, Mistral, and Llama with automatic MCP tool selection. [code block] Demonstrates: Per-ste",
    "content": "providers.ts Multi-provider workflow using OpenAI, Anthropic, Mistral, and Llama with automatic MCP tool selection. [code block] Demonstrates: Per-step LLM provider switching, automatic tool selection, context flow between providers View source on GitHub →",
    "headings": ["providers.ts"],
    "path": "/docs/examples#providersts",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "providers.ts",
      "automatic",
      "multi-provider",
      "workflow",
      "using",
      "openai,",
      "anthropic,",
      "mistral,",
      "llama",
      "selection."
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "codeBlocks": ["npx tsx examples/providers.ts\n"],
    "anchor": "providersts",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-observability-quick-start",
    "title": "Quick Start",
    "description": "Quick Start Step 1: Configure OpenTelemetry SDK (one-time setup): [code block] Step 2: Enable telemetry in your agents: [code block] That's it! Volcan",
    "content": "Quick Start Step 1: Configure OpenTelemetry SDK (one-time setup): [code block] Step 2: Enable telemetry in your agents: [code block] That's it! Volcano spans and metrics are now being sent to your observability backend. View them in Jaeger, Grafana, DataDog, NewRelic, or any OTEL-compatible system.",
    "headings": ["Quick Start"],
    "path": "/docs/observability#quick-start",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "quick",
      "start",
      "[code",
      "block]",
      "configure",
      "opentelemetry",
      "(one-time",
      "setup):",
      "enable",
      "telemetry"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "codeBlocks": [
      "// app.ts or index.ts - run once at startup\nimport { NodeSDK } from \"@opentelemetry/sdk-node\";\nimport { OTLPTraceExporter } from \"@opentelemetry/exporter-trace-otlp-http\";\n\n// Configure OTEL to send to your observability backend\nconst sdk = new NodeSDK({\n  serviceName: \"my-agent-service\",\n  traceExporter: new OTLPTraceExporter({\n    url: \"http://localhost:4318/v1/traces\", // Your OTEL collector\n    // For production: configure your observability backend URL\n    // url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT\n    // headers: { 'Authorization': `Bearer ${process.env.API_KEY}` }\n  }),\n});\n\nsdk.start();\n\n// Optional: Graceful shutdown\nprocess.on(\"SIGTERM\", async () => {\n  await sdk.shutdown();\n});\n",
      "import { agent, llmOpenAI, createVolcanoTelemetry } from \"volcano-sdk\";\n\n// Create telemetry (uses global OTEL SDK configured above)\nconst telemetry = createVolcanoTelemetry({\n  serviceName: \"my-agent-service\",\n});\n\n// Pass to agent\nconst results = await agent({\n  llm: llmOpenAI({ apiKey: process.env.OPENAI_API_KEY! }),\n  telemetry, // Enable observability\n})\n  .then({ prompt: \"Analyze data\" })\n  .then({ prompt: \"Generate report\" })\n  .run();\n\n// Traces and metrics automatically sent to your configured backend!\n"
    ],
    "anchor": "quick-start",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-providers-requirements",
    "title": "Requirements",
    "description": "Requirements gen(): Basic text generation method (required) genWithTools(): Function calling support (required for MCP tools) genStream(): Streaming s",
    "content": "Requirements gen(): Basic text generation method (required) genWithTools(): Function calling support (required for MCP tools) genStream(): Streaming support (optional) id & model: Identifiers for logging and error messages Tip: Look at existing providers in src/llms/ for reference implementations. The OpenAI provider is a good starting point.",
    "headings": ["Requirements"],
    "path": "/docs/providers#requirements",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "requirements",
      "support",
      "gen():",
      "basic",
      "generation",
      "method",
      "(required)",
      "genwithtools():",
      "function",
      "calling"
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "anchor": "requirements",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-features-retries-timeouts",
    "title": "Retries & Timeouts",
    "description": "Retries & Timeouts",
    "content": "Retries & Timeouts",
    "headings": ["Retries & Timeouts"],
    "path": "/docs/features#retries-timeouts",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["retries", "timeouts"],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "retries-timeouts",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-features-retry-semantics",
    "title": "Retry Semantics",
    "description": "Retry Semantics Non-retryable errors (like ValidationError) abort immediately Retryable errors include: timeouts, 429, 5xx, network errors On retry ex",
    "content": "Retry Semantics Non-retryable errors (like ValidationError) abort immediately Retryable errors include: timeouts, 429, 5xx, network errors On retry exhaustion, the last error is thrown You cannot set both delay and backoff",
    "headings": ["Retry Semantics"],
    "path": "/docs/features#retry-semantics",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "retry",
      "semantics",
      "errors",
      "non-retryable",
      "(like",
      "validationerror)",
      "abort",
      "immediately",
      "retryable",
      "include:"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "retry-semantics",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-features-retry-strategies",
    "title": "Retry Strategies",
    "description": "Retry Strategies",
    "content": "Retry Strategies",
    "headings": ["Retry Strategies"],
    "path": "/docs/features#retry-strategies",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["retry", "strategies"],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "retry-strategies",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-retry-until-success",
    "title": "Retry Until Success",
    "description": "Retry Until Success Self-correcting agents that retry until a success condition is met: [code block]",
    "content": "Retry Until Success Self-correcting agents that retry until a success condition is met: [code block]",
    "headings": ["Retry Until Success"],
    "path": "/docs/patterns#retry-until-success",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "retry",
      "until",
      "success",
      "self-correcting",
      "agents",
      "condition",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "await agent({ llm })\n  .retryUntil(\n    (a) => a.then({ prompt: \"Generate a haiku about AI\" }),\n    (result) => {\n      // Validate 5-7-5 syllable structure\n      const lines = result.llmOutput?.split(\"\\n\") || [];\n      return lines.length === 3; // Simple validation\n    },\n    { maxAttempts: 5, backoff: 1.5 }\n  )\n  .run();\n"
    ],
    "anchor": "retry-until-success",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-api-retryconfig",
    "title": "RetryConfig",
    "description": "RetryConfig Configuration for retry behavior: [code block]",
    "content": "RetryConfig Configuration for retry behavior: [code block]",
    "headings": ["RetryConfig"],
    "path": "/docs/api#retryconfig",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "retryconfig",
      "configuration",
      "retry",
      "behavior:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "type RetryConfig = {\n  delay?: number; // Seconds to wait between retries (mutually exclusive with backoff)\n  backoff?: number; // Exponential factor (waits 1s, factor^n each retry)\n  retries?: number; // Total attempts including first (default: 3)\n};\n"
    ],
    "anchor": "retryconfig",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-features-return-value",
    "title": "Return Value",
    "description": "Return Value Returns Promise<StepResult[]> with all step results: [code block]",
    "content": "Return Value Returns Promise<StepResult[]> with all step results: [code block]",
    "headings": ["Return Value"],
    "path": "/docs/features#return-value",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "return",
      "value",
      "returns",
      "promise<stepresult[]>",
      "results:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "codeBlocks": [
      "type StepResult = {\n  prompt?: string;\n  llmOutput?: string;\n  durationMs?: number;\n  llmMs?: number;\n  toolCalls?: Array<{ name: string; result: any; ms?: number }>;\n  // Aggregated metrics (on final step only):\n  totalDurationMs?: number;\n  totalLlmMs?: number;\n  totalMcpMs?: number;\n};\n"
    ],
    "anchor": "return-value",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-examples-run-an-example",
    "title": "Run an Example",
    "description": "Run an Example ```bash",
    "content": "Run an Example ```bash",
    "headings": ["Run an Example"],
    "path": "/docs/examples#run-an-example",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["example", "```bash"],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "anchor": "run-an-example",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-features-run-method",
    "title": "run() Method",
    "description": "run() Method Execute the complete agent workflow and return all step results at once. [code block]",
    "content": "run() Method Execute the complete agent workflow and return all step results at once. [code block]",
    "headings": ["run() Method"],
    "path": "/docs/features#run-method",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "run()",
      "method",
      "execute",
      "complete",
      "agent",
      "workflow",
      "return",
      "results",
      "once.",
      "[code"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "codeBlocks": [
      "const results = await agent({ llm })\n  .then({ prompt: \"Analyze user data\", mcps: [analytics] })\n  .then({ prompt: \"Generate insights\" })\n  .then({ prompt: \"Create recommendations\" })\n  .run();\n\n// All steps complete before results are returned\nconsole.log(results); // Array of all StepResult objects\nconsole.log(results[0].llmOutput); // First step output\nconsole.log(results[1].llmOutput); // Second step output\nconsole.log(results[results.length - 1].totalDurationMs); // Total time\n"
    ],
    "anchor": "run-method",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-examples-running-examples",
    "title": "Running Examples",
    "description": "Running Examples",
    "content": "Running Examples",
    "headings": ["Running Examples"],
    "path": "/docs/examples#running-examples",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["running", "examples"],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "anchor": "running-examples",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-schema-validation",
    "title": "Schema Validation",
    "description": "Schema Validation Tool arguments are validated against JSON Schema before execution: [code block]",
    "content": "Schema Validation Tool arguments are validated against JSON Schema before execution: [code block]",
    "headings": ["Schema Validation"],
    "path": "/docs/mcp-tools#schema-validation",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "schema",
      "validation",
      "arguments",
      "validated",
      "against",
      "before",
      "execution:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "// If the MCP tool has this schema:\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"city\": { \"type\": \"string\" },\n    \"units\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"] }\n  },\n  \"required\": [\"city\"]\n}\n\n// This call would fail validation:\nawait agent({ llm })\n  .then({\n    mcp: weather,\n    tool: \"get_weather\",\n    args: { units: \"kelvin\" }  // ❌ Missing required \"city\", invalid enum\n  })\n  .run();\n\n// Error: ValidationError\n// Message: \"arguments failed schema validation: city is required; units must be one of...\"\n"
    ],
    "anchor": "schema-validation",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-examples-set-environment-variables",
    "title": "Set Environment Variables",
    "description": "Set Environment Variables Most examples require API keys. Set them in your environment: ```bash export OPENAIAPIKEY=\"your-openai-key\" export ANTHROPIC",
    "content": "Set Environment Variables Most examples require API keys. Set them in your environment: ```bash export OPENAIAPIKEY=\"your-openai-key\" export ANTHROPICAPIKEY=\"your-anthropic-key\" export MISTRALAPIKEY=\"your-mistral-key\"",
    "headings": ["Set Environment Variables"],
    "path": "/docs/examples#set-environment-variables",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "environment",
      "variables",
      "export",
      "examples",
      "require",
      "keys.",
      "environment:",
      "```bash",
      "openaiapikey=\"your-openai-key\"",
      "anthropicapikey=\"your-anthropic-key\""
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "anchor": "set-environment-variables",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-observability-span-attributes",
    "title": "Span Attributes",
    "description": "Span Attributes Each span includes rich attributes for debugging: Span Type Attributes ---------------- ----------------------------------------------",
    "content": "Span Attributes Each span includes rich attributes for debugging: Span Type Attributes ---------------- ----------------------------------------------------------- agent.run agent.stepcount, volcano.version step.execute step.index, step.type (llm \\ mcpauto \\ mcpexplicit) llm.generate llm.provider, llm.model, llm.promptlength mcp.\\* mcp.endpoint, mcp.operation, mcp.has_auth",
    "headings": ["Span Attributes"],
    "path": "/docs/observability#span-attributes",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "span",
      "attributes",
      "includes",
      "debugging:",
      "----------------",
      "-----------------------------------------------------------",
      "agent.run",
      "agent.stepcount,",
      "volcano.version",
      "step.execute",
      "step.index,"
    ],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "anchor": "span-attributes",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-examples-special-features",
    "title": "Special Features",
    "description": "Special Features",
    "content": "Special Features",
    "headings": ["Special Features"],
    "path": "/docs/examples#special-features",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["special", "features"],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "anchor": "special-features",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-features-step-hooks",
    "title": "Step Hooks",
    "description": "Step Hooks Add pre and post hooks for fine-grained control over execution flow: [code block]",
    "content": "Step Hooks Add pre and post hooks for fine-grained control over execution flow: [code block]",
    "headings": ["Step Hooks"],
    "path": "/docs/features#step-hooks",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "step",
      "hooks",
      "fine-grained",
      "control",
      "execution",
      "flow:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "codeBlocks": [
      "await agent({ llm })\n  .then({\n    prompt: \"Analyze the user data\",\n    mcps: [analytics],\n    pre: () => {\n      console.log(\"Starting analysis...\");\n    },\n    post: () => {\n      console.log(\"Analysis complete!\");\n    },\n  })\n  .then({\n    prompt: \"Generate report\",\n    pre: () => {\n      startTimer();\n    },\n    post: () => {\n      endTimer();\n      saveMetrics();\n    },\n  })\n  .run((step, stepIndex) => {\n    console.log(`Step ${stepIndex + 1} finished`);\n  });\n"
    ],
    "anchor": "step-hooks",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-api-step-results",
    "title": "Step Results",
    "description": "Step Results Each step returns a StepResult object with execution details: [code block]",
    "content": "Step Results Each step returns a StepResult object with execution details: [code block]",
    "headings": ["Step Results"],
    "path": "/docs/api#step-results",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "step",
      "results",
      "returns",
      "stepresult",
      "object",
      "execution",
      "details:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "type StepResult = {\n  // Basic fields\n  prompt?: string;\n  llmOutput?: string;\n\n  // Timing metrics (milliseconds)\n  durationMs?: number; // Total step wall time\n  llmMs?: number; // Time spent in LLM calls\n\n  // MCP tool results (explicit call)\n  mcp?: {\n    endpoint: string;\n    tool: string;\n    result: any;\n    ms?: number; // Tool execution time\n  };\n\n  // MCP tool calls (automatic selection)\n  toolCalls?: Array<{\n    name: string;\n    endpoint: string;\n    result: any;\n    ms?: number; // Tool execution time\n  }>;\n\n  // Parallel execution results\n  parallel?: Record<string, StepResult>;\n  parallelResults?: StepResult[];\n\n  // Aggregated metrics (on final step only)\n  totalDurationMs?: number;\n  totalLlmMs?: number;\n  totalMcpMs?: number;\n};\n"
    ],
    "anchor": "step-results",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-api-step-types",
    "title": "Step Types",
    "description": "Step Types Volcano SDK supports three types of steps:",
    "content": "Step Types Volcano SDK supports three types of steps:",
    "headings": ["Step Types"],
    "path": "/docs/api#step-types",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["step", "types", "volcano", "supports", "three", "steps:"],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "anchor": "step-types",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-features-stream-method",
    "title": "stream() Method",
    "description": "stream() Method Stream step results in real-time as they complete using the stream() method. [code block]",
    "content": "stream() Method Stream step results in real-time as they complete using the stream() method. [code block]",
    "headings": ["stream() Method"],
    "path": "/docs/features#stream-method",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "stream()",
      "method",
      "stream",
      "results",
      "real-time",
      "complete",
      "using",
      "method.",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "codeBlocks": [
      "for await (const stepResult of agent({ llm })\n  .then({ prompt: \"Analyze user data\", mcps: [analytics] })\n  .then({ prompt: \"Generate insights\" })\n  .then({ prompt: \"Create recommendations\" })\n  .stream()) {\n  console.log(`Step completed: ${stepResult.prompt}`);\n  console.log(`Duration: ${stepResult.durationMs}ms`);\n\n  if (stepResult.llmOutput) {\n    console.log(`Result: ${stepResult.llmOutput}`);\n  }\n}\n"
    ],
    "anchor": "stream-method",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-features-streaming-with-progress-tracking",
    "title": "Streaming with Progress Tracking",
    "description": "Streaming with Progress Tracking [code block]",
    "content": "Streaming with Progress Tracking [code block]",
    "headings": ["Streaming with Progress Tracking"],
    "path": "/docs/features#streaming-with-progress-tracking",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["streaming", "progress", "tracking", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "codeBlocks": [
      "let completedSteps = 0;\nconst totalSteps = 3;\n\nfor await (const stepResult of workflow.stream((step, stepIndex) => {\n  completedSteps++;\n  console.log(`Progress: ${completedSteps}/${totalSteps}`);\n})) {\n  updateProgressBar(completedSteps / totalSteps);\n  displayStepResult(stepResult);\n}\n"
    ],
    "anchor": "streaming-with-progress-tracking",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-examples-streamingts",
    "title": "streaming.ts",
    "description": "streaming.ts Stream step results in real-time with automatic MCP tool selection. [code block] Demonstrates: .stream() method for progressive result de",
    "content": "streaming.ts Stream step results in real-time with automatic MCP tool selection. [code block] Demonstrates: .stream() method for progressive result delivery, tool selection in streaming mode View source on GitHub →",
    "headings": ["streaming.ts"],
    "path": "/docs/examples#streamingts",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "streaming.ts",
      "stream",
      "results",
      "real-time",
      "automatic",
      "selection.",
      "[code",
      "block]",
      "demonstrates:",
      ".stream()"
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "codeBlocks": ["npx tsx examples/streaming.ts\n"],
    "anchor": "streamingts",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-providers-structured-outputs-json-schema-validation",
    "title": "Structured Outputs (JSON Schema Validation)",
    "description": "Structured Outputs (JSON Schema Validation) Use llmOpenAIResponses() for guaranteed JSON output that matches your schema exactly. Unlike basic JSON mo",
    "content": "Structured Outputs (JSON Schema Validation) Use llmOpenAIResponses() for guaranteed JSON output that matches your schema exactly. Unlike basic JSON mode, structured outputs enforce strict schema compliance. [code block]",
    "headings": ["Structured Outputs (JSON Schema Validation)"],
    "path": "/docs/providers#structured-outputs-json-schema-validation",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "structured",
      "outputs",
      "(json",
      "schema",
      "validation)",
      "llmopenairesponses()",
      "guaranteed",
      "output",
      "matches",
      "exactly."
    ],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "import { llmOpenAIResponses } from \"volcano-sdk\";\n\nconst llm = llmOpenAIResponses({\n  apiKey: process.env.OPENAI_API_KEY!,\n  model: \"gpt-4o-mini\",\n  options: {\n    jsonSchema: {\n      name: \"order_response\",\n      description: \"Order information\",\n      schema: {\n        type: \"object\",\n        properties: {\n          item: { type: \"string\" },\n          price: { type: \"number\" },\n          category: { type: \"string\" },\n        },\n        required: [\"item\", \"price\", \"category\"],\n        additionalProperties: false,\n      },\n    },\n  },\n});\n\nconst response = await llm.gen(\"Info for Espresso: $5.25, Coffee\");\nconst data = JSON.parse(response);\n// Guaranteed: { item: \"Espresso\", price: 5.25, category: \"Coffee\" }\n"
    ],
    "anchor": "structured-outputs-json-schema-validation",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-sub-agent-composition",
    "title": "Sub-Agent Composition",
    "description": "Sub-Agent Composition Build reusable agent components and compose them into larger workflows.",
    "content": "Sub-Agent Composition Build reusable agent components and compose them into larger workflows.",
    "headings": ["Sub-Agent Composition"],
    "path": "/docs/patterns#sub-agent-composition",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "sub-agent",
      "composition",
      "build",
      "reusable",
      "agent",
      "components",
      "compose",
      "larger",
      "workflows."
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "anchor": "sub-agent-composition",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-examples-sub-agentsts-compositionts",
    "title": "sub-agents.ts & composition.ts",
    "description": "sub-agents.ts & composition.ts Build reusable sub-agents with automatic MCP tool integration. [code block] Demonstrates: .runAgent() for modular agent",
    "content": "sub-agents.ts & composition.ts Build reusable sub-agents with automatic MCP tool integration. [code block] Demonstrates: .runAgent() for modular agent design with tool selection View source on GitHub →",
    "headings": ["sub-agents.ts & composition.ts"],
    "path": "/docs/examples#sub-agentsts-compositionts",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "sub-agents.ts",
      "composition.ts",
      "build",
      "reusable",
      "sub-agents",
      "automatic",
      "integration.",
      "[code",
      "block]",
      "demonstrates:"
    ],
    "lastModified": "2025-10-10T20:24:48.994Z",
    "codeBlocks": [
      "npx tsx examples/sub-agents.ts\nnpx tsx examples/composition.ts\n"
    ],
    "anchor": "sub-agentsts-compositionts",
    "parentTitle": "Examples - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-switchcase-branching",
    "title": "Switch/Case Branching",
    "description": "Switch/Case Branching Handle multiple branches with a default fallback: [code block]",
    "content": "Switch/Case Branching Handle multiple branches with a default fallback: [code block]",
    "headings": ["Switch/Case Branching"],
    "path": "/docs/patterns#switchcase-branching",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "switch/case",
      "branching",
      "handle",
      "multiple",
      "branches",
      "default",
      "fallback:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "await agent({ llm })\n  .then({ prompt: \"Classify ticket priority: HIGH, MEDIUM, or LOW\" })\n  .switch((history) => history[0].llmOutput?.toUpperCase().trim() || \"\", {\n    HIGH: (a) => a.then({ mcp: pagerduty, tool: \"create_incident\" }),\n    MEDIUM: (a) => a.then({ mcp: jira, tool: \"create_ticket\" }),\n    LOW: (a) => a.then({ mcp: email, tool: \"queue_for_review\" }),\n    default: (a) => a.then({ prompt: \"Escalate unknown priority\" }),\n  })\n  .run();\n"
    ],
    "anchor": "switchcase-branching",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-features-timeouts",
    "title": "Timeouts",
    "description": "Timeouts Set per-step or global timeouts (in seconds): [code block]",
    "content": "Timeouts Set per-step or global timeouts (in seconds): [code block]",
    "headings": ["Timeouts"],
    "path": "/docs/features#timeouts",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "timeouts",
      "per-step",
      "global",
      "seconds):",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "codeBlocks": [
      "await agent({ llm, timeout: 60 })\n  .then({ prompt: \"Quick check\", timeout: 5 }) // Override to 5s\n  .then({ prompt: \"Next step uses agent default (60s)\" })\n  .run();\n"
    ],
    "anchor": "timeouts",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-tool-discovery-cache",
    "title": "Tool Discovery Cache",
    "description": "Tool Discovery Cache Cached with TTL: listTools() results cached for 60s Per-endpoint cache: Each MCP server cached independently Invalidation on fail",
    "content": "Tool Discovery Cache Cached with TTL: listTools() results cached for 60s Per-endpoint cache: Each MCP server cached independently Invalidation on failure: Cache cleared if server becomes unavailable Reduced latency: Subsequent requests use cached tool definitions",
    "headings": ["Tool Discovery Cache"],
    "path": "/docs/mcp-tools#tool-discovery-cache",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "tool",
      "discovery",
      "cache",
      "cached",
      "server",
      "listtools()",
      "results",
      "per-endpoint",
      "cache:",
      "independently",
      "invalidation"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "anchor": "tool-discovery-cache",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-api-tooldefinition",
    "title": "ToolDefinition",
    "description": "ToolDefinition [code block]",
    "content": "ToolDefinition [code block]",
    "headings": ["ToolDefinition"],
    "path": "/docs/api#tooldefinition",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["tooldefinition", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "codeBlocks": [
      "type ToolDefinition = {\n  name: string;\n  description: string;\n  parameters: any; // JSON Schema\n  mcpHandle?: MCPHandle;\n};\n"
    ],
    "anchor": "tooldefinition",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-observability-trace-hierarchy",
    "title": "Trace Hierarchy",
    "description": "Trace Hierarchy [code block]",
    "content": "Trace Hierarchy [code block]",
    "headings": ["Trace Hierarchy"],
    "path": "/docs/observability#trace-hierarchy",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["trace", "hierarchy", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.805Z",
    "codeBlocks": [
      "Trace: agent-run-abc123\n├── Span: agent.run (parent)\n│   ├── Span: step[0].execute (type: mcp_auto)\n│   │   ├── Span: llm.generate (provider: openai, model: gpt-4o-mini)\n│   │   ├── Span: mcp.discover_tools (endpoint: http://...)\n│   │   └── Span: mcp.call_tool (tool: get_weather)\n│   ├── Span: step[1].execute (type: llm)\n│   │   └── Span: llm.generate (provider: anthropic, model: claude-3-haiku)\n│   └── Span: step[2].execute (type: mcp_explicit)\n│       └── Span: mcp.call_tool (tool: send_notification)\n"
    ],
    "anchor": "trace-hierarchy",
    "parentTitle": "Observability - Volcano SDK"
  },
  {
    "id": "Documentation-api-type-reference",
    "title": "Type Reference",
    "description": "Type Reference",
    "content": "Type Reference",
    "headings": ["Type Reference"],
    "path": "/docs/api#type-reference",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["type", "reference"],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "anchor": "type-reference",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-features-use-cases",
    "title": "Use Cases",
    "description": "Use Cases Performance monitoring and timing Logging and debugging State management Notifications and alerts Metrics collection",
    "content": "Use Cases Performance monitoring and timing Logging and debugging State management Notifications and alerts Metrics collection",
    "headings": ["Use Cases"],
    "path": "/docs/features#use-cases",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "cases",
      "performance",
      "monitoring",
      "timing",
      "logging",
      "debugging",
      "state",
      "management",
      "notifications",
      "alerts"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "use-cases",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-use-cases",
    "title": "Use Cases",
    "description": "Use Cases Email triage and routing Content moderation with different actions Customer support ticket classification Approval workflows",
    "content": "Use Cases Email triage and routing Content moderation with different actions Customer support ticket classification Approval workflows",
    "headings": ["Use Cases"],
    "path": "/docs/patterns#use-cases",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "cases",
      "email",
      "triage",
      "routing",
      "content",
      "moderation",
      "different",
      "actions",
      "customer",
      "support"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "anchor": "use-cases",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-use-cases",
    "title": "Use Cases",
    "description": "Use Cases Batch processing of items Data pagination and processing Email campaigns Self-correcting content generation Iterative refinement",
    "content": "Use Cases Batch processing of items Data pagination and processing Email campaigns Self-correcting content generation Iterative refinement",
    "headings": ["Use Cases"],
    "path": "/docs/patterns#use-cases",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "cases",
      "processing",
      "batch",
      "items",
      "pagination",
      "email",
      "campaigns",
      "self-correcting",
      "content",
      "generation"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "anchor": "use-cases",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-providers-using-your-custom-provider",
    "title": "Using Your Custom Provider",
    "description": "Using Your Custom Provider [code block]",
    "content": "Using Your Custom Provider [code block]",
    "headings": ["Using Your Custom Provider"],
    "path": "/docs/providers#using-your-custom-provider",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["using", "your", "custom", "provider", "[code", "block]"],
    "lastModified": "2025-10-11T05:21:51.880Z",
    "codeBlocks": [
      "import { agent } from \"volcano-sdk\";\nimport { llmCustom } from \"./my-custom-provider\";\n\nconst customLLM = llmCustom({\n  apiKey: process.env.CUSTOM_API_KEY!,\n  model: \"my-model-v1\",\n  baseURL: \"https://api.myservice.com/v1\",\n});\n\nconst results = await agent({ llm: customLLM })\n  .then({ prompt: \"Hello from custom provider!\" })\n  .run();\n\nconsole.log(results[0].llmOutput);\n"
    ],
    "anchor": "using-your-custom-provider",
    "parentTitle": "LLM Providers - Volcano SDK"
  },
  {
    "id": "Documentation-api-utility-functions",
    "title": "Utility Functions",
    "description": "Utility Functions",
    "content": "Utility Functions",
    "headings": ["Utility Functions"],
    "path": "/docs/api#utility-functions",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["utility", "functions"],
    "lastModified": "2025-10-11T05:21:51.687Z",
    "anchor": "utility-functions",
    "parentTitle": "API Reference - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-when-to-use-explicit-calling",
    "title": "When to Use Explicit Calling",
    "description": "When to Use Explicit Calling You know exactly which tool to call The arguments are predetermined You want fine-grained control over execution order Yo",
    "content": "When to Use Explicit Calling You know exactly which tool to call The arguments are predetermined You want fine-grained control over execution order You're building deterministic workflows",
    "headings": ["When to Use Explicit Calling"],
    "path": "/docs/mcp-tools#when-to-use-explicit-calling",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "when",
      "explicit",
      "calling",
      "exactly",
      "arguments",
      "predetermined",
      "fine-grained",
      "control",
      "execution",
      "order",
      "you're"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "anchor": "when-to-use-explicit-calling",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-features-when-to-use-run",
    "title": "When to Use run()",
    "description": "When to Use run() Batch processing where you need complete results Scripts that can wait for full completion Analysis workflows needing aggregated met",
    "content": "When to Use run() Batch processing where you need complete results Scripts that can wait for full completion Analysis workflows needing aggregated metrics APIs returning complete responses Testing and debugging (inspect all steps together)",
    "headings": ["When to Use run()"],
    "path": "/docs/features#when-to-use-run",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "when",
      "run()",
      "complete",
      "batch",
      "processing",
      "results",
      "scripts",
      "completion",
      "analysis",
      "workflows",
      "needing"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "when-to-use-run",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-features-when-to-use-streaming",
    "title": "When to Use Streaming",
    "description": "When to Use Streaming",
    "content": "When to Use Streaming",
    "headings": ["When to Use Streaming"],
    "path": "/docs/features#when-to-use-streaming",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": ["when", "streaming"],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "anchor": "when-to-use-streaming",
    "parentTitle": "Features - Volcano SDK"
  },
  {
    "id": "Documentation-patterns-while-loop",
    "title": "While Loop",
    "description": "While Loop Continue until a condition becomes false: [code block]",
    "content": "While Loop Continue until a condition becomes false: [code block]",
    "headings": ["While Loop"],
    "path": "/docs/patterns#while-loop",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "while",
      "loop",
      "continue",
      "until",
      "condition",
      "becomes",
      "false:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.833Z",
    "codeBlocks": [
      "await agent({ llm })\n  .while(\n    (history) => {\n      if (history.length === 0) return true;\n      const last = history[history.length - 1];\n      return !last.llmOutput?.includes(\"COMPLETE\");\n    },\n    (a) => a.then({ prompt: \"Process next chunk\", mcps: [database] }),\n    { maxIterations: 10 }\n  )\n  .then({ prompt: \"Generate final summary\" })\n  .run();\n"
    ],
    "anchor": "while-loop",
    "parentTitle": "Advanced Patterns - Volcano SDK"
  },
  {
    "id": "Documentation-mcp-tools-with-llm-step",
    "title": "With LLM Step",
    "description": "With LLM Step Combine LLM reasoning with explicit tool calls: [code block]",
    "content": "With LLM Step Combine LLM reasoning with explicit tool calls: [code block]",
    "headings": ["With LLM Step"],
    "path": "/docs/mcp-tools#with-llm-step",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "step",
      "combine",
      "reasoning",
      "explicit",
      "calls:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.788Z",
    "codeBlocks": [
      "await agent({ llm })\n  .then({\n    mcp: database,\n    tool: \"query_users\",\n    args: { status: \"active\" },\n    prompt: \"Analyze the results and summarize\", // LLM processes tool output\n  })\n  .run();\n"
    ],
    "anchor": "with-llm-step",
    "parentTitle": "MCP Tools - Volcano SDK"
  },
  {
    "id": "Documentation-features-with-logging-callback",
    "title": "With Logging Callback",
    "description": "With Logging Callback Pass a callback function to log each step as it completes: [code block]",
    "content": "With Logging Callback Pass a callback function to log each step as it completes: [code block]",
    "headings": ["With Logging Callback"],
    "path": "/docs/features#with-logging-callback",
    "section": "Documentation",
    "type": "Documentation",
    "keywords": [
      "logging",
      "callback",
      "function",
      "completes:",
      "[code",
      "block]"
    ],
    "lastModified": "2025-10-11T05:21:51.733Z",
    "codeBlocks": [
      "const results = await agent({ llm })\n  .then({ prompt: \"Step 1\" })\n  .then({ prompt: \"Step 2\" })\n  .then({ prompt: \"Step 3\" })\n  .run((stepResult, stepIndex) => {\n    console.log(`Step ${stepIndex + 1} completed`);\n    console.log(`Duration: ${stepResult.durationMs}ms`);\n    console.log(`Output: ${stepResult.llmOutput}`);\n  });\n\n// Callback is called for each step as it completes\n// Final results array is returned when all steps finish\n"
    ],
    "anchor": "with-logging-callback",
    "parentTitle": "Features - Volcano SDK"
  }
]
