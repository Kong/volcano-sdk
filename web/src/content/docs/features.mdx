---
title: "Features - Volcano SDK"
---

# Features

Core features for building AI agents: execution methods, streaming, retries, timeouts, hooks, error handling, and MCP tool integration.

## run() Method

Execute the complete agent workflow and return all step results at once.

```typescript
const results = await agent({ llm })
  .then({ prompt: "Analyze user data", mcps: [analytics] })
  .then({ prompt: "Generate insights" })
  .then({ prompt: "Create recommendations" })
  .run();

// All steps complete before results are returned
console.log(results); // Array of all StepResult objects
console.log(results[0].llmOutput); // First step output
console.log(results[1].llmOutput); // Second step output
console.log(results.at(-1).totalDurationMs); // Total time
```

### With Logging Callback

Pass a callback function to log each step as it completes:

```typescript
const results = await agent({ llm })
  .then({ prompt: "Step 1" })
  .then({ prompt: "Step 2" })
  .then({ prompt: "Step 3" })
  .run((stepResult, stepIndex) => {
    console.log(`Step ${stepIndex + 1} completed`);
    console.log(`Duration: ${stepResult.durationMs}ms`);
    console.log(`Output: ${stepResult.llmOutput}`);
  });

// Callback is called for each step as it completes
// Final results array is returned when all steps finish
```

### Return Value

Returns `Promise<StepResult[]>` with all step results:

```typescript
type StepResult = {
  prompt?: string;
  llmOutput?: string;
  durationMs?: number;
  llmMs?: number;
  toolCalls?: Array<{ name: string; result: any; ms?: number }>;
  // Aggregated metrics (on final step only):
  totalDurationMs?: number;
  totalLlmMs?: number;
  totalMcpMs?: number;
};
```

### Characteristics

- **Waits for completion:** Returns only after all steps finish
- **Aggregated metrics:** Final step includes total duration, LLM time, and MCP time
- **Error handling:** Throws on first failure (use try/catch)
- **Sequential execution:** Steps run in order, one after another
- **Full results:** Access all step data for analysis or debugging

### When to Use run()

- Batch processing where you need complete results
- Scripts that can wait for full completion
- Analysis workflows needing aggregated metrics
- APIs returning complete responses
- Testing and debugging (inspect all steps together)

## Conversational Results

Ask natural language questions about what your agent did. Instead of manually parsing results, use an LLM to analyze execution and provide contextual answers.

### Basic Usage

```typescript
const results = await agent({ llm })
  .then({ prompt: "Analyze sales data", mcps: [database] })
  .then({ prompt: "Generate report" })
  .run();

// Ask questions in natural language
const summary = await results.summary(llm);
console.log(summary);
// "The agent analyzed sales data from Q3 2025 and generated a comprehensive report..."

const tools = await results.toolsUsed(llm);
console.log(tools);
// "The agent used database.query_sales to fetch sales data..."
```

### Available Methods

#### `results.ask(llm, question)`
Ask any question about the execution:

```typescript
await results.ask(llm, "What did the agent accomplish?");
await results.ask(llm, "Were there any errors?");
await results.ask(llm, "How many API calls were made?");
await results.ask(llm, "What should I do next?");
await results.ask(llm, "Summarize in one sentence");
```

#### `results.summary(llm)`
Get a high-level overview:

```typescript
const summary = await results.summary(llm);
// "The agent completed 3 steps in 15.2 seconds, analyzing 30 emails and detecting no spam."
```

#### `results.toolsUsed(llm)`
Understand which tools were called:

```typescript
const tools = await results.toolsUsed(llm);
// "The agent called list_all_unread_emails to fetch 30 emails from Gmail..."
```

#### `results.errors(llm)`
Check for issues:

```typescript
const errors = await results.errors(llm);
// "No errors detected." or "Step 2 failed due to timeout..."
```

### Cost Optimization

Use a cheaper model for analyzing results:

```typescript
const workLlm = llmOpenAI({ model: "gpt-5" });  // Expensive, for actual work
const summaryLlm = llmOpenAI({ model: "gpt-4o-mini" });  // Cheap, for summaries

const results = await agent({ llm: workLlm })
  .then({ prompt: "Complex analysis", mcps: [tools] })
  .run();

// Use cheap model for post-analysis
await results.summary(summaryLlm);
await results.ask(summaryLlm, "What were the key findings?");
```

### Real-World Example: Gmail Spam Detector

```typescript
// Before: 40 lines of manual result parsing
const toolCalls = results[0].toolCalls || [];
const spamMarked = toolCalls.filter(t => t.name.includes('mark_as_spam'));
// ... complex parsing logic ...

// After: 2 lines with conversational API
await results.summary(summaryLlm);
await results.ask(summaryLlm, "List all spam emails with sender and subject");
```

### Benefits

- âœ… **Contextual** - LLM understands what the agent actually did
- âœ… **Flexible** - Ask any question, get relevant answers
- âœ… **Clean code** - No manual result parsing
- âœ… **Self-documenting** - LLM explains execution in plain English
- âœ… **Domain-agnostic** - Works for any workflow
- âœ… **Interactive** - Drill down with follow-up questions

### How It Works

When you call `results.ask()` or `results.summary()`:

1. Volcano builds a context string with all step results
2. The context includes prompts, LLM outputs, tool calls, and timing
3. Your question is added to the context
4. The LLM analyzes the full execution and answers
5. The answer is returned as a string

The LLM sees the complete execution history and can provide intelligent, contextual answers about what happened.

## stream() Method

Stream step results in real-time as they complete using the `stream()` method.

```typescript
for await (const stepResult of agent({ llm })
  .then({ prompt: "Analyze user data", mcps: [analytics] })
  .then({ prompt: "Generate insights" })
  .then({ prompt: "Create recommendations" })
  .stream()) {
  console.log(`Step completed: ${stepResult.prompt}`);
  console.log(`Duration: ${stepResult.durationMs}ms`);

  if (stepResult.llmOutput) {
    console.log(`Result: ${stepResult.llmOutput}`);
  }
}
```

### Streaming with Progress Tracking

```typescript
let completedSteps = 0;
const totalSteps = 3;

for await (const stepResult of workflow.stream((step, stepIndex) => {
  completedSteps++;
  console.log(`Progress: ${completedSteps}/${totalSteps}`);
})) {
  updateProgressBar(completedSteps / totalSteps);
  displayStepResult(stepResult);
}
```

### When to Use Streaming

#### Use `stream()` for:

- Interactive applications needing live updates
- Long-running workflows (>5 seconds)
- Real-time dashboards
- WebSocket/SSE applications
- Memory-sensitive environments
- Early termination scenarios

#### Use `run()` for:

- Batch processing
- Simple scripts
- Analysis workflows
- APIs returning complete responses
- Testing and debugging
- When you need aggregated metrics

### Characteristics

- **Immediate feedback:** First result available when first step completes
- **Memory efficient:** Results can be processed and discarded incrementally
- **Progressive display:** Results appear as they complete
- **Same execution model:** Retries, timeouts, and validation work identically to `run()`

### Token-Level Streaming

Stream individual tokens as they arrive from the LLM for real-time chat interfaces.

#### Per-Step Token Streaming

```typescript
await agent({ llm })
  .then({
    prompt: "Explain AI",
    onToken: (token: string) => {
      process.stdout.write(token);
      // Or: res.write(`data: ${token}\n\n`);
    },
  })
  .run();
```

#### Stream-Level Token Streaming with Metadata

```typescript
for await (const step of agent({ llm })
  .then({ prompt: "Analyze data" })
  .then({ prompt: "Generate report" })
  .stream({
    onToken: (token, meta) => {
      // meta.stepIndex, meta.handledByStep, meta.stepPrompt, meta.llmProvider
      res.write(`data: ${JSON.stringify({ token, step: meta.stepIndex })}\n\n`);
    },
    onStep: (step, index) => {
      console.log(`Step ${index} complete`);
    },
  })) {
  // Step results yield here
}
```

**ðŸ’¡ Precedence:** Step-level `onToken` takes precedence over stream-level. If a step defines `onToken`, `meta.handledByStep` will be `true` and stream-level callback won't receive those tokens.

## Autonomous Multi-Agent Crews â­

Build crews of specialized agents that automatically coordinate with each otherâ€”like automatic tool selection, but for agents!

Define agents with names and descriptions, then let the LLM coordinator intelligently route tasks to the right agent. No manual orchestration, no complex state machinesâ€”just describe what each agent does.

### How It Works

```typescript
import { agent, llmOpenAI } from "volcano-sdk";

const llm = llmOpenAI({ apiKey: process.env.OPENAI_API_KEY! });

// 1. Define specialized agents with clear roles
const researcher = agent({
  llm,
  name: 'researcher',
  description: 'Analyzes topics, gathers data, and provides factual information. Use when you need research or analysis.'
});

const writer = agent({
  llm,
  name: 'writer',
  description: 'Creates engaging, well-structured articles and content. Use when you need creative writing.'
});

// 2. Create a coordinator that autonomously delegates tasks
const results = await agent({ llm })
  .then({
    prompt: 'Create a comprehensive blog post about AI safety',
    agents: [researcher, writer],
    maxAgentIterations: 5  // Allow multiple agent calls
  })
  .run();

// The coordinator automatically:
// - Decides which agent to use based on descriptions
// - Delegates "research AI safety" to researcher
// - Delegates "write blog post" to writer
// - Coordinates between them until task complete
```

### Why This Matters

ðŸŽ¯ **No Manual Routing** - The LLM coordinator reads agent descriptions and decides which one to use  
ðŸ”„ **Automatic Coordination** - Agents can call each other iteratively until the task is complete  
ðŸ§© **Composable** - Each agent can be a complex multi-step workflow itself  
âš¡ **Flexible** - Add/remove agents without changing coordinator logic  
ðŸŽ¨ **Natural** - Just describe what each agent doesâ€”no complex orchestration code

### Real-World Example

```typescript
const dataAnalyst = agent({
  llm,
  name: 'dataAnalyst',
  description: 'Analyzes data, creates reports, extracts insights from numbers'
}).then({ prompt: 'Load dataset' })
  .then({ prompt: 'Run statistical analysis' })
  .then({ prompt: 'Identify trends' });

const codeReviewer = agent({
  llm,
  name: 'codeReviewer',  
  description: 'Reviews code, finds bugs, suggests improvements'
});

const documentor = agent({
  llm,
  name: 'documentor',
  description: 'Writes clear documentation and technical guides'
});

// Coordinator handles complex task automatically
await agent({ llm })
  .then({
    prompt: 'Analyze our user data from Q4 and create a comprehensive report with recommendations',
    agents: [dataAnalyst, codeReviewer, documentor],
    maxAgentIterations: 10
  })
  .run();

// Coordinator autonomously:
// 1. Uses dataAnalyst to crunch numbers
// 2. Might use codeReviewer if code issues found
// 3. Uses documentor to write the final report
```

**[See Multi-Agent Crews in Advanced Patterns â†’](https://volcano.dev/docs/patterns#sub-agent-composition)**

## Events & Progress Tracking

Volcano provides rich event hooks and beautiful built-in progress output for real-time visibility into workflow execution.

### Built-In Progress Output

Gorgeous TTY progress is **enabled by default** for all workflows:

```typescript
agent({ llm })
  .then({ prompt: "Analyze data" })
  .then({ prompt: "Generate insights", agents: [researcher, writer] })
  .run();
```

To disable progress output:

```typescript
agent({ llm, hideProgress: true })
  .then({ prompt: "Silent execution" })
  .run();
```

**Output:**
```
ðŸŒ‹ Volcano SDK
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ðŸ”· Step 1/2: Analyze data
   ðŸ’­ GPT-4o | 127 tokens | 2.3s | âš¡ 55 tok/s
âœ… Complete (2.3s)

ðŸ”· Step 2/2: Generate insights
   âš¡ researcher â†’ Researching...
      ðŸ’­ 234 tokens | 2.4s | âš¡ 98 tok/s
   âœ… researcher complete (612 chars, 2.4s)
   
   âš¡ writer â†’ Creating content...
      ðŸ’­ 412 tokens | 3.8s | âš¡ 108 tok/s
   âœ… writer complete (934 chars, 3.8s)
âœ… Complete (6.2s)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸŽ‰ Workflow complete! 2 steps in 8.5s
```

**Features:**
- âœ¨ Animated live updates
- ðŸ“Š Token counts and throughput
- â±ï¸ Real-time timing
- ðŸŽ¨ TTY-aware (safe for CI logs)
- ðŸ”„ Works for all step types

### Custom Event Handlers

#### Agent Lifecycle Events

Track autonomous agent coordination:

```typescript
.then({
  agents: [researcher, writer, editor],
  
  onAgentEvent: (event) => {
    if (event.type === 'start') {
      console.log(`Starting ${event.agentName}: ${event.task}`);
      startTimer(event.agentName);
    } else if (event.type === 'complete') {
      console.log(`${event.agentName} done: ${event.result.length} chars in ${event.durationMs}ms`);
      logMetrics(event);
    } else if (event.type === 'error') {
      console.error(`${event.agentName} failed: ${event.error}`);
      sendAlert(event);
    }
  }
})
```

**Event Types:**
- `start` - Agent begins execution
- `complete` - Agent finishes successfully
- `error` - Agent encounters error

#### Token-Level Events with Agent Context

When using agent coordination, `onToken` receives enhanced metadata:

```typescript
.then({
  agents: [researcher, writer],
  
  onToken: (token, meta) => {
    if (meta.agentName) {
      // Inside agent execution
      console.log(`${meta.agentName}: token #${meta.tokenCount}`);
      sendToSSE({ agent: meta.agentName, token, count: meta.tokenCount });
    } else {
      // Regular step
      console.log(`${meta.llmProvider}: token`);
    }
  }
})
```

**Enhanced TokenMetadata fields:**
- `agentName` - Which agent is executing (when in agent coordination)
- `agentTask` - Task delegated to the agent
- `tokenCount` - Total tokens processed by this agent

#### Combining Progress + Custom Events

Built-in progress + custom handlers:

```typescript
agent({ llm })  // Progress enabled by default
  .then({
    agents: [researcher, writer],
    
    // Add custom logic without losing default progress
    onAgentEvent: (event) => {
      // This OVERRIDES default - but you can call both:
      if (event.type === 'complete') {
        sendToAnalytics(event);  // Your custom logic
      }
    }
  })
  .run();
```

**ðŸ’¡ Tip:** If you provide custom `onAgentEvent` or `onToken`, the default progress for that callback is disabled. Progress is enabled by default; use `hideProgress: true` to disable it entirely.

### SSE Integration Example

Perfect for real-time web UIs:

```typescript
app.post('/api/create-content', async (req, res) => {
  res.setHeader('Content-Type', 'text/event-stream');
  
  await agent({ llm })
    .then({
      prompt: req.body.prompt,
      agents: [researcher, writer, editor],
      
      onAgentEvent: (event) => {
        res.write(`data: ${JSON.stringify({ type: 'agent', ...event })}\n\n`);
      },
      
      onToken: (token, meta) => {
        if (meta.agentName) {
          res.write(`data: ${JSON.stringify({ 
            type: 'token',
            agent: meta.agentName,
            token,
            count: meta.tokenCount
          })}\n\n`);
        }
      }
    })
    .run();
  
  res.write('data: [DONE]\n\n');
  res.end();
});
```

## Retries & Timeouts

### Timeouts

Set per-step or global timeouts (in seconds):

```typescript
await agent({ llm, timeout: 60 })
  .then({ prompt: "Quick check", timeout: 5 }) // Override to 5s
  .then({ prompt: "Next step uses agent default (60s)" })
  .run();
```

### Retry Strategies

#### Immediate Retry (Default)

Retry immediately without waiting:

```typescript
await agent({ llm, retry: { retries: 3 } })
  .then({ prompt: "hello" })
  .run();
```

#### Delayed Retry

Wait a fixed duration between attempts:

```typescript
await agent({ llm, retry: { delay: 20, retries: 3 } })
  .then({ prompt: "unstable action" })
  .run();
// Waits 20s between each retry
```

#### Exponential Backoff

Progressively longer waits between retries:

```typescript
await agent({ llm, retry: { backoff: 2, retries: 4 } })
  .then({ prompt: "might fail" })
  .run();
// Waits: 1s, 2s, 4s, 8s between attempts
```

### Per-Step Override

```typescript
await agent({ llm, retry: { delay: 20 } })
  .then({ prompt: "override to immediate", retry: { delay: 0 } })
  .run();
```

### Retry Semantics

- Non-retryable errors (like `ValidationError`) abort immediately
- Retryable errors include: timeouts, 429, 5xx, network errors
- On retry exhaustion, the last error is thrown
- You cannot set both `delay` and `backoff`

## Step Hooks

Add `pre` and `post` hooks for fine-grained control over execution flow:

```typescript
await agent({ llm })
  .then({
    prompt: "Analyze the user data",
    mcps: [analytics],
    pre: () => {
      console.log("Starting analysis...");
    },
    post: () => {
      console.log("Analysis complete!");
    },
  })
  .then({
    prompt: "Generate report",
    pre: () => {
      startTimer();
    },
    post: () => {
      endTimer();
      saveMetrics();
    },
  })
  .run((step, stepIndex) => {
    console.log(`Step ${stepIndex + 1} finished`);
  });
```

### Hook Execution Order

1. `pre()` hook (before step execution)
2. Step execution (LLM/MCP calls)
3. `post()` hook (after step completion)
4. `run()` callback (with step result and index)

### Hook Characteristics

- Hooks are **synchronous functions** (`() => void`)
- Hook errors are **caught and logged** but don't fail the step
- Hooks execute on **every retry attempt** (pre) or **only on success** (post)
- Hooks have access to **closure variables** for state management

**Note:** Hook errors are caught and logged but do not fail the step. Pre-hooks execute on every retry attempt. Post-hooks execute only on successful completion.

### Use Cases

- Performance monitoring and timing
- Logging and debugging
- State management
- Notifications and alerts
- Metrics collection

## Error Handling

Volcano surfaces typed errors with rich metadata for easy debugging.

### Error Types

| Error Type              | Description                         |
| ----------------------- | ----------------------------------- |
| `AgentConcurrencyError` | run() called twice on same instance |
| `TimeoutError`          | Step exceeded timeout limit         |
| `ValidationError`       | Tool args failed schema validation  |
| `RetryExhaustedError`   | Final failure after all retries     |
| `LLMError`              | LLM provider error                  |
| `MCPToolError`          | MCP tool execution error            |
| `MCPConnectionError`    | MCP connection error                |

### Error Metadata

All Volcano errors include metadata for debugging:

```typescript
try {
  await agent({ llm, retry: { backoff: 2, retries: 4 }, timeout: 30 })
    .then({ prompt: "auto", mcps: [mcp("http://localhost:3211/mcp")] })
    .run();
} catch (err) {
  if (err && typeof err === "object" && "meta" in err) {
    const e = err as VolcanoError;

    console.error(e.name, e.message);
    console.error("Metadata:", {
      stepId: e.meta.stepId, // 0-based step index
      provider: e.meta.provider, // llm:openai or mcp:localhost
      requestId: e.meta.requestId, // Upstream request ID
      retryable: e.meta.retryable, // Should retry?
    });

    if (e.meta?.retryable) {
      // Maybe enqueue for retry later
    }
  }
}
```

### Metadata Fields

- `stepId`: 0-based index of the failing step
- `provider`: `llm:<id|model>` or `mcp:<host>`
- `requestId`: Upstream provider request ID when available
- `retryable`: Volcano's hint (true for 429/5xx/timeouts; false for validation/4xx)
