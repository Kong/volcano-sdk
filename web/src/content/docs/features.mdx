---
title: "Features - Volcano SDK"
---

# Features

Core features for building AI agents: execution methods, streaming, retries, timeouts, hooks, error handling, and MCP tool integration.

## run() Method

Execute the complete agent workflow and return all step results at once.

```typescript
const results = await agent({ llm })
  .then({ prompt: "Analyze user data", mcps: [analytics] })
  .then({ prompt: "Generate insights" })
  .then({ prompt: "Create recommendations" })
  .run();

// All steps complete before results are returned
console.log(results); // Array of all StepResult objects
console.log(results[0].llmOutput); // First step output
console.log(results[1].llmOutput); // Second step output
console.log(results.at(-1).totalDurationMs); // Total time
```

### With Logging Callback

Pass a callback function to log each step as it completes:

```typescript
const results = await agent({ llm })
  .then({ prompt: "Step 1" })
  .then({ prompt: "Step 2" })
  .then({ prompt: "Step 3" })
  .run((stepResult, stepIndex) => {
    console.log(`Step ${stepIndex + 1} completed`);
    console.log(`Duration: ${stepResult.durationMs}ms`);
    console.log(`Output: ${stepResult.llmOutput}`);
  });

// Callback is called for each step as it completes
// Final results array is returned when all steps finish
```

### Return Value

Returns `Promise<StepResult[]>` with all step results:

```typescript
type StepResult = {
  prompt?: string;
  llmOutput?: string;
  durationMs?: number;
  llmMs?: number;
  toolCalls?: Array<{ name: string; result: any; ms?: number }>;
  // Aggregated metrics (on final step only):
  totalDurationMs?: number;
  totalLlmMs?: number;
  totalMcpMs?: number;
};
```

### Characteristics

- **Waits for completion:** Returns only after all steps finish
- **Aggregated metrics:** Final step includes total duration, LLM time, and MCP time
- **Error handling:** Throws on first failure (use try/catch)
- **Sequential execution:** Steps run in order, one after another
- **Full results:** Access all step data for analysis or debugging

### When to Use run()

- Batch processing where you need complete results
- Scripts that can wait for full completion
- Analysis workflows needing aggregated metrics
- APIs returning complete responses
- Testing and debugging (inspect all steps together)

## stream() Method

Stream step results in real-time as they complete using the `stream()` method.

```typescript
for await (const stepResult of agent({ llm })
  .then({ prompt: "Analyze user data", mcps: [analytics] })
  .then({ prompt: "Generate insights" })
  .then({ prompt: "Create recommendations" })
  .stream()) {
  console.log(`Step completed: ${stepResult.prompt}`);
  console.log(`Duration: ${stepResult.durationMs}ms`);

  if (stepResult.llmOutput) {
    console.log(`Result: ${stepResult.llmOutput}`);
  }
}
```

### Streaming with Progress Tracking

```typescript
let completedSteps = 0;
const totalSteps = 3;

for await (const stepResult of workflow.stream((step, stepIndex) => {
  completedSteps++;
  console.log(`Progress: ${completedSteps}/${totalSteps}`);
})) {
  updateProgressBar(completedSteps / totalSteps);
  displayStepResult(stepResult);
}
```

### When to Use Streaming

#### Use `stream()` for:

- Interactive applications needing live updates
- Long-running workflows (>5 seconds)
- Real-time dashboards
- WebSocket/SSE applications
- Memory-sensitive environments
- Early termination scenarios

#### Use `run()` for:

- Batch processing
- Simple scripts
- Analysis workflows
- APIs returning complete responses
- Testing and debugging
- When you need aggregated metrics

### Characteristics

- **Immediate feedback:** First result available when first step completes
- **Memory efficient:** Results can be processed and discarded incrementally
- **Progressive display:** Results appear as they complete
- **Same execution model:** Retries, timeouts, and validation work identically to `run()`

### Token-Level Streaming

Stream individual tokens as they arrive from the LLM for real-time chat interfaces.

#### Per-Step Token Streaming

```typescript
await agent({ llm })
  .then({
    prompt: "Explain AI",
    onToken: (token: string) => {
      process.stdout.write(token);
      // Or: res.write(`data: ${token}\n\n`);
    },
  })
  .run();
```

#### Stream-Level Token Streaming with Metadata

```typescript
for await (const step of agent({ llm })
  .then({ prompt: "Analyze data" })
  .then({ prompt: "Generate report" })
  .stream({
    onToken: (token, meta) => {
      // meta.stepIndex, meta.handledByStep, meta.stepPrompt, meta.llmProvider
      res.write(`data: ${JSON.stringify({ token, step: meta.stepIndex })}\n\n`);
    },
    onStep: (step, index) => {
      console.log(`Step ${index} complete`);
    },
  })) {
  // Step results yield here
}
```

**ðŸ’¡ Precedence:** Step-level `onToken` takes precedence over stream-level. If a step defines `onToken`, `meta.handledByStep` will be `true` and stream-level callback won't receive those tokens.

## Autonomous Multi-Agent Crews â­

Build crews of specialized agents that automatically coordinate with each otherâ€”like automatic tool selection, but for agents!

Define agents with names and descriptions, then let the LLM coordinator intelligently route tasks to the right agent. No manual orchestration, no complex state machinesâ€”just describe what each agent does.

### How It Works

```typescript
import { agent, llmOpenAI } from "volcano-sdk";

const llm = llmOpenAI({ apiKey: process.env.OPENAI_API_KEY! });

// 1. Define specialized agents with clear roles
const researcher = agent({
  llm,
  name: 'researcher',
  description: 'Analyzes topics, gathers data, and provides factual information. Use when you need research or analysis.'
});

const writer = agent({
  llm,
  name: 'writer',
  description: 'Creates engaging, well-structured articles and content. Use when you need creative writing.'
});

// 2. Create a coordinator that autonomously delegates tasks
const results = await agent({ llm })
  .then({
    prompt: 'Create a comprehensive blog post about AI safety',
    agents: [researcher, writer],
    maxAgentIterations: 5  // Allow multiple agent calls
  })
  .run();

// The coordinator automatically:
// - Decides which agent to use based on descriptions
// - Delegates "research AI safety" to researcher
// - Delegates "write blog post" to writer
// - Coordinates between them until task complete
```

### Why This Matters

ðŸŽ¯ **No Manual Routing** - The LLM coordinator reads agent descriptions and decides which one to use  
ðŸ”„ **Automatic Coordination** - Agents can call each other iteratively until the task is complete  
ðŸ§© **Composable** - Each agent can be a complex multi-step workflow itself  
âš¡ **Flexible** - Add/remove agents without changing coordinator logic  
ðŸŽ¨ **Natural** - Just describe what each agent doesâ€”no complex orchestration code

### Real-World Example

```typescript
const dataAnalyst = agent({
  llm,
  name: 'dataAnalyst',
  description: 'Analyzes data, creates reports, extracts insights from numbers'
}).then({ prompt: 'Load dataset' })
  .then({ prompt: 'Run statistical analysis' })
  .then({ prompt: 'Identify trends' });

const codeReviewer = agent({
  llm,
  name: 'codeReviewer',  
  description: 'Reviews code, finds bugs, suggests improvements'
});

const documentor = agent({
  llm,
  name: 'documentor',
  description: 'Writes clear documentation and technical guides'
});

// Coordinator handles complex task automatically
await agent({ llm })
  .then({
    prompt: 'Analyze our user data from Q4 and create a comprehensive report with recommendations',
    agents: [dataAnalyst, codeReviewer, documentor],
    maxAgentIterations: 10
  })
  .run();

// Coordinator autonomously:
// 1. Uses dataAnalyst to crunch numbers
// 2. Might use codeReviewer if code issues found
// 3. Uses documentor to write the final report
```

**[See Multi-Agent Crews in Advanced Patterns â†’](https://volcano.dev/docs/patterns#sub-agent-composition)**

## Events & Progress Tracking

Volcano provides rich event hooks and beautiful built-in progress output for real-time visibility into workflow execution.

### Built-In Progress Output

Gorgeous TTY progress is **enabled by default** for all workflows:

```typescript
agent({ llm })
  .then({ prompt: "Analyze data" })
  .then({ prompt: "Generate insights", agents: [researcher, writer] })
  .run();
```

To disable progress output:

```typescript
agent({ llm, hideProgress: true })
  .then({ prompt: "Silent execution" })
  .run();
```

**Output:**
```
ðŸŒ‹ Volcano SDK
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ðŸ”· Step 1/2: Analyze data
   ðŸ’­ GPT-4o | 127 tokens | 2.3s | âš¡ 55 tok/s
âœ… Complete (2.3s)

ðŸ”· Step 2/2: Generate insights
   âš¡ researcher â†’ Researching...
      ðŸ’­ 234 tokens | 2.4s | âš¡ 98 tok/s
   âœ… researcher complete (612 chars, 2.4s)
   
   âš¡ writer â†’ Creating content...
      ðŸ’­ 412 tokens | 3.8s | âš¡ 108 tok/s
   âœ… writer complete (934 chars, 3.8s)
âœ… Complete (6.2s)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸŽ‰ Workflow complete! 2 steps in 8.5s
```

**Features:**
- âœ¨ Animated live updates
- ðŸ“Š Token counts and throughput
- â±ï¸ Real-time timing
- ðŸŽ¨ TTY-aware (safe for CI logs)
- ðŸ”„ Works for all step types

### Custom Event Handlers

#### Agent Lifecycle Events

Track autonomous agent coordination:

```typescript
.then({
  agents: [researcher, writer, editor],
  
  onAgentEvent: (event) => {
    if (event.type === 'start') {
      console.log(`Starting ${event.agentName}: ${event.task}`);
      startTimer(event.agentName);
    } else if (event.type === 'complete') {
      console.log(`${event.agentName} done: ${event.result.length} chars in ${event.durationMs}ms`);
      logMetrics(event);
    } else if (event.type === 'error') {
      console.error(`${event.agentName} failed: ${event.error}`);
      sendAlert(event);
    }
  }
})
```

**Event Types:**
- `start` - Agent begins execution
- `complete` - Agent finishes successfully
- `error` - Agent encounters error

#### Token-Level Events with Agent Context

When using agent coordination, `onToken` receives enhanced metadata:

```typescript
.then({
  agents: [researcher, writer],
  
  onToken: (token, meta) => {
    if (meta.agentName) {
      // Inside agent execution
      console.log(`${meta.agentName}: token #${meta.tokenCount}`);
      sendToSSE({ agent: meta.agentName, token, count: meta.tokenCount });
    } else {
      // Regular step
      console.log(`${meta.llmProvider}: token`);
    }
  }
})
```

**Enhanced TokenMetadata fields:**
- `agentName` - Which agent is executing (when in agent coordination)
- `agentTask` - Task delegated to the agent
- `tokenCount` - Total tokens processed by this agent

#### Combining Progress + Custom Events

Built-in progress + custom handlers:

```typescript
agent({ llm })  // Progress enabled by default
  .then({
    agents: [researcher, writer],
    
    // Add custom logic without losing default progress
    onAgentEvent: (event) => {
      // This OVERRIDES default - but you can call both:
      if (event.type === 'complete') {
        sendToAnalytics(event);  // Your custom logic
      }
    }
  })
  .run();
```

**ðŸ’¡ Tip:** If you provide custom `onAgentEvent` or `onToken`, the default progress for that callback is disabled. Progress is enabled by default; use `hideProgress: true` to disable it entirely.

### SSE Integration Example

Perfect for real-time web UIs:

```typescript
app.post('/api/create-content', async (req, res) => {
  res.setHeader('Content-Type', 'text/event-stream');
  
  await agent({ llm })
    .then({
      prompt: req.body.prompt,
      agents: [researcher, writer, editor],
      
      onAgentEvent: (event) => {
        res.write(`data: ${JSON.stringify({ type: 'agent', ...event })}\n\n`);
      },
      
      onToken: (token, meta) => {
        if (meta.agentName) {
          res.write(`data: ${JSON.stringify({ 
            type: 'token',
            agent: meta.agentName,
            token,
            count: meta.tokenCount
          })}\n\n`);
        }
      }
    })
    .run();
  
  res.write('data: [DONE]\n\n');
  res.end();
});
```

## Retries & Timeouts

### Timeouts

Set per-step or global timeouts (in seconds):

```typescript
await agent({ llm, timeout: 60 })
  .then({ prompt: "Quick check", timeout: 5 }) // Override to 5s
  .then({ prompt: "Next step uses agent default (60s)" })
  .run();
```

### Retry Strategies

#### Immediate Retry (Default)

Retry immediately without waiting:

```typescript
await agent({ llm, retry: { retries: 3 } })
  .then({ prompt: "hello" })
  .run();
```

#### Delayed Retry

Wait a fixed duration between attempts:

```typescript
await agent({ llm, retry: { delay: 20, retries: 3 } })
  .then({ prompt: "unstable action" })
  .run();
// Waits 20s between each retry
```

#### Exponential Backoff

Progressively longer waits between retries:

```typescript
await agent({ llm, retry: { backoff: 2, retries: 4 } })
  .then({ prompt: "might fail" })
  .run();
// Waits: 1s, 2s, 4s, 8s between attempts
```

### Per-Step Override

```typescript
await agent({ llm, retry: { delay: 20 } })
  .then({ prompt: "override to immediate", retry: { delay: 0 } })
  .run();
```

### Retry Semantics

- Non-retryable errors (like `ValidationError`) abort immediately
- Retryable errors include: timeouts, 429, 5xx, network errors
- On retry exhaustion, the last error is thrown
- You cannot set both `delay` and `backoff`

## Step Hooks

Add `pre` and `post` hooks for fine-grained control over execution flow:

```typescript
await agent({ llm })
  .then({
    prompt: "Analyze the user data",
    mcps: [analytics],
    pre: () => {
      console.log("Starting analysis...");
    },
    post: () => {
      console.log("Analysis complete!");
    },
  })
  .then({
    prompt: "Generate report",
    pre: () => {
      startTimer();
    },
    post: () => {
      endTimer();
      saveMetrics();
    },
  })
  .run((step, stepIndex) => {
    console.log(`Step ${stepIndex + 1} finished`);
  });
```

### Hook Execution Order

1. `pre()` hook (before step execution)
2. Step execution (LLM/MCP calls)
3. `post()` hook (after step completion)
4. `run()` callback (with step result and index)

### Hook Characteristics

- Hooks are **synchronous functions** (`() => void`)
- Hook errors are **caught and logged** but don't fail the step
- Hooks execute on **every retry attempt** (pre) or **only on success** (post)
- Hooks have access to **closure variables** for state management

**Note:** Hook errors are caught and logged but do not fail the step. Pre-hooks execute on every retry attempt. Post-hooks execute only on successful completion.

### Use Cases

- Performance monitoring and timing
- Logging and debugging
- State management
- Notifications and alerts
- Metrics collection

## Error Handling

Volcano surfaces typed errors with rich metadata for easy debugging.

### Error Types

| Error Type              | Description                         |
| ----------------------- | ----------------------------------- |
| `AgentConcurrencyError` | run() called twice on same instance |
| `TimeoutError`          | Step exceeded timeout limit         |
| `ValidationError`       | Tool args failed schema validation  |
| `RetryExhaustedError`   | Final failure after all retries     |
| `LLMError`              | LLM provider error                  |
| `MCPToolError`          | MCP tool execution error            |
| `MCPConnectionError`    | MCP connection error                |

### Error Metadata

All Volcano errors include metadata for debugging:

```typescript
try {
  await agent({ llm, retry: { backoff: 2, retries: 4 }, timeout: 30 })
    .then({ prompt: "auto", mcps: [mcp("http://localhost:3211/mcp")] })
    .run();
} catch (err) {
  if (err && typeof err === "object" && "meta" in err) {
    const e = err as VolcanoError;

    console.error(e.name, e.message);
    console.error("Metadata:", {
      stepId: e.meta.stepId, // 0-based step index
      provider: e.meta.provider, // llm:openai or mcp:localhost
      requestId: e.meta.requestId, // Upstream request ID
      retryable: e.meta.retryable, // Should retry?
    });

    if (e.meta?.retryable) {
      // Maybe enqueue for retry later
    }
  }
}
```

### Metadata Fields

- `stepId`: 0-based index of the failing step
- `provider`: `llm:<id|model>` or `mcp:<host>`
- `requestId`: Upstream provider request ID when available
- `retryable`: Volcano's hint (true for 429/5xx/timeouts; false for validation/4xx)
