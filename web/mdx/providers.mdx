# LLM Providers

Volcano SDK supports 100s of models from 7 providers with function calling and MCP integration. Providers can be mixed within a single workflow.

## Provider Support Matrix

| Provider | Basic Generation | Function Calling | Streaming | MCP Integration |
|----------|-----------------|------------------|-----------|--------------------|
| **OpenAI** | âœ… Full | âœ… Native | âœ… Native | âœ… Complete |
| **Anthropic** | âœ… Full | âœ… Native (tool_use) | âœ… Native | âœ… Complete |
| **Mistral** | âœ… Full | âœ… Native | âœ… Native | âœ… Complete |
| **Llama** | âœ… Full | âœ… Via Ollama | âœ… Native | âœ… Complete |
| **AWS Bedrock** | âœ… Full | âœ… Native (Converse API) | âœ… Native | âœ… Complete |
| **Google Vertex Studio** | âœ… Full | âœ… Native (Function calling) | âœ… Native | âœ… Complete |
| **Azure AI** | âœ… Full | âœ… Native (Responses API) | âœ… Native | âœ… Complete |

**All providers support automatic tool selection and multi-step workflows.**

## OpenAI Provider

Full support for OpenAI's GPT models with function calling and streaming.

```typescript
import { llmOpenAI } from "volcano-sdk";

const openai = llmOpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
  model: "gpt-4o-mini",
  baseURL: "https://api.openai.com/v1", // Optional
  options: {
    temperature: 0.7,
    max_completion_tokens: 2000,
    top_p: 0.9,
    seed: 42
  }
});
```

### Authentication

| Method | Required | Description |
|--------|----------|-------------|
| `apiKey` | **Required** | OpenAI API key from platform.openai.com |
| `baseURL` | Optional | Custom API endpoint (default: https://api.openai.com/v1) |

### Configuration

| Parameter | Required | Description |
|-----------|----------|-------------|
| `model` | **Required** | Model identifier (e.g., gpt-4o-mini, gpt-4o) |
| `apiKey` | **Required** | OpenAI API key |
| `baseURL` | Optional | Custom API endpoint |
| `options` | Optional | Model-specific parameters (see below) |

### Options Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `temperature` | 0-2 | Controls randomness. Higher = more creative, lower = more focused |
| `max_completion_tokens` | number | Maximum tokens to generate (recommended for all models) |
| `max_tokens` | number | Legacy parameter (use max_completion_tokens instead) |
| `top_p` | 0-1 | Nucleus sampling - alternative to temperature |
| `frequency_penalty` | -2 to 2 | Reduces repetition based on token frequency |
| `presence_penalty` | -2 to 2 | Encourages topic diversity |
| `stop` | string \| string[] | Stop sequences to end generation |
| `seed` | number | For deterministic outputs (same seed = same output) |
| `response_format` | object | Force JSON output: `{ type: "json_object" }` |

## Anthropic (Claude) Provider

Native support for Claude models with tool calling.

```typescript
import { llmAnthropic } from "volcano-sdk";

const claude = llmAnthropic({
  apiKey: process.env.ANTHROPIC_API_KEY!,
  model: "claude-3-5-sonnet-20241022",
  baseURL: "https://api.anthropic.com", // Optional
  version: "2023-06-01", // Optional
  options: {
    temperature: 0.7,
    max_tokens: 2000,
    top_k: 50,
    top_p: 0.9,
    stop_sequences: ["\n\n"]
  }
});
```

### Authentication

| Method | Required | Description |
|--------|----------|-------------|
| `apiKey` | **Required** | Anthropic API key from console.anthropic.com |
| `baseURL` | Optional | Custom API endpoint (default: https://api.anthropic.com) |
| `version` | Optional | API version header (default: 2023-06-01) |

### Configuration

| Parameter | Required | Description |
|-----------|----------|-------------|
| `model` | **Required** | Model identifier (e.g., claude-3-5-sonnet-20241022) |
| `apiKey` | **Required** | Anthropic API key |
| `options` | Optional | Model-specific parameters |

### Options Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `temperature` | 0-1 | Controls randomness (note: range is 0-1, not 0-2) |
| `max_tokens` | number | Maximum tokens to generate |
| `top_p` | 0-1 | Nucleus sampling |
| `top_k` | number | Sample from top K options |
| `stop_sequences` | string[] | Array of stop sequences |
| `thinking` | object | Extended thinking configuration (Claude-specific) |

## Mistral Provider

Support for Mistral's open and proprietary models via OpenAI-compatible API.

```typescript
import { llmMistral } from "volcano-sdk";

const mistral = llmMistral({
  apiKey: process.env.MISTRAL_API_KEY!,
  model: "mistral-small-latest",
  baseURL: "https://api.mistral.ai", // Optional
  options: {
    temperature: 0.7,
    max_tokens: 2000,
    top_p: 0.9,
    safe_prompt: true,
    random_seed: 42
  }
});
```

### Authentication

| Method | Required | Description |
|--------|----------|-------------|
| `apiKey` | **Required** | Mistral API key from console.mistral.ai |
| `baseURL` | Optional | Custom API endpoint (default: https://api.mistral.ai) |

### Configuration

| Parameter | Required | Description |
|-----------|----------|-------------|
| `model` | **Required** | Model identifier |
| `apiKey` | **Required** | Mistral API key |
| `options` | Optional | Model-specific parameters |

### Options Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `temperature` | 0-1 | Controls randomness |
| `max_tokens` | number | Maximum tokens to generate |
| `top_p` | 0-1 | Nucleus sampling |
| `stop` | string \| string[] | Stop sequences |
| `safe_prompt` | boolean | Enable safety mode |
| `random_seed` | number | For deterministic outputs |
| `response_format` | object | For JSON mode: `{ type: "json_object" }` |

## Llama Provider

Run Llama models locally with Ollama or via OpenAI-compatible endpoints.

```typescript
import { llmLlama } from "volcano-sdk";

// Local Ollama setup
const llama = llmLlama({
  baseURL: "http://127.0.0.1:11434",
  model: "llama3.2:3b",
  apiKey: "", // Optional, not needed for local Ollama
  options: {
    temperature: 0.7,
    max_tokens: 2000,
    top_p: 0.9,
    top_k: 40,
    repeat_penalty: 1.1,
    seed: 42,
    num_predict: 2000
  }
});
```

> **Setup:** Install Ollama from [ollama.ai](https://ollama.ai) and run `ollama pull llama3.2:3b`

### Authentication

| Method | Required | Description |
|--------|----------|-------------|
| `baseURL` | Optional | OpenAI-compatible endpoint (default: http://localhost:11434 for Ollama) |
| `apiKey` | Optional | API key if your endpoint requires authentication |

### Configuration

| Parameter | Required | Description |
|-----------|----------|-------------|
| `model` | **Required** | Model identifier (e.g., llama3.2:3b for Ollama) |
| `baseURL` | Optional | Server endpoint |
| `options` | Optional | Model-specific parameters |

### Options Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `temperature` | 0-2 | Controls randomness |
| `max_tokens` | number | Maximum tokens to generate |
| `top_p` | 0-1 | Nucleus sampling |
| `top_k` | number | Sample from top K options |
| `stop` | string \| string[] | Stop sequences |
| `repeat_penalty` | number | Penalize repetitions (Ollama-specific, default: 1.1) |
| `seed` | number | For deterministic outputs |
| `num_predict` | number | Number of tokens to predict (Ollama-specific) |

## AWS Bedrock Provider

Access foundation models via AWS Bedrock with native tool calling support using the Converse API.

```typescript
import { llmBedrock } from "volcano-sdk";

const bedrock = llmBedrock({
  model: "anthropic.claude-3-sonnet-20240229-v1:0",
  region: "us-east-1",
  // Uses AWS credential chain by default
  options: {
    temperature: 0.7,
    max_tokens: 2000,
    top_p: 0.9,
    stop_sequences: ["\n\n"]
  }
});
```

### Authentication

AWS Bedrock supports multiple authentication methods (in priority order):

| Method | Parameters | Description |
|--------|------------|-------------|
| Explicit Credentials | `accessKeyId`, `secretAccessKey`, `sessionToken` | Directly provide AWS credentials |
| Bearer Token | `bearerToken` | Use bearer token authentication |
| AWS Profile | `profile` | Use credentials from ~/.aws/credentials |
| IAM Role | `roleArn` | Assume an IAM role |
| Default Chain | (none) | Environment variables, instance profiles, ECS/EKS roles |

### Authentication Examples

```typescript
// 1. Explicit credentials
const bedrock1 = llmBedrock({
  model: "anthropic.claude-3-sonnet-20240229-v1:0",
  region: "us-east-1",
  accessKeyId: process.env.AWS_ACCESS_KEY_ID!,
  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,
  sessionToken: process.env.AWS_SESSION_TOKEN // Optional
});

// 2. Bearer token
const bedrock2 = llmBedrock({
  model: "anthropic.claude-3-sonnet-20240229-v1:0",
  region: "us-east-1",
  bearerToken: process.env.AWS_BEARER_TOKEN_BEDROCK!
});

// 3. AWS Profile
const bedrock3 = llmBedrock({
  model: "anthropic.claude-3-sonnet-20240229-v1:0",
  region: "us-east-1",
  profile: "my-aws-profile"
});

// 4. IAM Role
const bedrock4 = llmBedrock({
  model: "anthropic.claude-3-sonnet-20240229-v1:0",
  region: "us-east-1",
  roleArn: "arn:aws:iam::123456789012:role/my-bedrock-role"
});

// 5. Default chain (recommended)
const bedrock5 = llmBedrock({
  model: "anthropic.claude-3-sonnet-20240229-v1:0",
  region: "us-east-1"
  // Automatically uses environment, instance profiles, etc.
});
```

### Configuration

| Parameter | Required | Description |
|-----------|----------|-------------|
| `model` | **Required** | Bedrock model identifier |
| `region` | Optional | AWS region (default: us-east-1) |
| `options` | Optional | Model-specific parameters |

### Options Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `temperature` | 0-1 | Controls randomness |
| `max_tokens` | number | Maximum tokens to generate |
| `top_p` | 0-1 | Nucleus sampling |
| `stop_sequences` | string[] | Array of stop sequences |

## Google Vertex Studio Provider

Google's Gemini models with function calling via AI Studio API.

```typescript
import { llmVertexStudio } from "volcano-sdk";

const vertex = llmVertexStudio({
  model: "gemini-2.0-flash-exp",
  apiKey: process.env.GCP_VERTEX_API_KEY!,
  baseURL: "https://aiplatform.googleapis.com/v1", // Optional
  options: {
    temperature: 0.8,
    max_output_tokens: 2048,
    top_k: 40,
    top_p: 0.95,
    stop_sequences: ["\n\n"],
    candidate_count: 1,
    response_mime_type: "text/plain"
  }
});
```

### Authentication

| Method | Required | Description |
|--------|----------|-------------|
| `apiKey` | **Required** | Google AI Studio API key |
| `baseURL` | Optional | Custom API endpoint (default: https://aiplatform.googleapis.com/v1) |

### Configuration

| Parameter | Required | Description |
|-----------|----------|-------------|
| `model` | **Required** | Gemini model identifier |
| `apiKey` | **Required** | Google AI Studio API key |
| `options` | Optional | Model-specific parameters |

### Options Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `temperature` | 0-2 | Controls randomness |
| `max_output_tokens` | number | Maximum tokens to generate |
| `top_p` | 0-1 | Nucleus sampling |
| `top_k` | number | Sample from top K options |
| `stop_sequences` | string[] | Array of stop sequences |
| `candidate_count` | number | Number of response variations (usually 1) |
| `response_mime_type` | string | For JSON mode: "application/json" |

> âš ï¸ **Function calling limitations:** Multiple tools per call only supported for search tools. Use multi-step workflows for complex tool orchestration.

## Azure AI Provider

Azure OpenAI Service with enterprise authentication via the Responses API.

```typescript
import { llmAzure } from "volcano-sdk";

const azure = llmAzure({
  model: "gpt-4o-mini",
  endpoint: "https://your-resource.openai.azure.com/openai/responses",
  apiKey: process.env.AZURE_AI_API_KEY!,
  apiVersion: "2025-04-01-preview" // Optional
});
```

### Authentication

Azure AI supports three authentication methods (in priority order):

| Method | Parameters | Description |
|--------|------------|-------------|
| API Key | `apiKey` | Simplest method - use your Azure resource key |
| Entra ID Token | `accessToken` | Use Microsoft Entra ID (Azure AD) access token |
| Default Credential Chain | (none) | Uses Azure SDK: Managed Identity, Service Principal, CLI credentials |

### Authentication Examples

```typescript
// 1. API Key (simplest)
const azure1 = llmAzure({
  model: "gpt-4o-mini",
  endpoint: "https://your-resource.openai.azure.com/openai/responses",
  apiKey: process.env.AZURE_AI_API_KEY!
});

// 2. Entra ID Access Token
const azure2 = llmAzure({
  model: "gpt-4o-mini",
  endpoint: "https://your-resource.openai.azure.com/openai/responses",
  accessToken: process.env.AZURE_ACCESS_TOKEN!
});

// 3. Azure Default Credential Chain
const azure3 = llmAzure({
  model: "gpt-4o-mini",
  endpoint: "https://your-resource.openai.azure.com/openai/responses"
  // Uses Azure SDK credential providers automatically
});
```

### Configuration

| Parameter | Required | Description |
|-----------|----------|-------------|
| `model` | **Required** | Deployment model name |
| `endpoint` | **Required** | Azure resource URL |
| `apiVersion` | Optional | API version (default: 2025-04-01-preview) |

### Options Parameters

> âš ï¸ **Important:** Azure Responses API currently does **not support** optional configuration parameters. All inference parameters (`temperature`, `max_tokens`, `seed`, etc.) are rejected with HTTP 400 errors. This is a limitation of Azure's Responses API endpoint.

The `AzureOptions` type is defined for API consistency but parameters cannot be used in practice.

## Creating a Custom Provider

You can create your own LLM provider by implementing the `LLMHandle` interface:

### LLMHandle Interface

```typescript
type LLMHandle = {
  id: string;
  model: string;
  gen: (prompt: string) => Promise<string>;
  genWithTools: (prompt: string, tools: ToolDefinition[]) => Promise<LLMToolResult>;
  genStream: (prompt: string) => AsyncGenerator<string, void, unknown>;
  client: any; // Your provider-specific client
};
```

### Example: Custom Provider

```typescript
import type { LLMHandle, ToolDefinition, LLMToolResult } from "volcano-sdk";

export function llmCustom(config: {
  apiKey: string;
  model: string;
  baseURL: string;
}): LLMHandle {
  
  return {
    id: "custom",
    model: config.model,
    client: null, // Your HTTP client or SDK instance
    
    async gen(prompt: string): Promise<string> {
      // Call your LLM API
      const response = await fetch(`${config.baseURL}/generate`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${config.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: config.model,
          prompt: prompt
        })
      });
      
      const data = await response.json();
      return data.text;
    },
    
    async genWithTools(prompt: string, tools: ToolDefinition[]): Promise<LLMToolResult> {
      // Call your LLM API with tool definitions
      const response = await fetch(`${config.baseURL}/generate-with-tools`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${config.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: config.model,
          prompt: prompt,
          tools: tools.map(t => ({
            name: t.name,
            description: t.description,
            parameters: t.parameters
          }))
        })
      });
      
      const data = await response.json();
      
      // Map your API response to LLMToolResult format
      return {
        content: data.text,
        toolCalls: data.tool_calls?.map((tc: any) => ({
          name: tc.name,
          arguments: tc.arguments,
          mcpHandle: tools.find(t => t.name === tc.name)?.mcpHandle
        })) || []
      };
    },
    
    async *genStream(prompt: string): AsyncGenerator<string, void, unknown> {
      // Implement streaming if your provider supports it
      const response = await fetch(`${config.baseURL}/stream`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${config.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: config.model,
          prompt: prompt
        })
      });
      
      const reader = response.body?.getReader();
      if (!reader) return;
      
      const decoder = new TextDecoder();
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        yield decoder.decode(value);
      }
    }
  };
}
```

### Using Your Custom Provider

```typescript
import { agent } from "volcano-sdk";
import { llmCustom } from "./my-custom-provider";

const customLLM = llmCustom({
  apiKey: process.env.CUSTOM_API_KEY!,
  model: "my-model-v1",
  baseURL: "https://api.myservice.com/v1"
});

const results = await agent({ llm: customLLM })
  .then({ prompt: "Hello from custom provider!" })
  .run();

console.log(results[0].llmOutput);
```

### Requirements

- **gen():** Basic text generation method (required)
- **genWithTools():** Function calling support (required for MCP tools)
- **genStream():** Streaming support (optional)
- **id & model:** Identifiers for logging and error messages

> ðŸ’¡ **Tip:** Look at existing providers in `src/llms/` for reference implementations. The OpenAI provider is a good starting point.